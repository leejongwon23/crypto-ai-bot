# safe_cleanup.py (LOCK-SAFE FINAL â€” ì ˆëŒ€ .lock ì‚­ì œ/ì ‘ê·¼ ê¸ˆì§€ + 10GB ì„œë²„ ìµœì í™”)
import os
import time
import threading
import gc
from datetime import datetime, timedelta

# --- emergency SAFE_MODE kill-switch (no-op) ---
SAFE_MODE = os.getenv("SAFE_MODE", "0") == "1"
if SAFE_MODE:
    print("[safe_cleanup] SAFE_MODE=1 â†’ cleanup ê¸°ëŠ¥ ì „ë©´ ë¹„í™œì„±í™”")

# ========= ENV helpers =========
def _env_float(key: str, default: float) -> float:
    try:
        v = os.getenv(key, None)
        return float(v) if v is not None and str(v).strip() != "" else float(default)
    except Exception:
        return float(default)

def _env_int(key: str, default: int) -> int:
    try:
        v = os.getenv(key, None)
        return int(float(v)) if v is not None and str(v).strip() != "" else int(default)
    except Exception:
        return int(default)

def _env_bool(key: str, default: bool) -> bool:
    v = os.getenv(key, None)
    if v is None:
        return bool(default)
    return str(v).strip().lower() in {"1", "true", "yes", "y", "on"}

# ====== ê¸°ë³¸ ê²½ë¡œ (env ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸) ======
ROOT_DIR = os.getenv("PERSIST_ROOT", "/persistent")
LOG_DIR = os.path.join(ROOT_DIR, "logs")
MODEL_DIR = os.path.join(ROOT_DIR, "models")
SSL_DIR = os.path.join(ROOT_DIR, "ssl_models")
LOCK_DIR = os.path.join(ROOT_DIR, "locks")
DELETED_LOG_PATH = os.path.join(LOG_DIR, "deleted_log.txt")

# ====== ì •ì±…(10GB í™˜ê²½ ê¸°ë³¸ê°’) + âœ… SAFE_* í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸° ======
KEEP_DAYS   = _env_int("MAX_LOG_AGE_DAYS", 1)
HARD_CAP_GB = _env_float("SAFE_HARD_CAP_GB", 9.6)
SOFT_CAP_GB = _env_float("SAFE_SOFT_CAP_GB", 9.0)
TRIGGER_GB  = _env_float("SAFE_TRIGGER_GB", 7.5)
MIN_FREE_GB = _env_float("MIN_FREE_GB", 0.8)

CSV_MAX_MB  = _env_int("CSV_MAX_MB", 50)
CSV_BACKUPS = _env_int("CSV_BACKUPS", 3)

MAX_MODELS_KEEP_GLOBAL = _env_int("MAX_MODELS_KEEP_GLOBAL", 200)
MAX_MODELS_PER_KEY     = _env_int("KEEP_RECENT_MODELS_PER_SYMBOL", 2)

# ğŸ†• SSL ìºì‹œ ë³´ì¡´ ê°œìˆ˜/ì†Œí”„íŠ¸ìº¡ (ì—†ìœ¼ë©´ ê¸°ë³¸ 1ê°œ/1.0GB)
SSL_KEEP_PER_KEY = _env_int("SSL_KEEP_PER_KEY", 1)
SSL_SOFT_CAP_GB  = _env_float("SSL_SOFT_CAP_GB", 1.0)

PROTECT_HOURS = _env_int("PROTECT_HOURS", 12)
LOCK_PATH = os.path.join(LOCK_DIR, "train_or_predict.lock")
DRYRUN = _env_bool("SAFE_DRYRUN", False)

# âœ… ëª¨ë¸/ë©”íƒ€ íŒŒì¼ ì¸ì‹
MODEL_EXTS = (".pt", ".ptz", ".safetensors")
META_EXT = ".meta.json"
_PREFERRED_WEIGHT_EXTS = (".ptz", ".safetensors", ".pt")  # ë³´ì¡´ ìš°ì„ ìˆœìœ„

DELETE_PREFIXES = ["prediction_", "evaluation_", "wrong_", "model_", "ssl_", "meta_", "evo_"]
EXCLUDE_FILES = {
    "prediction_log.csv", "train_log.csv", "evaluation_result.csv",
    "deleted_log.txt", "wrong_predictions.csv", "fine_tune_target.csv"
}

ROOT_CSVS = [
    os.path.join(ROOT_DIR, "prediction_log.csv"),
    os.path.join(ROOT_DIR, "wrong_predictions.csv"),
    os.path.join(ROOT_DIR, "evaluation_result.csv"),
    os.path.join(ROOT_DIR, "train_log.csv"),
]

LOCK_SUFFIX = ".lock"

# ----------------- ê³µí†µ ìœ í‹¸ -----------------
def _size_bytes(path: str) -> int:
    try:
        return os.path.getsize(path)
    except Exception:
        return 0

def get_directory_size_gb(path):
    if not os.path.isdir(path):
        return 0.0
    total = 0
    for dirpath, _, filenames in os.walk(path):
        # ğŸ”’ LOCK ë””ë ‰í„°ë¦¬ëŠ” ì•„ì˜ˆ ìˆœíšŒ ì œì™¸
        if os.path.abspath(dirpath).startswith(os.path.abspath(LOCK_DIR)):
            continue
        for f in filenames:
            # ğŸ”’ ì–´ë–¤ ê²½ë¡œë¼ë„ *.lock ì€ ìš©ëŸ‰ ê³„ì‚° ëŒ€ìƒì—ì„œë„ ì œì™¸(ì•ˆì „)
            if f.endswith(LOCK_SUFFIX):
                continue
            fp = os.path.join(dirpath, f)
            if os.path.isfile(fp):
                total += _size_bytes(fp)
    return total / (1024 ** 3)

def _human_gb(v): return f"{v:.2f}GB"

def _list_files(dir_path):
    try:
        # ğŸ”’ LOCK_DIR ì€ í˜¸ì¶œì„ ì—ì„œ ì ˆëŒ€ ë„˜ê¸°ì§€ ì•Šì§€ë§Œ, í˜¹ì‹œ ë„˜ì–´ì™€ë„ ë°˜í™˜ì„ ë¹„ì›€
        if os.path.abspath(dir_path).startswith(os.path.abspath(LOCK_DIR)):
            return []
        return [os.path.join(dir_path, f) for f in os.listdir(dir_path)]
    except Exception:
        return []

def _ensure_dirs():
    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(MODEL_DIR, exist_ok=True)
    os.makedirs(SSL_DIR, exist_ok=True)
    os.makedirs(LOCK_DIR, exist_ok=True)

def _is_within(child: str, parent: str) -> bool:
    try:
        child_abs  = os.path.abspath(child)
        parent_abs = os.path.abspath(parent)
        return os.path.commonpath([child_abs, parent_abs]) == parent_abs
    except Exception:
        return False

def _is_lock_file(path: str) -> bool:
    """ì–´ë–¤ ê²½ë¡œë¼ë„ .lock íŒŒì¼ ë˜ëŠ” LOCK_DIR ë‚´ë¶€ëŠ” ë¬´ì¡°ê±´ ë³´í˜¸."""
    try:
        if not isinstance(path, str):
            return False
        if path.endswith(LOCK_SUFFIX):
            return True
        return _is_within(path, LOCK_DIR)
    except Exception:
        return False

def _is_model_file(path: str) -> bool:
    """models/ ë‚´ë¶€ì˜ .pt/.ptz/.safetensors ë° ì§ ë©”íƒ€ë¥¼ ëª¨ë¸ë¡œ ë³¸ë‹¤."""
    if not isinstance(path, str):
        return False
    base = os.path.basename(path)
    if any(base.endswith(ext) for ext in MODEL_EXTS):
        return True
    if base.endswith(META_EXT):
        return True
    return False

def _should_delete_file(fname: str) -> bool:
    """
    ê¸°ì¡´ ê·œì¹™ + (NEW) models/ ì•ˆì˜ ëª¨ë¸ í™•ì¥ìëŠ” ì ‘ë‘ì‚¬ ì—†ì´ë„ ì •ë¦¬ ëŒ€ìƒìœ¼ë¡œ ì¸ì •.
    ë‹¨, ğŸ”’ ë½ íŒŒì¼/ë””ë ‰í„°ë¦¬ëŠ” ì ˆëŒ€ ì‚­ì œí•˜ì§€ ì•ŠìŒ.
    """
    if _is_lock_file(fname):
        return False
    base = os.path.basename(fname)
    if base in EXCLUDE_FILES:
        return False
    try:
        if _is_within(fname, MODEL_DIR) and _is_model_file(fname):
            return True
    except Exception:
        pass
    return any(base.startswith(p) for p in DELETE_PREFIXES)

def _is_recent(path: str, hours: float) -> bool:
    try:
        mtime = datetime.fromtimestamp(os.path.getmtime(path))
        return (datetime.now() - mtime) < timedelta(hours=hours)
    except Exception:
        return False

def _rollover_csv(path: str, max_mb: int, backups: int):
    if not os.path.isfile(path):
        return []
    # ğŸ”’ í˜¹ì‹œ CSV ê²½ë¡œê°€ ì˜ëª» ë“¤ì–´ì™€ë„ .lock ì€ ë¬´ì¡°ê±´ ì œì™¸
    if _is_lock_file(path):
        return []
    size_mb = _size_bytes(path) / (1024 ** 2)
    if size_mb <= max_mb:
        return []
    deleted = []
    for i in range(backups, 0, -1):
        bak = f"{path}.{i}"
        older = f"{path}.{i+1}"
        if os.path.exists(older):
            if not DRYRUN:
                os.remove(older)
            deleted.append(older)
        if os.path.exists(bak):
            if not DRYRUN:
                os.rename(bak, older)
    if not DRYRUN:
        os.rename(path, f"{path}.1")
        open(path, "w", encoding="utf-8").close()
    return deleted

def _delete_file(path: str, deleted_log: list):
    try:
        # ğŸ”’ ë½ íŒŒì¼/í´ë”ëŠ” ì ˆëŒ€ ì‚­ì œ ê¸ˆì§€
        if _is_lock_file(path):
            return
        if DRYRUN:
            print(f"[DRYRUN] ì‚­ì œ ì˜ˆì •: {path}")
            return
        os.remove(path)
        deleted_log.append(path)
        print(f"[ğŸ—‘ ì‚­ì œ] {path}")
    except Exception as e:
        print(f"[ê²½ê³ ] ì‚­ì œ ì‹¤íŒ¨: {path} | {e}")

# ----------------- (NEW) ssl_models ì •ë¦¬ -----------------
def _cleanup_ssl_models_impl(keep_per_key, soft_cap_gb, deleted_log):
    """
    ssl_models í´ë” ìŠ¬ë¦¼í™”:
      - íŒŒì¼ íŒ¨í„´: <ì‹¬ë³¼>_(ë‹¨ê¸°|ì¤‘ê¸°|ì¥ê¸°)_ssl*.pt
      - ì‹¬ë³¼Ã—ì „ëµë³„ ìµœì‹  keep_per_keyê°œë§Œ ìœ ì§€
      - í´ë” ì „ì²´ ìš©ëŸ‰ì´ soft_cap_gb ì´ˆê³¼ ì‹œ, ê°€ì¥ ì˜¤ë˜ëœ íŒŒì¼ë¶€í„° ì¶”ê°€ ì‚­ì œ
    """
    if SAFE_MODE:
        return
    try:
        import re
        os.makedirs(SSL_DIR, exist_ok=True)
        files = [p for p in _list_files(SSL_DIR) if os.path.isfile(p) and p.endswith(".pt") and not _is_lock_file(p)]
        rgx = re.compile(r"^(?P<sym>.+?)_(?P<strat>ë‹¨ê¸°|ì¤‘ê¸°|ì¥ê¸°)_ssl.*\.pt$", re.U)

        buckets = {}
        for p in files:
            key = None
            try:
                m = rgx.match(os.path.basename(p))
                key = f"{m.group('sym')}_{m.group('strat')}" if m else os.path.basename(p)
            except Exception:
                key = os.path.basename(p)
            buckets.setdefault(key, []).append(p)

        # í‚¤ë³„ ìµœì‹ ë§Œ ë³´ê´€
        for key, arr in buckets.items():
            arr.sort(key=lambda x: os.path.getmtime(x), reverse=True)
            for i, path in enumerate(arr):
                if i >= keep_per_key and not _is_recent(path, PROTECT_HOURS):
                    _delete_file(path, deleted_log)

        # ì†Œí”„íŠ¸ìº¡ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ê²ƒ ì¶”ê°€ ì‚­ì œ
        def _ssl_size():
            return get_directory_size_gb(SSL_DIR)
        while _ssl_size() > soft_cap_gb:
            rest = [p for p in _list_files(SSL_DIR) if os.path.isfile(p) and not _is_lock_file(p)]
            if not rest:
                break
            rest.sort(key=lambda x: os.path.getmtime(x))  # oldest first
            victim = None
            # ë³´í˜¸ì‹œê°„ ì§€ë‚œ ê²ƒ ìš°ì„ 
            for p in rest:
                if not _is_recent(p, PROTECT_HOURS):
                    victim = p
                    break
            if victim is None:
                # ëª¨ë‘ ë³´í˜¸ì‹œê°„ ì´ë‚´ë©´, ë” ì§„í–‰í•˜ì§€ ì•ŠìŒ(ë³´ìˆ˜ì )
                break
            _delete_file(victim, deleted_log)
    except Exception as e:
        print(f"[ssl_cleanup] ì‹¤íŒ¨: {e}")

def cleanup_ssl_models(keep_per_key=None, soft_cap_gb=None, deleted_log=None):
    """
    ì™¸ë¶€ í˜¸ì¶œìš© ë˜í¼. ì¸ì ìƒëµ ì‹œ í™˜ê²½ë³€ìˆ˜/ê¸°ë³¸ê°’ ì‚¬ìš©.
    """
    if SAFE_MODE:
        return []  # no-op
    if keep_per_key is None:
        keep_per_key = SSL_KEEP_PER_KEY
    if soft_cap_gb is None:
        soft_cap_gb = SSL_SOFT_CAP_GB
    if deleted_log is None:
        deleted_log = []
    _cleanup_ssl_models_impl(int(keep_per_key), float(soft_cap_gb), deleted_log)
    return deleted_log

# ----------------- ëª¨ë¸Â·ë©”íƒ€ ì„¸íŠ¸ ê´€ë¦¬ -----------------
def _split_stem_and_ext(path: str):
    """
    returns: (stem, ext)
      - meta: "xxx.meta.json" -> ("xxx", ".meta.json")
      - weight: "xxx.ptz" -> ("xxx", ".ptz")
    """
    base = os.path.basename(path)
    if base.endswith(".meta.json"):
        stem = base[:-10]
        ext = ".meta.json"
        return stem, ext
    root, ext = os.path.splitext(base)
    return root, ext

def _collect_model_sets():
    """
    models/ ë‚´ íŒŒì¼ë“¤ì„ stem ê¸°ì¤€ìœ¼ë¡œ ì„¸íŠ¸(ê°€ì¤‘ì¹˜+ë©”íƒ€)ë¡œ ë¬¶ì–´ì„œ ë°˜í™˜.
    return: dict[stem] = {"weights": {ext: path}, "meta": path|None, "mtime": latest_mtime}
    (ë³´í˜¸ì‹œê°„ì— í•´ë‹¹í•˜ëŠ” ìµœì‹  íŒŒì¼ì€ ì œì™¸í•˜ì—¬ ì‚­ì œ í›„ë³´ë§Œ ëŒ€ìƒìœ¼ë¡œ í•¨)
    """
    sets = {}
    for p in _list_files(MODEL_DIR):
        if not os.path.isfile(p) or _is_lock_file(p):
            continue
        if not _is_model_file(p):
            continue
        if _is_recent(p, PROTECT_HOURS):
            continue
        stem, ext = _split_stem_and_ext(p)
        st = sets.setdefault(stem, {"weights": {}, "meta": None, "mtime": 0.0})
        try:
            mtime = os.path.getmtime(p)
        except Exception:
            mtime = 0.0
        st["mtime"] = max(st["mtime"], mtime)
        if ext == ".meta.json":
            st["meta"] = p
        else:
            st["weights"][ext] = p
    return sets

def _key_from_stem(stem: str) -> str:
    """
    stem: 'BTCUSDT_ë‹¨ê¸°_lstm_group1_cls3' -> key: 'BTCUSDT_ë‹¨ê¸°_lstm'
    (ì‹¬ë³¼_ì „ëµ_ëª¨ë¸ ë‹¨ìœ„ë¡œ ë²„í‚·íŒ…)
    """
    parts = stem.split("_")
    return "_".join(parts[:3]) if len(parts) >= 3 else stem

# ----------------- ì‚­ì œ/ë³´ì¡´ ë¡œì§ -----------------
def _delete_old_by_days(paths, cutoff_dt, deleted_log, accept_all=False):
    for d in paths:
        for p in _list_files(d):
            if not os.path.isfile(p) or _is_lock_file(p):
                continue
            if not accept_all and not _should_delete_file(p):
                continue
            if _is_recent(p, PROTECT_HOURS):
                continue
            try:
                mtime = datetime.fromtimestamp(os.path.getmtime(p))
            except Exception:
                continue
            if mtime < cutoff_dt:
                _delete_file(p, deleted_log)

def _delete_until_target(deleted_log, target_gb):
    candidates = []
    # LOG/MODEL
    for d in [LOG_DIR, MODEL_DIR]:
        for p in _list_files(d):
            if os.path.isfile(p) and not _is_lock_file(p) and _should_delete_file(p):
                if _is_recent(p, PROTECT_HOURS):
                    continue
                try:
                    ctime = os.path.getctime(p)
                except Exception:
                    ctime = 0
                candidates.append((ctime, p))
    # SSL: ëŒ€ìš©ëŸ‰ ìš°ì„  ì œê±°
    for p in _list_files(SSL_DIR):
        if os.path.isfile(p) and not _is_lock_file(p) and not _is_recent(p, PROTECT_HOURS):
            try:
                ctime = os.path.getctime(p)
            except Exception:
                ctime = 0
            candidates.append((ctime, p))

    candidates.sort(key=lambda x: x[0])  # ì˜¤ë˜ëœ ê²ƒë¶€í„°
    while get_directory_size_gb(ROOT_DIR) > target_gb and candidates:
        _, p = candidates.pop(0)
        _delete_file(p, deleted_log)

def _limit_models_per_key(deleted_log):
    """
    ëª¨ë¸/ë©”íƒ€ë¥¼ 'ì„¸íŠ¸'ë¡œ ë¬¶ì–´ ì‹¬ë³¼_ì „ëµ_ëª¨ë¸ ë‹¨ìœ„ë¡œ MAX_MODELS_PER_KEY 'ì„¸íŠ¸'ë§Œ ë‚¨ê¸´ë‹¤.
    - ì„¸íŠ¸ ë‚´ ê°€ì¤‘ì¹˜ëŠ” ì„ í˜¸ í™•ì¥ì 1ê°œë§Œ ìœ ì§€(.ptz > .safetensors > .pt)
    - ì„¸íŠ¸ ì™¸ ë‚˜ë¨¸ì§€ ê°€ì¤‘ì¹˜/ë©”íƒ€ëŠ” ì‚­ì œ
    - ì „ì²´ ìƒí•œ(MAX_MODELS_KEEP_GLOBAL)ë„ ì„¸íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì ìš©
    """
    sets = _collect_model_sets()
    if not sets:
        return

    items = [(stem, data) for stem, data in sets.items()]
    items.sort(key=lambda x: x[1]["mtime"], reverse=True)

    # ê¸€ë¡œë²Œ ìƒí•œ
    if len(items) > MAX_MODELS_KEEP_GLOBAL:
        for stem, data in items[MAX_MODELS_KEEP_GLOBAL:]:
            for wpath in data["weights"].values():
                _delete_file(wpath, deleted_log)
            if data["meta"]:
                _delete_file(data["meta"], deleted_log)
        items = items[:MAX_MODELS_KEEP_GLOBAL]

    # ë²„í‚· ìƒí•œ
    from collections import defaultdict
    buckets = defaultdict(list)
    for stem, data in items:
        buckets[_key_from_stem(stem)].append((stem, data))

    for key, arr in buckets.items():
        arr.sort(key=lambda x: x[1]["mtime"], reverse=True)
        keep = arr[:MAX_MODELS_PER_KEY]
        drop = arr[MAX_MODELS_PER_KEY:]

        # keep: ê°€ì¤‘ì¹˜ 1ê°œë§Œ ìœ ì§€
        for stem, data in keep:
            chosen = None
            for ext in _PREFERRED_WEIGHT_EXTS:
                if ext in data["weights"]:
                    chosen = ext
                    break
            for ext, wpath in list(data["weights"].items()):
                if chosen is not None and ext == chosen:
                    continue
                _delete_file(wpath, deleted_log)
            # ë©”íƒ€ëŠ” ë³´ì¡´

        # drop: ì „ë¶€ ì‚­ì œ
        for stem, data in drop:
            for wpath in data["weights"].values():
                _delete_file(wpath, deleted_log)
            if data["meta"]:
                _delete_file(data["meta"], deleted_log)

def _vacuum_sqlite():
    targets = []
    for base in [ROOT_DIR, LOG_DIR]:
        for f in _list_files(base):
            if os.path.isfile(f) and f.lower().endswith(".db") and not _is_lock_file(f):
                targets.append(f)
    for path in targets:
        try:
            import sqlite3
            con = sqlite3.connect(path)
            con.execute("VACUUM;")
            con.close()
            print(f"[VACUUM] {os.path.basename(path)} ì™„ë£Œ")
        except Exception as e:
            print(f"[ê²½ê³ ] VACUUM ì‹¤íŒ¨: {path} | {e}")

def _locked_by_runtime() -> bool:
    # ğŸ”’ LOCK_DIR ë‚´ë¶€ë‚˜ *.lock ì´ ë³´ì´ë©´ ì •ë¦¬ ì¤‘ë‹¨
    if os.path.exists(LOCK_PATH):
        print(f"[â›” ì¤‘ë‹¨] LOCK ë°œê²¬: {LOCK_PATH}")
        return True
    try:
        for f in _list_files(LOCK_DIR):
            if f.endswith(LOCK_SUFFIX):
                print(f"[â›” ì¤‘ë‹¨] LOCK ë°œê²¬: {f}")
                return True
    except Exception:
        pass
    return False

# ========= ğŸ†˜ EMERGENCY PURGE =========
def emergency_purge(target_gb=None):
    """
    ë””ìŠ¤í¬ê°€ ê½‰ ì°¼ì„ ë•Œ ì¦‰ì‹œ ìš©ëŸ‰ í™•ë³´.
    - ì ‘ë‘ì‚¬/ë³´í˜¸ì‹œê°„ ë¬´ì‹œ
    - ssl_models â†’ models â†’ logs ìˆœì„œ
    - ì˜¤ë˜ëœ íŒŒì¼ë¶€í„° ì‚­ì œ
    - ğŸ”’ ì–´ë–¤ ê²½ìš°ì—ë„ .lock/LOCK_DIR ì€ ì‚­ì œí•˜ì§€ ì•ŠìŒ
    - target_gb ë¯¸ì§€ì •: max(SOFT_CAP_GB, HARD_CAP_GB - MIN_FREE_GB)
    """
    if SAFE_MODE:
        return  # no-op
    _ensure_dirs()
    deleted = []
    target = target_gb or max(SOFT_CAP_GB, HARD_CAP_GB - MIN_FREE_GB)

    def _collect_all(dirpath):
        items = []
        for p in _list_files(dirpath):
            if not os.path.isfile(p) or _is_lock_file(p):
                continue
            if os.path.basename(p) == "deleted_log.txt":
                continue
            try:
                mtime = os.path.getmtime(p)
            except Exception:
                mtime = 0
            items.append((mtime, p))
        items.sort(key=lambda x: x[0])  # ì˜¤ë˜ëœ ê²ƒ ë¨¼ì €
        return [p for _, p in items]

    print("[ğŸ†˜ EMERGENCY] ì¦‰ì‹œ ê°•ì œ ì •ë¦¬ ì‹œì‘ (ë½ ë³´í˜¸ ìœ ì§€)")
    ordered_dirs = [SSL_DIR, MODEL_DIR, LOG_DIR]
    candidates = []
    for d in ordered_dirs:
        candidates.extend(_collect_all(d))

    while get_directory_size_gb(ROOT_DIR) > target and candidates:
        p = candidates.pop(0)
        _delete_file(p, deleted)

    _vacuum_sqlite()

    if deleted:
        now = datetime.now()
        try:
            os.makedirs(LOG_DIR, exist_ok=True)
            with open(DELETED_LOG_PATH, "a", encoding="utf-8") as f:
                f.write(f"[{now.strftime('%Y-%m-%d %H:%M:%S')}] [EMERGENCY] ì‚­ì œ íŒŒì¼:\n")
                for path in deleted:
                    f.write(f"  - {path}\n")
            print(f"[ğŸ†˜ EMERGENCY] ì´ {len(deleted)}ê°œ íŒŒì¼ ì‚­ì œ")
        except Exception as e:
            print(f"[âš ï¸ EMERGENCY ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨] â†’ {e}")
            print(f"[ğŸ†˜ EMERGENCY] ì´ {len(deleted)}ê°œ íŒŒì¼ ì‚­ì œ(ë¡œê·¸ ê¸°ë¡ ìƒëµ)")

def run_emergency_purge():
    """ì•±ì—ì„œ í•œ ì¤„ë¡œ í˜¸ì¶œí•˜ê¸° ìœ„í•œ ë˜í¼"""
    if SAFE_MODE:
        return 0  # no-op
    emergency_purge()
    return 0

# ========= ì¼ë°˜ ì£¼ê¸° ì •ë¦¬ =========
def auto_delete_old_logs():
    if SAFE_MODE:
        return  # no-op
    _ensure_dirs()

    if _locked_by_runtime():
        return

    now = datetime.now()
    cutoff = now - timedelta(days=KEEP_DAYS)
    deleted = []

    # ğŸ†• 0) ssl_models ë¨¼ì € ì •ë¦¬(ì•ˆì „: ìºì‹œ) â€” ìµœì‹  Nê°œ + ì†Œí”„íŠ¸ìº¡
    cleanup_ssl_models(keep_per_key=SSL_KEEP_PER_KEY, soft_cap_gb=SSL_SOFT_CAP_GB, deleted_log=deleted)

    current_gb = get_directory_size_gb(ROOT_DIR)
    print(f"[ìš©ëŸ‰] í˜„ì¬={_human_gb(current_gb)} | íŠ¸ë¦¬ê±°={_human_gb(TRIGGER_GB)} | ëª©í‘œ={_human_gb(SOFT_CAP_GB)} | í•˜ë“œìº¡={_human_gb(HARD_CAP_GB)}")

    # CSV ë¡¤ì˜¤ë²„
    for csv_path in ROOT_CSVS + [os.path.join(LOG_DIR, n) for n in ["prediction_log.csv", "train_log.csv", "evaluation_result.csv", "wrong_predictions.csv"]]:
        deleted += _rollover_csv(csv_path, CSV_MAX_MB, CSV_BACKUPS)

    if current_gb >= HARD_CAP_GB:
        print(f"[ğŸš¨ í•˜ë“œìº¡ ì´ˆê³¼] ì¦‰ì‹œ ê°•ì œ ì •ë¦¬ ì‹œì‘")
        _delete_old_by_days([SSL_DIR],  cutoff, deleted_log=deleted, accept_all=True)
        _delete_old_by_days([MODEL_DIR, LOG_DIR], cutoff, deleted_log=deleted)
        _delete_until_target(deleted, max(SOFT_CAP_GB, HARD_CAP_GB - MIN_FREE_GB))
        _limit_models_per_key(deleted)
        # ë§ˆë¬´ë¦¬ë¡œ ssl ì¬ì ê²€(ìº¡ ìœ ì§€)
        cleanup_ssl_models(keep_per_key=SSL_KEEP_PER_KEY, soft_cap_gb=SSL_SOFT_CAP_GB, deleted_log=deleted)
        _vacuum_sqlite()

    elif current_gb >= TRIGGER_GB:
        print(f"[âš ï¸ íŠ¸ë¦¬ê±° ì´ˆê³¼] ì •ë¦¬ ì‹œì‘")
        _delete_old_by_days([SSL_DIR],  cutoff, deleted_log=deleted, accept_all=True)
        _delete_old_by_days([MODEL_DIR, LOG_DIR], cutoff, deleted_log=deleted)
        _delete_until_target(deleted, SOFT_CAP_GB)
        _limit_models_per_key(deleted)
        # ë§ˆë¬´ë¦¬ë¡œ ssl ì¬ì ê²€(ìº¡ ìœ ì§€)
        cleanup_ssl_models(keep_per_key=SSL_KEEP_PER_KEY, soft_cap_gb=SSL_SOFT_CAP_GB, deleted_log=deleted)
        _vacuum_sqlite()
    else:
        print(f"[âœ… ìš©ëŸ‰ì •ìƒ] ì •ë¦¬ ë¶ˆí•„ìš”")
        _limit_models_per_key(deleted)
        # ì •ìƒ ìƒíƒœì—ì„œë„ ssl í´ë”ëŠ” ì–Œì „í•˜ê²Œ ìœ ì§€
        cleanup_ssl_models(keep_per_key=SSL_KEEP_PER_KEY, soft_cap_gb=SSL_SOFT_CAP_GB, deleted_log=deleted)

    if deleted:
        try:
            os.makedirs(LOG_DIR, exist_ok=True)
            with open(DELETED_LOG_PATH, "a", encoding="utf-8") as f:
                f.write(f"[{now.strftime('%Y-%m-%d %H:%M:%S')}] ì‚­ì œëœ íŒŒì¼ ëª©ë¡:\n")
                for path in deleted:
                    f.write(f"  - {path}\n")
            print(f"[ğŸ§¹ ì‚­ì œ ì™„ë£Œ] ì´ {len(deleted)}ê°œ íŒŒì¼ ì •ë¦¬")
        except Exception as e:
            print(f"[âš ï¸ ì‚­ì œ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨] â†’ {e}")
            print(f"[ğŸ§¹ ì‚­ì œ ì™„ë£Œ] ì´ {len(deleted)}ê°œ íŒŒì¼ ì •ë¦¬(ë¡œê·¸ ê¸°ë¡ ìƒëµ)")

def cleanup_logs_and_models():
    if SAFE_MODE:
        return  # no-op
    auto_delete_old_logs()

# ====== ê²½ëŸ‰/ì£¼ê¸° ì‹¤í–‰ ìœ í‹¸ ======
# minutes â†’ seconds (render.yamlì—ì„œ SAFE_CLEANUP_INTERVAL_MIN ì‚¬ìš©)
INTERVAL_SEC = _env_int("SAFE_CLEANUP_INTERVAL_MIN", 5) * 60
RUN_ON_START = _env_bool("SAFE_CLEANUP_RUN_ON_START", True)
_VERBOSE = _env_bool("SAFE_CLEANUP_VERBOSE", True)

def _log(msg: str):
    if _VERBOSE:
        print(f"[safe_cleanup] {msg}")

def _light_cleanup():
    if SAFE_MODE:
        return  # no-op
    try:
        from cache import CacheManager
        try:
            before = CacheManager.stats()
        except Exception:
            before = None
        pruned = CacheManager.prune()
        try:
            after = CacheManager.stats()
        except Exception:
            after = None
        _log(f"cache prune ok: before={before}, after={after}, pruned={pruned}")
    except Exception:
        pass
    try:
        gc.collect()
    except Exception:
        pass
    # ğŸ†• ê²½ëŸ‰ í´ë¦°ì—ë„ ssl ìŠ¬ë¦¼í™” í•œ ë²ˆ
    try:
        cleanup_ssl_models(keep_per_key=SSL_KEEP_PER_KEY, soft_cap_gb=SSL_SOFT_CAP_GB, deleted_log=[])
    except Exception as e:
        _log(f"ssl light cleanup skip: {e}")

def start_cleanup_scheduler(daemon: bool = True) -> threading.Thread:
    if SAFE_MODE:
        _log("SAFE_MODE=1 â†’ scheduler ì‹œì‘ ì•ˆ í•¨")
        # ë”ë¯¸ ìŠ¤ë ˆë“œ í•¸ë“¤ ë°˜í™˜
        t = threading.Thread(target=lambda: None, name="safe-cleanup-scheduler-dummy", daemon=daemon)
        return t

    def _loop():
        if RUN_ON_START:
            _log("ì´ˆê¸° 1íšŒ ì‹¤í–‰")
            auto_delete_old_logs()
        while True:
            time.sleep(INTERVAL_SEC)
            _log("ì£¼ê¸° ì‹¤í–‰")
            auto_delete_old_logs()
    t = threading.Thread(target=_loop, name="safe-cleanup-scheduler", daemon=daemon)
    t.start()
    _log(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘(ì£¼ê¸° {INTERVAL_SEC}s, daemon={daemon})")
    return t

def trigger_light_cleanup():
    if SAFE_MODE:
        return  # no-op
    _light_cleanup()

if __name__ == "__main__":
    if SAFE_MODE:
        print("[safe_cleanup] SAFE_MODE=1 â†’ ë©”ì¸ ì§„ì… ë¬´ì‹œ")
    else:
        start_cleanup_scheduler(daemon=False)
