# safe_cleanup.py (FIXED-CONFIG: env ÏóÜÏù¥ ÎèôÏûë, Ïä§ÏºÄÏ§ÑÎü¨ Ìè¨Ìï® / micro-fix3, 10GB ÏÑúÎ≤ÑÏö© ÌäúÎãù + Î™®Îç∏¬∑Î©îÌÉÄ ÏÑ∏Ìä∏ Ï†ïÎ¶¨ Í∞ïÌôî)
import os
import time
import threading
import gc
from datetime import datetime, timedelta

# ====== Í∏∞Î≥∏ Í≤ΩÎ°ú (env ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Í∏∞Î≥∏) ======
ROOT_DIR = os.getenv("PERSIST_ROOT", "/persistent")
LOG_DIR = os.path.join(ROOT_DIR, "logs")
MODEL_DIR = os.path.join(ROOT_DIR, "models")
SSL_DIR = os.path.join(ROOT_DIR, "ssl_models")
LOCK_DIR = os.path.join(ROOT_DIR, "locks")
DELETED_LOG_PATH = os.path.join(LOG_DIR, "deleted_log.txt")

# ====== Ï†ïÏ±Ö(Í≥†Ï†ïÍ∞í / 10GB ÌôòÍ≤Ω ÏµúÏ†ÅÌôî) ======
KEEP_DAYS   = 1
HARD_CAP_GB = 9.6   # 10GB ÌïúÍ≥Ñ ÎåÄÎπÑ Ïó¨Ïú†
SOFT_CAP_GB = 9.0
TRIGGER_GB  = 7.5   # Ïó¨Ïú† ÌôïÎ≥¥Î•º ÏúÑÌï¥ ÏïΩÍ∞Ñ ÏÉÅÌñ•(7.0‚Üí7.5)
MIN_FREE_GB = 0.8   # ÌïòÎìúÏ∫° Ìï¥Ï†ú ÌõÑ ÏµúÏÜå ÌôïÎ≥¥ Î™©Ìëú

CSV_MAX_MB = 50
CSV_BACKUPS = 3

MAX_MODELS_KEEP_GLOBAL = 200
MAX_MODELS_PER_KEY = 2

PROTECT_HOURS = 12
LOCK_PATH = os.path.join(LOCK_DIR, "train_or_predict.lock")
DRYRUN = False

# ‚úÖ (5Î≤à) ÏïïÏ∂ï Î™®Îç∏ ÌôïÏû•ÏûêÎèÑ ÎèôÏùº Ï∑®Í∏â
MODEL_EXTS = (".pt", ".ptz", ".safetensors")
META_EXT = ".meta.json"
_PREFERRED_WEIGHT_EXTS = (".ptz", ".safetensors", ".pt")  # Î≥¥Ï°¥ Ïö∞ÏÑ†ÏàúÏúÑ

DELETE_PREFIXES = ["prediction_", "evaluation_", "wrong_", "model_", "ssl_", "meta_", "evo_"]
EXCLUDE_FILES = {
    "prediction_log.csv", "train_log.csv", "evaluation_result.csv",
    "deleted_log.txt", "wrong_predictions.csv", "fine_tune_target.csv"
}

ROOT_CSVS = [
    os.path.join(ROOT_DIR, "prediction_log.csv"),
    os.path.join(ROOT_DIR, "wrong_predictions.csv"),
    os.path.join(ROOT_DIR, "evaluation_result.csv"),
    os.path.join(ROOT_DIR, "train_log.csv"),
]

# ----------------- Í≥µÌÜµ Ïú†Ìã∏ -----------------
def _size_bytes(path: str) -> int:
    try:
        return os.path.getsize(path)
    except Exception:
        return 0

def get_directory_size_gb(path):
    if not os.path.isdir(path):
        return 0.0
    total = 0
    for dirpath, _, filenames in os.walk(path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            if os.path.isfile(fp):
                total += _size_bytes(fp)
    return total / (1024 ** 3)

def _human_gb(v): return f"{v:.2f}GB"

def _list_files(dir_path):
    try:
        return [os.path.join(dir_path, f) for f in os.listdir(dir_path)]
    except Exception:
        return []

def _ensure_dirs():
    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(MODEL_DIR, exist_ok=True)
    os.makedirs(SSL_DIR, exist_ok=True)
    os.makedirs(LOCK_DIR, exist_ok=True)

def _is_within(child: str, parent: str) -> bool:
    try:
        child_abs  = os.path.abspath(child)
        parent_abs = os.path.abspath(parent)
        return os.path.commonpath([child_abs, parent_abs]) == parent_abs
    except Exception:
        return False

def _is_model_file(path: str) -> bool:
    """models/ ÎÇ¥Î∂ÄÏùò .pt/.ptz/.safetensors Î∞è Ïßù Î©îÌÉÄÎ•º Î™®Îç∏Î°ú Î≥∏Îã§."""
    if not isinstance(path, str):
        return False
    base = os.path.basename(path)
    # Î™®Îç∏ Í∞ÄÏ§ëÏπò
    if any(base.endswith(ext) for ext in MODEL_EXTS):
        return True
    # Î©îÌÉÄ(Î™®Îç∏Í≥º ÏÑ∏Ìä∏)
    if base.endswith(META_EXT):
        return True
    return False

def _should_delete_file(fname: str) -> bool:
    """
    Í∏∞Ï°¥ Í∑úÏπô + (NEW) models/ ÏïàÏùò Î™®Îç∏ ÌôïÏû•ÏûêÎäî Ï†ëÎëêÏÇ¨ ÏóÜÏù¥ÎèÑ Ï†ïÎ¶¨ ÎåÄÏÉÅÏúºÎ°ú Ïù∏Ï†ï.
    """
    base = os.path.basename(fname)
    # Î≥¥Ìò∏ Î™©Î°ù
    if base in EXCLUDE_FILES:
        return False
    # models/ ÎîîÎ†âÌÜ†Î¶¨Ïùò Î™®Îç∏/Î©îÌÉÄ ÌååÏùºÏùÄ Ï†ëÎëêÏÇ¨ÏôÄ Î¨¥Í¥ÄÌïòÍ≤å ÏÇ≠Ï†ú ÌõÑÎ≥¥
    try:
        if _is_within(fname, MODEL_DIR):
            if _is_model_file(fname):
                return True
    except Exception:
        pass
    # ÏùºÎ∞ò Ï†ëÎëêÏÇ¨ Í∑úÏπô
    return any(base.startswith(p) for p in DELETE_PREFIXES)

def _is_recent(path: str, hours: float) -> bool:
    try:
        mtime = datetime.fromtimestamp(os.path.getmtime(path))
        return (datetime.now() - mtime) < timedelta(hours=hours)
    except Exception:
        return False

def _rollover_csv(path: str, max_mb: int, backups: int):
    if not os.path.isfile(path):
        return []
    size_mb = _size_bytes(path) / (1024 ** 2)
    if size_mb <= max_mb:
        return []
    deleted = []
    for i in range(backups, 0, -1):
        bak = f"{path}.{i}"
        older = f"{path}.{i+1}"
        if os.path.exists(older):
            if not DRYRUN:
                os.remove(older)
            deleted.append(older)
        if os.path.exists(bak):
            if not DRYRUN:
                os.rename(bak, older)
    if not DRYRUN:
        os.rename(path, f"{path}.1")
        open(path, "w", encoding="utf-8").close()
    return deleted

def _delete_file(path: str, deleted_log: list):
    try:
        if DRYRUN:
            print(f"[DRYRUN] ÏÇ≠Ï†ú ÏòàÏ†ï: {path}")
            return
        os.remove(path)
        deleted_log.append(path)
        print(f"[üóë ÏÇ≠Ï†ú] {path}")
    except Exception as e:
        print(f"[Í≤ΩÍ≥†] ÏÇ≠Ï†ú Ïã§Ìå®: {path} | {e}")

# ----------------- Î™®Îç∏¬∑Î©îÌÉÄ ÏÑ∏Ìä∏ Í¥ÄÎ¶¨ -----------------
def _split_stem_and_ext(path: str):
    """
    returns: (stem, ext)
      - meta: "xxx.meta.json" -> ("xxx", ".meta.json")
      - weight: "xxx.ptz" -> ("xxx", ".ptz")
    """
    base = os.path.basename(path)
    if base.endswith(".meta.json"):
        stem = base[:-10]  # drop ".meta.json"
        ext = ".meta.json"
        return stem, ext
    root, ext = os.path.splitext(base)
    return root, ext

def _collect_model_sets():
    """
    models/ ÎÇ¥ ÌååÏùºÎì§ÏùÑ stem Í∏∞Ï§ÄÏúºÎ°ú ÏÑ∏Ìä∏(Í∞ÄÏ§ëÏπò+Î©îÌÉÄ)Î°ú Î¨∂Ïñ¥ÏÑú Î∞òÌôò.
    return: dict[stem] = {"weights": {ext: path}, "meta": path|None, "mtime": latest_mtime}
    (Î≥¥Ìò∏ÏãúÍ∞ÑÏóê Ìï¥ÎãπÌïòÎäî ÏµúÏã† ÌååÏùºÏùÄ Ï†úÏô∏ÌïòÏó¨ ÏÇ≠Ï†ú ÌõÑÎ≥¥Îßå ÎåÄÏÉÅÏúºÎ°ú Ìï®)
    """
    sets = {}
    for p in _list_files(MODEL_DIR):
        if not os.path.isfile(p):
            continue
        if not _is_model_file(p):
            continue
        if _is_recent(p, PROTECT_HOURS):
            continue
        stem, ext = _split_stem_and_ext(p)
        st = sets.setdefault(stem, {"weights": {}, "meta": None, "mtime": 0.0})
        try:
            mtime = os.path.getmtime(p)
        except Exception:
            mtime = 0.0
        st["mtime"] = max(st["mtime"], mtime)
        if ext == ".meta.json":
            st["meta"] = p
        else:
            st["weights"][ext] = p
    return sets

def _key_from_stem(stem: str) -> str:
    """
    stem: 'BTCUSDT_Îã®Í∏∞_lstm_group1_cls3' -> key: 'BTCUSDT_Îã®Í∏∞_lstm'
    (Ïã¨Î≥º_Ï†ÑÎûµ_Î™®Îç∏ Îã®ÏúÑÎ°ú Î≤ÑÌÇ∑ÌåÖ)
    """
    parts = stem.split("_")
    return "_".join(parts[:3]) if len(parts) >= 3 else stem

# ----------------- ÏÇ≠Ï†ú/Î≥¥Ï°¥ Î°úÏßÅ -----------------
def _delete_old_by_days(paths, cutoff_dt, deleted_log, accept_all=False):
    for d in paths:
        for p in _list_files(d):
            if not os.path.isfile(p):
                continue
            if not accept_all and not _should_delete_file(p):
                continue
            if _is_recent(p, PROTECT_HOURS):
                continue
            try:
                mtime = datetime.fromtimestamp(os.path.getmtime(p))
            except Exception:
                continue
            if mtime < cutoff_dt:
                _delete_file(p, deleted_log)

def _delete_until_target(deleted_log, target_gb):
    candidates = []
    # LOG/MODEL: Í∑úÏπô Í∏∞Î∞ò ÌõÑÎ≥¥ ÏàòÏßë
    for d in [LOG_DIR, MODEL_DIR]:
        for p in _list_files(d):
            if os.path.isfile(p) and _should_delete_file(p):
                if _is_recent(p, PROTECT_HOURS):
                    continue
                try:
                    ctime = os.path.getctime(p)
                except Exception:
                    ctime = 0
                candidates.append((ctime, p))
    # SSL: ÎåÄÏö©Îüâ Ï∫êÏãú Ïö∞ÏÑ† Ï†úÍ±∞
    for p in _list_files(SSL_DIR):
        if os.path.isfile(p) and not _is_recent(p, PROTECT_HOURS):
            try:
                ctime = os.path.getctime(p)
            except Exception:
                ctime = 0
            candidates.append((ctime, p))

    # Ïò§ÎûòÎêú Í≤ÉÎ∂ÄÌÑ∞
    candidates.sort(key=lambda x: x[0])
    while get_directory_size_gb(ROOT_DIR) > target_gb and candidates:
        _, p = candidates.pop(0)
        _delete_file(p, deleted_log)

def _limit_models_per_key(deleted_log):
    """
    Î™®Îç∏/Î©îÌÉÄÎ•º 'ÏÑ∏Ìä∏'Î°ú Î¨∂Ïñ¥ Ïã¨Î≥º_Ï†ÑÎûµ_Î™®Îç∏ Îã®ÏúÑÎ°ú MAX_MODELS_PER_KEY 'ÏÑ∏Ìä∏'Îßå ÎÇ®Í∏¥Îã§.
    - ÏÑ∏Ìä∏ ÎÇ¥ Í∞ÄÏ§ëÏπòÎäî ÏÑ†Ìò∏ ÌôïÏû•Ïûê 1Í∞úÎßå Ïú†ÏßÄ(.ptz > .safetensors > .pt)
    - ÏÑ∏Ìä∏ Ïô∏ ÎÇòÎ®∏ÏßÄ Í∞ÄÏ§ëÏπò/Î©îÌÉÄÎäî ÏÇ≠Ï†ú
    - Ï†ÑÏ≤¥ ÏÉÅÌïú(MAX_MODELS_KEEP_GLOBAL)ÎèÑ ÏÑ∏Ìä∏ Í∏∞Ï§ÄÏúºÎ°ú Ï†ÅÏö©
    """
    sets = _collect_model_sets()
    if not sets:
        return

    # ÏÑ∏Ìä∏ Îã®ÏúÑÎ°ú Ï†ïÎ†¨(ÏµúÏã† mtime Ïö∞ÏÑ†)
    items = [(stem, data) for stem, data in sets.items()]
    items.sort(key=lambda x: x[1]["mtime"], reverse=True)

    # Í∏ÄÎ°úÎ≤å ÏÉÅÌïú: Ï¥àÍ≥º ÏÑ∏Ìä∏Îäî Ï†ÑÎ∂Ä ÏÇ≠Ï†ú
    if len(items) > MAX_MODELS_KEEP_GLOBAL:
        for stem, data in items[MAX_MODELS_KEEP_GLOBAL:]:
            for wpath in data["weights"].values():
                _delete_file(wpath, deleted_log)
            if data["meta"]:
                _delete_file(data["meta"], deleted_log)
        items = items[:MAX_MODELS_KEEP_GLOBAL]

    # Î≤ÑÌÇ∑(Ïã¨Î≥º_Ï†ÑÎûµ_Î™®Îç∏)Î≥Ñ ÏÉÅÌïú Ï†ÅÏö©
    from collections import defaultdict
    buckets = defaultdict(list)
    for stem, data in items:
        buckets[_key_from_stem(stem)].append((stem, data))

    for key, arr in buckets.items():
        # ÏµúÏã† ÏÑ∏Ìä∏ MAX_MODELS_PER_KEYÍ∞úÎßå Î≥¥Ï°¥, ÎÇòÎ®∏ÏßÄÎäî ÏÇ≠Ï†ú
        arr.sort(key=lambda x: x[1]["mtime"], reverse=True)
        keep = arr[:MAX_MODELS_PER_KEY]
        drop = arr[MAX_MODELS_PER_KEY:]

        # Î≥¥Ï°¥ ÏÑ∏Ìä∏: Í∞ÄÏ§ëÏπòÎäî ÏÑ†Ìò∏ ÌôïÏû•Ïûê 1Í∞úÎßå Ïú†ÏßÄ(+Î©îÌÉÄÎäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ)
        for stem, data in keep:
            chosen = None
            for ext in _PREFERRED_WEIGHT_EXTS:
                if ext in data["weights"]:
                    chosen = ext
                    break
            for ext, wpath in list(data["weights"].items()):
                if chosen is not None and ext == chosen:
                    continue
                _delete_file(wpath, deleted_log)
            # Î©îÌÉÄÎäî ÏûàÏúºÎ©¥ Î≥¥Ï°¥ (ÏÇ≠Ï†úÌïòÏßÄ ÏïäÏùå)

        # ÎìúÎ°≠ ÏÑ∏Ìä∏: Ï†ÑÎ∂Ä ÏÇ≠Ï†ú(Í∞ÄÏ§ëÏπò+Î©îÌÉÄ)
        for stem, data in drop:
            for wpath in data["weights"].values():
                _delete_file(wpath, deleted_log)
            if data["meta"]:
                _delete_file(data["meta"], deleted_log)

def _vacuum_sqlite():
    targets = []
    for base in [ROOT_DIR, LOG_DIR]:
        for f in _list_files(base):
            if os.path.isfile(f) and f.lower().endswith(".db"):
                targets.append(f)
    for path in targets:
        try:
            import sqlite3
            con = sqlite3.connect(path)
            con.execute("VACUUM;")
            con.close()
            print(f"[VACUUM] {os.path.basename(path)} ÏôÑÎ£å")
        except Exception as e:
            print(f"[Í≤ΩÍ≥†] VACUUM Ïã§Ìå®: {path} | {e}")

def _locked_by_runtime() -> bool:
    if os.path.exists(LOCK_PATH):
        print(f"[‚õî Ï§ëÎã®] LOCK Î∞úÍ≤¨: {LOCK_PATH}")
        return True
    try:
        for f in _list_files(LOCK_DIR):
            if f.endswith(".lock"):
                print(f"[‚õî Ï§ëÎã®] LOCK Î∞úÍ≤¨: {f}")
                return True
    except Exception:
        pass
    return False

# ========= üÜò EMERGENCY PURGE (Ï†ëÎëêÏÇ¨/Î≥¥Ìò∏ÏãúÍ∞Ñ/ÎùΩ Î¨¥Ïãú) =========
def emergency_purge(target_gb=None):
    """
    ÎîîÏä§ÌÅ¨Í∞Ä ÍΩâ Ï∞ºÏùÑ Îïå Ï¶âÏãú Ïö©Îüâ ÌôïÎ≥¥.
    - Ï†ëÎëêÏÇ¨/Î≥¥Ìò∏ÏãúÍ∞Ñ/ÎùΩ Ï°∞Í±¥ Î¨¥Ïãú
    - ssl_models ‚Üí models ‚Üí logs ÏàúÏÑú
    - Ïò§ÎûòÎêú ÌååÏùºÎ∂ÄÌÑ∞ ÏÇ≠Ï†ú
    - target_gb ÎØ∏ÏßÄÏ†ï Ïãú: max(SOFT_CAP_GB, HARD_CAP_GB - MIN_FREE_GB)
    """
    _ensure_dirs()
    deleted = []
    target = target_gb or max(SOFT_CAP_GB, HARD_CAP_GB - MIN_FREE_GB)

    def _collect_all(dirpath):
        items = []
        for p in _list_files(dirpath):
            if not os.path.isfile(p):
                continue
            if os.path.basename(p) == "deleted_log.txt":
                continue
            try:
                mtime = os.path.getmtime(p)
            except Exception:
                mtime = 0
            items.append((mtime, p))
        items.sort(key=lambda x: x[0])  # Ïò§ÎûòÎêú Í≤É Î®ºÏ†Ä
        return [p for _, p in items]

    print("[üÜò EMERGENCY] Ï¶âÏãú Í∞ïÏ†ú Ï†ïÎ¶¨ ÏãúÏûë (ÎùΩ/Î≥¥Ìò∏ÏãúÍ∞Ñ Î¨¥Ïãú)")
    ordered_dirs = [SSL_DIR, MODEL_DIR, LOG_DIR]
    candidates = []
    for d in ordered_dirs:
        candidates.extend(_collect_all(d))

    while get_directory_size_gb(ROOT_DIR) > target and candidates:
        p = candidates.pop(0)
        _delete_file(p, deleted)

    _vacuum_sqlite()

    if deleted:
        now = datetime.now()
        try:
            os.makedirs(LOG_DIR, exist_ok=True)
            with open(DELETED_LOG_PATH, "a", encoding="utf-8") as f:
                f.write(f"[{now.strftime('%Y-%m-%d %H:%M:%S')}] [EMERGENCY] ÏÇ≠Ï†ú ÌååÏùº:\n")
                for path in deleted:
                    f.write(f"  - {path}\n")
            print(f"[üÜò EMERGENCY] Ï¥ù {len(deleted)}Í∞ú ÌååÏùº ÏÇ≠Ï†ú")
        except Exception as e:
            print(f"[‚ö†Ô∏è EMERGENCY Î°úÍ∑∏ Í∏∞Î°ù Ïã§Ìå®] ‚Üí {e}")
            print(f"[üÜò EMERGENCY] Ï¥ù {len(deleted)}Í∞ú ÌååÏùº ÏÇ≠Ï†ú(Î°úÍ∑∏ Í∏∞Î°ù ÏÉùÎûµ)")

def run_emergency_purge():
    """Ïï±ÏóêÏÑú Ìïú Ï§ÑÎ°ú Ìò∏Ï∂úÌïòÍ∏∞ ÏúÑÌïú ÎûòÌçº"""
    emergency_purge()

# ========= ÏùºÎ∞ò Ï£ºÍ∏∞ Ï†ïÎ¶¨ =========
def auto_delete_old_logs():
    _ensure_dirs()

    if _locked_by_runtime():
        return

    now = datetime.now()
    cutoff = now - timedelta(days=KEEP_DAYS)
    deleted = []

    current_gb = get_directory_size_gb(ROOT_DIR)
    print(f"[Ïö©Îüâ] ÌòÑÏû¨={_human_gb(current_gb)} | Ìä∏Î¶¨Í±∞={_human_gb(TRIGGER_GB)} | Î™©Ìëú={_human_gb(SOFT_CAP_GB)} | ÌïòÎìúÏ∫°={_human_gb(HARD_CAP_GB)}")

    # CSV Î°§Ïò§Î≤Ñ(Î®ºÏ†Ä Í≥µÍ∞Ñ Ï°∞Í∏à ÌôïÎ≥¥)
    for csv_path in ROOT_CSVS + [os.path.join(LOG_DIR, n) for n in ["prediction_log.csv", "train_log.csv", "evaluation_result.csv", "wrong_predictions.csv"]]:
        deleted += _rollover_csv(csv_path, CSV_MAX_MB, CSV_BACKUPS)

    if current_gb >= HARD_CAP_GB:
        print(f"[üö® ÌïòÎìúÏ∫° Ï¥àÍ≥º] Ï¶âÏãú Í∞ïÏ†ú Ï†ïÎ¶¨ ÏãúÏûë")
        # 1) SSL(ÎåÄÏö©Îüâ) ‚Üí 2) Î™®Îç∏/Î°úÍ∑∏ Ïàú
        _delete_old_by_days([SSL_DIR],  cutoff, deleted_log=deleted, accept_all=True)
        _delete_old_by_days([MODEL_DIR, LOG_DIR], cutoff, deleted_log=deleted)
        _delete_until_target(deleted, max(SOFT_CAP_GB, HARD_CAP_GB - MIN_FREE_GB))
        _limit_models_per_key(deleted)
        _vacuum_sqlite()

    elif current_gb >= TRIGGER_GB:
        print(f"[‚ö†Ô∏è Ìä∏Î¶¨Í±∞ Ï¥àÍ≥º] Ï†ïÎ¶¨ ÏãúÏûë")
        _delete_old_by_days([SSL_DIR],  cutoff, deleted_log=deleted, accept_all=True)
        _delete_old_by_days([MODEL_DIR, LOG_DIR], cutoff, deleted_log=deleted)
        _delete_until_target(deleted, SOFT_CAP_GB)
        _limit_models_per_key(deleted)
        _vacuum_sqlite()
    else:
        print(f"[‚úÖ Ïö©ÎüâÏ†ïÏÉÅ] Ï†ïÎ¶¨ Î∂àÌïÑÏöî")
        _limit_models_per_key(deleted)

    if deleted:
        try:
            os.makedirs(LOG_DIR, exist_ok=True)
            with open(DELETED_LOG_PATH, "a", encoding="utf-8") as f:
                f.write(f"[{now.strftime('%Y-%m-%d %H:%M:%S')}] ÏÇ≠Ï†úÎêú ÌååÏùº Î™©Î°ù:\n")
                for path in deleted:
                    f.write(f"  - {path}\n")
            print(f"[üßπ ÏÇ≠Ï†ú ÏôÑÎ£å] Ï¥ù {len(deleted)}Í∞ú ÌååÏùº Ï†ïÎ¶¨")
        except Exception as e:
            print(f"[‚ö†Ô∏è ÏÇ≠Ï†ú Î°úÍ∑∏ Í∏∞Î°ù Ïã§Ìå®] ‚Üí {e}")
            print(f"[üßπ ÏÇ≠Ï†ú ÏôÑÎ£å] Ï¥ù {len(deleted)}Í∞ú ÌååÏùº Ï†ïÎ¶¨(Î°úÍ∑∏ Í∏∞Î°ù ÏÉùÎûµ)")

def cleanup_logs_and_models():
    auto_delete_old_logs()

# ====== Í≤ΩÎüâ/Ï£ºÍ∏∞ Ïã§Ìñâ Ïú†Ìã∏(Í≥†Ï†ïÍ∞í) ======
INTERVAL_SEC = 300
RUN_ON_START = True
_VERBOSE = True

def _log(msg: str):
    if _VERBOSE:
        print(f"[safe_cleanup] {msg}")

def _light_cleanup():
    try:
        from cache import CacheManager
        try:
            before = CacheManager.stats()
        except Exception:
            before = None
        pruned = CacheManager.prune()
        try:
            after = CacheManager.stats()
        except Exception:
            after = None
        _log(f"cache prune ok: before={before}, after={after}, pruned={pruned}")
    except Exception:
        pass
    try:
        gc.collect()
    except Exception:
        pass

def start_cleanup_scheduler(daemon: bool = True) -> threading.Thread:
    def _loop():
        if RUN_ON_START:
            _log("Ï¥àÍ∏∞ 1Ìöå Ïã§Ìñâ")
            auto_delete_old_logs()
        while True:
            time.sleep(INTERVAL_SEC)
            _log("Ï£ºÍ∏∞ Ïã§Ìñâ")
            auto_delete_old_logs()
    t = threading.Thread(target=_loop, name="safe-cleanup-scheduler", daemon=daemon)
    t.start()
    _log(f"Ïä§ÏºÄÏ§ÑÎü¨ ÏãúÏûë(Ï£ºÍ∏∞ {INTERVAL_SEC}s, daemon={daemon})")
    return t

def trigger_light_cleanup():
    _light_cleanup()

if __name__ == "__main__":
    start_cleanup_scheduler(daemon=False)
