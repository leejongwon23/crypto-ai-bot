# safe_cleanup.py
import os
import shutil
from datetime import datetime, timedelta

# ====== ê¸°ë³¸ ê²½ë¡œ ======
ROOT_DIR = os.getenv("PERSIST_ROOT", "/persistent")
LOG_DIR = os.path.join(ROOT_DIR, "logs")
MODEL_DIR = os.path.join(ROOT_DIR, "models")
DELETED_LOG_PATH = os.path.join(LOG_DIR, "deleted_log.txt")

# ====== ì •ì±…(í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥) ======
KEEP_DAYS = int(os.getenv("SAFE_KEEP_DAYS", "1"))                # ë©°ì¹  ì§€ë‚œ íŒŒì¼ë¶€í„° ìš°ì„  ì‚­ì œ
HARD_CAP_GB = float(os.getenv("SAFE_HARD_CAP_GB", "9.8"))        # ì ˆëŒ€ ìƒí•œì„ (ë„˜ìœ¼ë©´ ê°•ì œ ì •ë¦¬)
TRIGGER_GB = float(os.getenv("SAFE_TRIGGER_GB", "7.0"))          # ì´ ê°’ ì´ìƒì´ë©´ ì •ë¦¬ ì‹œì‘
SOFT_CAP_GB = float(os.getenv("SAFE_SOFT_CAP_GB", "9.2"))        # ì •ë¦¬ ëª©í‘œì¹˜(ì—¬ê¸°ê¹Œì§€ ë‚®ì¶”ê¸°)
MIN_FREE_GB = float(os.getenv("SAFE_MIN_FREE_GB", "1.0"))        # ìµœì†Œ ì—¬ìœ ê³µê°„ í™•ë³´ ëª©í‘œ

CSV_MAX_MB = int(os.getenv("SAFE_CSV_MAX_MB", "50"))             # ì´ˆê³¼í•˜ë©´ ë¡¤ì˜¤ë²„
CSV_BACKUPS = int(os.getenv("SAFE_CSV_BACKUPS", "3"))            # ë¡¤ì˜¤ë²„ ë³´ê´€ ê°œìˆ˜

MAX_MODELS_KEEP_GLOBAL = int(os.getenv("SAFE_MAX_MODELS_KEEP_GLOBAL", "200"))  # models ì „ì²´ ìƒí•œ
MAX_MODELS_PER_KEY = int(os.getenv("SAFE_MAX_MODELS_PER_KEY", "2"))           # ê°™ì€ í‚¤(ì‹¬ë³¼/ì „ëµ) ë³´ê´€ ê°œìˆ˜

DRYRUN = os.getenv("SAFE_CLEANUP_DRYRUN", "0") == "1"            # ì‚­ì œ ì‹œë„ë§Œ ë¡œê·¸(ì‹¤ì œ ì‚­ì œ X)

# ì‚­ì œ í›„ë³´ ì ‘ë‘ì‚¬ / ì œì™¸ íŒŒì¼
DELETE_PREFIXES = ["prediction_", "evaluation_", "wrong_", "model_", "ssl_", "meta_", "evo_"]
EXCLUDE_FILES = {
    "prediction_log.csv", "train_log.csv", "evaluation_result.csv",
    "deleted_log.txt", "wrong_predictions.csv", "fine_tune_target.csv"
}

def _size_bytes(path: str) -> int:
    try:
        return os.path.getsize(path)
    except Exception:
        return 0

def get_directory_size_gb(path):
    total = 0
    for dirpath, _, filenames in os.walk(path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            if os.path.isfile(fp):
                total += _size_bytes(fp)
    return total / (1024 ** 3)

def _human_gb(v): return f"{v:.2f}GB"

def _list_files(dir_path):
    try:
        return [os.path.join(dir_path, f) for f in os.listdir(dir_path)]
    except Exception:
        return []

def _ensure_dirs():
    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(MODEL_DIR, exist_ok=True)

def _should_delete_file(fname: str) -> bool:
    if os.path.basename(fname) in EXCLUDE_FILES:
        return False
    return any(os.path.basename(fname).startswith(p) for p in DELETE_PREFIXES)

def _rollover_csv(path: str, max_mb: int, backups: int):
    if not os.path.isfile(path):
        return []
    size_mb = _size_bytes(path) / (1024 ** 2)
    if size_mb <= max_mb:
        return []
    deleted = []
    # ì˜¤ë˜ëœ ë°±ì—…ë¶€í„° ì œê±°
    for i in range(backups, 0, -1):
        bak = f"{path}.{i}"
        older = f"{path}.{i+1}"
        if os.path.exists(older):
            if not DRYRUN:
                os.remove(older)
            deleted.append(older)
        if os.path.exists(bak):
            if not DRYRUN:
                os.rename(bak, older)
    # í˜„ì¬ íŒŒì¼ â†’ .1 ë¡œ íšŒì „
    if not DRYRUN:
        os.rename(path, f"{path}.1")
        open(path, "w", encoding="utf-8").close()
    return deleted

def _delete_file(path: str, deleted_log: list):
    try:
        if DRYRUN:
            print(f"[DRYRUN] ì‚­ì œ ì˜ˆì •: {path}")
            return
        os.remove(path)
        deleted_log.append(path)
        print(f"[ğŸ—‘ ì‚­ì œ] {path}")
    except Exception as e:
        print(f"[ê²½ê³ ] ì‚­ì œ ì‹¤íŒ¨: {path} | {e}")

def _delete_old_by_days(paths, cutoff_dt, deleted_log):
    for d in paths:
        for p in _list_files(d):
            if not os.path.isfile(p):
                continue
            if not _should_delete_file(p):
                continue
            try:
                mtime = datetime.fromtimestamp(os.path.getmtime(p))
            except Exception:
                continue
            if mtime < cutoff_dt:
                _delete_file(p, deleted_log)

def _delete_until_target(deleted_log, target_gb):
    # ì˜¤ë˜ëœ ìˆœìœ¼ë¡œ ì‚­ì œ
    candidates = []
    for d in [LOG_DIR, MODEL_DIR]:
        for p in _list_files(d):
            if os.path.isfile(p) and _should_delete_file(p):
                try:
                    ctime = os.path.getctime(p)
                except Exception:
                    ctime = 0
                candidates.append((ctime, p))
    candidates.sort(key=lambda x: x[0])  # ì˜¤ë˜ëœ ê²ƒë¶€í„°
    while get_directory_size_gb(ROOT_DIR) > target_gb and candidates:
        _, p = candidates.pop(0)
        _delete_file(p, deleted_log)

def _limit_models_per_key(deleted_log):
    # íŒŒì¼ëª… ê·œì¹™ ì˜ˆ: SYMBOL_ì „ëµ_ëª¨ë¸íƒ€ì…_groupX_clsY.pt / .meta.json
    files = [p for p in _list_files(MODEL_DIR) if os.path.isfile(p)]
    # ì „ì²´ ìƒí•œ ë¨¼ì €
    files.sort(key=os.path.getmtime, reverse=True)
    if len(files) > MAX_MODELS_KEEP_GLOBAL:
        for p in files[MAX_MODELS_KEEP_GLOBAL:]:
            _delete_file(p, deleted_log)

    # í‚¤ë³„ ìƒí•œ
    from collections import defaultdict
    buckets = defaultdict(list)
    for p in files:
        base = os.path.basename(p)
        # í‚¤ ì¶”ì¶œ(í™•ì¥ì ì œê±°)
        key = base.split(".")[0]
        # ì‹¬í”Œ í‚¤(ì‹¬ë³¼_ì „ëµ_ëª¨ë¸ëª…)ë§Œ ë‚¨ê¸°ê³  group/clsëŠ” ê°™ì´ ë¬¶ì„
        parts = key.split("_")
        if len(parts) >= 3:
            simple = "_".join(parts[:3])
        else:
            simple = key
        buckets[simple].append(p)

    for key, items in buckets.items():
        items.sort(key=os.path.getmtime, reverse=True)
        for p in items[MAX_MODELS_PER_KEY:]:
            _delete_file(p, deleted_log)

def _vacuum_sqlite():
    # sqlite íŒŒì¼ì´ ìˆë‹¤ë©´ VACUUM (ê³µê°„ íšŒìˆ˜)
    for fname in ["success_log.db", "failure_log.db"]:
        path = os.path.join(ROOT_DIR, fname)
        if not os.path.isfile(path):
            continue
        try:
            import sqlite3
            con = sqlite3.connect(path)
            con.execute("VACUUM;")
            con.close()
            print(f"[VACUUM] {fname} ì™„ë£Œ")
        except Exception as e:
            print(f"[ê²½ê³ ] VACUUM ì‹¤íŒ¨: {fname} | {e}")

def auto_delete_old_logs():
    _ensure_dirs()
    now = datetime.now()
    cutoff = now - timedelta(days=KEEP_DAYS)
    deleted = []

    current_gb = get_directory_size_gb(ROOT_DIR)
    print(f"[ìš©ëŸ‰] í˜„ì¬={_human_gb(current_gb)} | íŠ¸ë¦¬ê±°={_human_gb(TRIGGER_GB)} | ëª©í‘œ={_human_gb(SOFT_CAP_GB)} | í•˜ë“œìº¡={_human_gb(HARD_CAP_GB)}")

    # CSV ë¡œê·¸ ë¡¤ì˜¤ë²„(ë„ˆë¬´ í° CSVëŠ” íšŒì „)
    for csv_name in ["prediction_log.csv", "train_log.csv", "evaluation_result.csv", "wrong_predictions.csv"]:
        csv_path = os.path.join(LOG_DIR, csv_name)
        deleted += _rollover_csv(csv_path, CSV_MAX_MB, CSV_BACKUPS)

    # í•˜ë“œìº¡ ì´ˆê³¼ë©´ ì¦‰ì‹œ ê°•ì œ ì •ë¦¬
    if current_gb >= HARD_CAP_GB:
        print(f"[ğŸš¨ í•˜ë“œìº¡ ì´ˆê³¼] ì¦‰ì‹œ ê°•ì œ ì •ë¦¬ ì‹œì‘")
        _delete_old_by_days([LOG_DIR, MODEL_DIR], cutoff, deleted)
        _delete_until_target(deleted, max(SOFT_CAP_GB, HARD_CAP_GB - MIN_FREE_GB))
        _limit_models_per_key(deleted)
        _vacuum_sqlite()

    # íŠ¸ë¦¬ê±° ì´ˆê³¼ë©´ ì¼ë°˜ ì •ë¦¬
    elif current_gb >= TRIGGER_GB:
        print(f"[âš ï¸ íŠ¸ë¦¬ê±° ì´ˆê³¼] ì •ë¦¬ ì‹œì‘")
        _delete_old_by_days([LOG_DIR, MODEL_DIR], cutoff, deleted)
        _delete_until_target(deleted, SOFT_CAP_GB)
        _limit_models_per_key(deleted)
        _vacuum_sqlite()
    else:
        print(f"[âœ… ìš©ëŸ‰ì •ìƒ] ì •ë¦¬ ë¶ˆí•„ìš”")
        _limit_models_per_key(deleted)  # ì •ìƒì´ì–´ë„ ëª¨ë¸ ìƒí•œ ìœ ì§€

    # ì‚­ì œ ë¡œê·¸ ê¸°ë¡
    if deleted:
        try:
            with open(DELETED_LOG_PATH, "a", encoding="utf-8") as f:
                f.write(f"[{now.strftime('%Y-%m-%d %H:%M:%S')}] ì‚­ì œëœ íŒŒì¼ ëª©ë¡:\n")
                for path in deleted:
                    f.write(f"  - {path}\n")
            print(f"[ğŸ§¹ ì‚­ì œ ì™„ë£Œ] ì´ {len(deleted)}ê°œ íŒŒì¼ ì •ë¦¬")
        except Exception as e:
            print(f"[âš ï¸ ì‚­ì œ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨] â†’ {e}")
            print(f"[ğŸ§¹ ì‚­ì œ ì™„ë£Œ] ì´ {len(deleted)}ê°œ íŒŒì¼ ì •ë¦¬(ë¡œê·¸ ê¸°ë¡ ìƒëµ)")

# ì•±ì—ì„œ ë¶ˆëŸ¬ì“°ëŠ” ë˜í¼
def cleanup_logs_and_models():
    auto_delete_old_logs()
