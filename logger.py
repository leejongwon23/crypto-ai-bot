# === logger.py (ë©”ëª¨ë¦¬ ì•ˆì „: ë¡œí…Œì´ì…˜ + ì²­í¬ ì§‘ê³„ + ëª¨ë¸ëª… ì •ê·œí™” + ì‹¤íŒ¨DB ë…¸ì´ì¦ˆ ì°¨ë‹¨ + íŒŒì¼ë½ + ì»¨í…ìŠ¤íŠ¸ ë¶„ê¸° + í•™ìŠµì§€í‘œ í™•ì¥) ===
import os, csv, json, datetime, pandas as pd, pytz, hashlib, shutil, re
import sqlite3
from collections import defaultdict
import threading, time  # ë™ì‹œì„±/ì¬ì‹œë„
# [ADD] per-class F1 ì¶œë ¥ìš© (ì„ íƒ)
from sklearn.metrics import classification_report

# -------------------------
# ê¸°ë³¸ ê²½ë¡œ/ë””ë ‰í† ë¦¬
# -------------------------
DIR = "/persistent"
LOG_DIR = os.path.join(DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# prediction_logëŠ” ë£¨íŠ¸ ê²½ë¡œë¡œ í†µì¼
PREDICTION_LOG = f"{DIR}/prediction_log.csv"
WRONG = f"{DIR}/wrong_predictions.csv"
EVAL_RESULT = f"{LOG_DIR}/evaluation_result.csv"

# í•™ìŠµ ë¡œê·¸ íŒŒì¼ëª…
TRAIN_LOG = f"{LOG_DIR}/train_log.csv"
AUDIT_LOG = f"{LOG_DIR}/evaluation_audit.csv"

# ê³µìš© í—¤ë”(ê¸°ì¡´ + í™•ì¥ ì»¬ëŸ¼)
BASE_PRED_HEADERS = [
    "timestamp","symbol","strategy","direction",
    "entry_price","target_price",
    "model","predicted_class","top_k","note",
    "success","reason","rate","return_value",
    "label","group_id","model_symbol","model_name",
    "source","volatility","source_exchange"
]
EXTRA_PRED_HEADERS = ["regime","meta_choice","raw_prob","calib_prob","calib_ver"]
CLASS_RANGE_HEADERS = ["class_return_min","class_return_max","class_return_text"]
NOTE_EXTRACT_HEADERS = ["position","hint_allow_long","hint_allow_short","hint_slope","used_minret_filter","explore_used","hint_ma_fast","hint_ma_slow"]

PREDICTION_HEADERS = BASE_PRED_HEADERS + EXTRA_PRED_HEADERS + ["feature_vector"] + CLASS_RANGE_HEADERS + NOTE_EXTRACT_HEADERS

# âœ… í•™ìŠµ ë¡œê·¸ í™•ì¥ í—¤ë”(ê¸°ì¡´ accuracy/f1/lossë¥¼ ë³´ì¡´ + ì„¸ë¶€ ì§€í‘œ ì¶”ê°€)
TRAIN_HEADERS = [
    "timestamp","symbol","strategy","model",
    "val_acc","val_f1","train_loss_sum",
    "engine","window","recent_cap",
    "rows","limit","min","augment_needed","enough_for_training",
    "note","source_exchange","status"
]

# ì²­í¬ í¬ê¸° ê¸°ë³¸ê°’
CHUNK = 50_000

now_kst = lambda: datetime.datetime.now(pytz.timezone("Asia/Seoul"))

# -------------------------
# (NEW) ê°„ë‹¨ íŒŒì¼ë½ ìœ í‹¸ (ë©€í‹°í”„ë¡œì„¸ìŠ¤ ë™ì‹œì“°ê¸° ë³´í˜¸)
# -------------------------
_PRED_LOCK_PATH = PREDICTION_LOG + ".lock"
_LOCK_STALE_SEC = 120  # ê³ ì•„ ë½ ì •ë¦¬ ê¸°ì¤€

class _FileLock:
    def __init__(self, path: str, timeout: float = 10.0, poll: float = 0.05):
        self.path = path
        self.timeout = float(timeout)
        self.poll = float(poll)

    def __enter__(self):
        deadline = time.time() + self.timeout
        while True:
            try:
                if os.path.exists(self.path):
                    try:
                        mtime = os.path.getmtime(self.path)
                        if (time.time() - mtime) > _LOCK_STALE_SEC:
                            os.remove(self.path)
                    except Exception:
                        pass
                fd = os.open(self.path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
                with os.fdopen(fd, "w", encoding="utf-8") as f:
                    f.write(f"pid={os.getpid()} ts={time.time()}\n")
                break
            except FileExistsError:
                if time.time() >= deadline:
                    try:
                        mtime = os.path.getmtime(self.path)
                        if (time.time() - mtime) > _LOCK_STALE_SEC:
                            os.remove(self.path)
                            continue
                    except Exception:
                        pass
                    raise TimeoutError(f"lock timeout: {self.path}")
                time.sleep(self.poll)
        return self

    def __exit__(self, exc_type, exc, tb):
        try:
            if os.path.exists(self.path):
                os.remove(self.path)
        except Exception:
            pass

# -------------------------
# ì•ˆì „í•œ ë¡œê·¸ íŒŒì¼ ë³´ì¥
# -------------------------
def _read_csv_header(path):
    try:
        with open(path, "r", encoding="utf-8-sig") as f:
            first = f.readline().strip()
        if not first:
            return []
        return [h.strip() for h in first.split(",")]
    except Exception:
        return []

def ensure_prediction_log_exists():
    try:
        os.makedirs(os.path.dirname(PREDICTION_LOG), exist_ok=True)
        if not os.path.exists(PREDICTION_LOG) or os.path.getsize(PREDICTION_LOG) == 0:
            with open(PREDICTION_LOG, "w", newline="", encoding="utf-8-sig") as f:
                csv.writer(f).writerow(PREDICTION_HEADERS)
            print("[âœ… ensure_prediction_log_exists] prediction_log.csv ìƒì„±(í™•ì¥ ìŠ¤í‚¤ë§ˆ)")
        else:
            existing = _read_csv_header(PREDICTION_LOG)
            if existing != PREDICTION_HEADERS:
                bak = PREDICTION_LOG + ".bak"
                os.replace(PREDICTION_LOG, bak)
                with open(PREDICTION_LOG, "w", newline="", encoding="utf-8-sig") as out, \
                     open(bak, "r", encoding="utf-8-sig") as src:
                    w = csv.writer(out); w.writerow(PREDICTION_HEADERS)
                    reader = csv.reader(src)
                    try: next(reader)
                    except StopIteration: reader = []
                    for row in reader:
                        row = (row + [""] * len(PREDICTION_HEADERS))[:len(PREDICTION_HEADERS)]
                        w.writerow(row)
                print("[âœ… ensure_prediction_log_exists] ê¸°ì¡´ íŒŒì¼ í—¤ë” ë³´ì •(í™•ì¥) ì™„ë£Œ")
    except Exception as e:
        print(f"[âš ï¸ ensure_prediction_log_exists] ì˜ˆì™¸: {e}")

# âœ… train.pyê°€ í˜¸ì¶œí•˜ëŠ” ë³´ì¡° ë³´ì¥ í•¨ìˆ˜ (í—¤ë” ìë™ ë§ˆì´ê·¸ë ˆì´ì…˜ í¬í•¨)
def ensure_train_log_exists():
    try:
        os.makedirs(os.path.dirname(TRAIN_LOG), exist_ok=True)
        if not os.path.exists(TRAIN_LOG) or os.path.getsize(TRAIN_LOG) == 0:
            with open(TRAIN_LOG, "w", newline="", encoding="utf-8-sig") as f:
                csv.writer(f).writerow(TRAIN_HEADERS)
            print("[âœ… ensure_train_log_exists] train_log.csv ìƒì„±(í™•ì¥ ìŠ¤í‚¤ë§ˆ)")
        else:
            existing = _read_csv_header(TRAIN_LOG)
            if existing != TRAIN_HEADERS:
                bak = TRAIN_LOG + ".bak"
                os.replace(TRAIN_LOG, bak)
                with open(TRAIN_LOG, "w", newline="", encoding="utf-8-sig") as out, \
                     open(bak, "r", encoding="utf-8-sig") as src:
                    w = csv.writer(out); w.writerow(TRAIN_HEADERS)
                    reader = csv.reader(src)
                    try: old_header = next(reader)
                    except StopIteration: old_header = []
                    for row in reader:
                        # ê°€ëŠ¥í•œ ê°’ ë§¤í•‘ (êµ¬ë²„ì „: timestamp,symbol,strategy,model,accuracy,f1,loss,note,source_exchange,status)
                        mapped = {h:row[i] for i,h in enumerate(old_header)} if old_header else {}
                        new_row = [
                            mapped.get("timestamp",""), mapped.get("symbol",""), mapped.get("strategy",""), mapped.get("model",""),
                            mapped.get("accuracy",""), mapped.get("f1",""), mapped.get("loss",""),
                            "", "", "", "", "", "", "", "",  # engine/window/recent_cap/rows/limit/min/aug/enough
                            mapped.get("note",""), mapped.get("source_exchange",""), mapped.get("status",""),
                        ]
                        w.writerow(new_row[:len(TRAIN_HEADERS)])
                print("[âœ… ensure_train_log_exists] train_log.csv í—¤ë” ë³´ì •(í™•ì¥) ì™„ë£Œ")
    except Exception as e:
        print(f"[âš ï¸ ensure_train_log_exists] ì˜ˆì™¸: {e}")

# -------------------------
# (NEW) ìš©ëŸ‰ ê¸°ë°˜ ë¡œê·¸ ë¡œí…Œì´ì…˜
# -------------------------
def rotate_prediction_log_if_needed(max_mb: int = 200, backups: int = 3):
    try:
        if not os.path.exists(PREDICTION_LOG):
            return
        size_mb = os.path.getsize(PREDICTION_LOG) / (1024 * 1024)
        if size_mb < max_mb:
            return
        for i in range(backups, 0, -1):
            old = f"{PREDICTION_LOG}.{i}"
            if i == backups and os.path.exists(old):
                os.remove(old)
            else:
                prev = f"{PREDICTION_LOG}.{i-1}" if i-1 > 0 else PREDICTION_LOG
                if os.path.exists(prev):
                    shutil.move(prev, old)
        ensure_prediction_log_exists()
        print(f"[logger] ğŸ” rotate: {size_mb:.1f}MB â†’ rotated with {backups} backups")
    except Exception as e:
        print(f"[logger] rotate error: {e}")

# -------------------------
# feature hash ìœ í‹¸
# -------------------------
def get_feature_hash(feature_row) -> str:
    try:
        import numpy as _np
        if feature_row is None:
            return "none"
        if "torch" in str(type(feature_row)):
            try:
                feature_row = feature_row.detach().cpu().numpy()
            except Exception:
                pass
        if isinstance(feature_row, _np.ndarray):
            arr = feature_row.flatten().astype(float)
        elif isinstance(feature_row, (list, tuple)):
            arr = _np.array(feature_row, dtype=float).flatten()
        else:
            arr = _np.array([float(feature_row)], dtype=float)
        rounded = [round(float(x), 2) for x in arr]
        joined = ",".join(map(str, rounded))
        return hashlib.sha1(joined.encode()).hexdigest()
    except Exception:
        return "hash_error"

# -------------------------
# SQLite: ëª¨ë¸ ì„±ê³µ/ì‹¤íŒ¨ ì§‘ê³„
# -------------------------
_db_conn = None
_DB_PATH = os.path.join(LOG_DIR, "failure_patterns.db")
_db_lock = threading.RLock()

def _apply_sqlite_pragmas(conn):
    try:
        cur = conn.cursor()
        cur.execute("PRAGMA journal_mode=WAL;")
        cur.execute("PRAGMA synchronous=NORMAL;")
        cur.execute("PRAGMA busy_timeout=5000;")
        cur.close()
    except Exception as e:
        print(f"[ê²½ê³ ] PRAGMA ì„¤ì • ì‹¤íŒ¨ â†’ {e}")

def _connect_sqlite():
    conn = sqlite3.connect(_DB_PATH, timeout=30, check_same_thread=False)
    _apply_sqlite_pragmas(conn)
    return conn

def get_db_connection():
    global _db_conn
    with _db_lock:
        if _db_conn is None:
            try:
                _db_conn = _connect_sqlite()
                print("[âœ… logger.py DB connection ìƒì„± ì™„ë£Œ]")
            except Exception as e:
                print(f"[ì˜¤ë¥˜] logger.py DB connection ìƒì„± ì‹¤íŒ¨ â†’ {e}")
                _db_conn = None
        return _db_conn

def _sqlite_exec_with_retry(sql, params=(), retries=5, sleep_base=0.2, commit=False):
    last_err = None
    for attempt in range(1, retries + 1):
        try:
            with _db_lock:
                conn = get_db_connection()
                if conn is None:
                    conn = _connect_sqlite()
                    globals()['_db_conn'] = conn
                cur = conn.cursor()
                cur.execute(sql, params)
                if commit:
                    conn.commit()
                try:
                    rows = cur.fetchall()
                except sqlite3.ProgrammingError:
                    rows = None
                cur.close()
                return rows
        except sqlite3.OperationalError as e:
            msg = str(e).lower()
            last_err = e
            if ("database is locked" in msg) or ("disk i/o error" in msg) or ("database is busy" in msg):
                with _db_lock:
                    try:
                        if globals().get("_db_conn"):
                            try:
                                globals()["_db_conn"].close()
                            except Exception:
                                pass
                        globals()["_db_conn"] = _connect_sqlite()
                    except Exception as ce:
                        last_err = ce
                time.sleep(sleep_base * attempt)
                continue
            else:
                raise
        except Exception as e:
            last_err = e
            time.sleep(sleep_base * attempt)
            continue
    raise last_err if last_err else RuntimeError("sqlite exec failed")

def ensure_success_db():
    try:
        _sqlite_exec_with_retry("""
            CREATE TABLE IF NOT EXISTS model_success (
                symbol TEXT,
                strategy TEXT,
                model TEXT,
                success INTEGER,
                fail INTEGER,
                PRIMARY KEY(symbol, strategy, model)
            )
        """, params=(), retries=5, commit=True)
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ensure_success_db ì‹¤íŒ¨ â†’ {e}")

def update_model_success(s, t, m, success):
    try:
        _sqlite_exec_with_retry("""
            INSERT INTO model_success (symbol, strategy, model, success, fail)
            VALUES (?, ?, ?, ?, ?)
            ON CONFLICT(symbol, strategy, model) DO UPDATE SET
                success = success + excluded.success,
                fail = fail + excluded.fail
        """, params=(s, t or "ì•Œìˆ˜ì—†ìŒ", m, int(success), int(!success)), retries=7, commit=True)
        print(f"[âœ… update_model_success] {s}-{t}-{m} ê¸°ë¡ ({'ì„±ê³µ' if success else 'ì‹¤íŒ¨'})")
    except Exception as e:
        print(f"[ì˜¤ë¥˜] update_model_success ì‹¤íŒ¨ â†’ {e}")

def get_model_success_rate(s, t, m):
    try:
        rows = _sqlite_exec_with_retry("""
            SELECT success, fail FROM model_success
            WHERE symbol=? AND strategy=? AND model=?
        """, params=(s, t or "ì•Œìˆ˜ì—†ìŒ", m), retries=5, commit=False)
        row = rows[0] if rows else None
        if row is None:
            return 0.0
        success_cnt, fail_cnt = row
        total = success_cnt + fail_cnt
        return (success_cnt / total) if total > 0 else 0.0
    except Exception as e:
        print(f"[ì˜¤ë¥˜] get_model_success_rate ì‹¤íŒ¨ â†’ {e}")
        return 0.0

# -------------------------
# failure_db ì´ˆê¸°í™”
# -------------------------
try:
    from failure_db import ensure_failure_db as _ensure_failure_db_once
    _ensure_failure_db_once()
    print("[logger] failure_db initialized (schema ready)")
except Exception as _e:
    print(f"[logger] failure_db init failed: {_e}")

# ì„œë²„ ì‹œì‘ ì‹œ ë³´ì¥
ensure_success_db()
ensure_prediction_log_exists()
ensure_train_log_exists()

# -------------------------
# íŒŒì¼ ë¡œë“œ/ìœ í‹¸
# -------------------------
def load_failure_count():
    path = os.path.join(LOG_DIR, "failure_count.csv")
    if not os.path.exists(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8-sig") as f:
            return {f"{r['symbol']}-{r['strategy']}": int(r["failures"]) for r in csv.DictReader(f)}
    except:
        return {}

def _normalize_status(df: pd.DataFrame) -> pd.DataFrame:
    if "status" in df.columns:
        df["status"] = df["status"].astype(str).str.lower().map(lambda x: "success" if x == "success" else "fail")
        return df
    if "success" in df.columns:
        s = df["success"].map(lambda x: str(x).strip().lower() in ["true","1","yes","y"])
        df["status"] = s.map(lambda b: "success" if b else "fail")
        return df
    df["status"] = ""
    return df

# -------------------------
# ë©”ëª¨ë¦¬ ì•ˆì „ ì§‘ê³„ (ì²­í¬ ê¸°ë°˜)
# -------------------------
def get_meta_success_rate(strategy, min_samples: int = 1):
    if not os.path.exists(PREDICTION_LOG):
        return 0.0
    usecols = ["timestamp","strategy","model","status","success"]
    succ = total = 0
    for chunk in pd.read_csv(
        PREDICTION_LOG, encoding="utf-8-sig",
        usecols=[c for c in usecols if c in PREDICTION_HEADERS or c in ["status","success"]],
        chunksize=CHUNK
    ):
        if "model" in chunk.columns:
            chunk = chunk[chunk["model"] == "meta"]
        if "strategy" in chunk.columns:
            chunk = chunk[chunk["strategy"] == strategy]
        if chunk.empty:
            continue
        if "status" in chunk.columns and chunk["status"].notna().any():
            mask = chunk["status"].astype(str).str.lower().isin(["success","fail","v_success","v_fail"])
            chunk = chunk[mask]
            s = chunk["status"].astype(str).str.lower().isin(["success","v_success"])
            succ += int(s.sum()); total += int(len(chunk))
        elif "success" in chunk.columns:
            s = chunk["success"].astype(str).str.lower().isin(["true","1","success","v_success"])
            succ += int(s.sum()); total += int(len(chunk))
    if total < max(1, min_samples):
        return 0.0
    return float(succ / total)

def get_strategy_eval_count(strategy: str):
    if not os.path.exists(PREDICTION_LOG):
        return 0
    usecols = ["strategy","status","success"]
    count = 0
    for chunk in pd.read_csv(
        PREDICTION_LOG, encoding="utf-8-sig",
        usecols=[c for c in usecols if c in PREDICTION_HEADERS or c in ["status","success"]],
        chunksize=CHUNK
    ):
        if "strategy" in chunk.columns:
            chunk = chunk[chunk["strategy"] == strategy]
        if chunk.empty:
            continue
        if "status" in chunk.columns and chunk["status"].notna().any():
            mask = chunk["status"].astype(str).str.lower().isin(["success","fail","v_success","v_fail"])
            count += int(mask.sum())
        elif "success" in chunk.columns:
            count += int(len(chunk))
    return int(count)

def get_actual_success_rate(strategy, min_samples: int = 1):
    if not os.path.exists(PREDICTION_LOG):
        return 0.0
    usecols = ["strategy","status","success"]
    succ = total = 0
    for chunk in pd.read_csv(
        PREDICTION_LOG, encoding="utf-8-sig",
        usecols=[c for c in usecols if c in PREDICTION_HEADERS or c in ["status","success"]],
        chunksize=CHUNK
    ):
        if "strategy" in chunk.columns:
            chunk = chunk[chunk["strategy"] == strategy]
        if chunk.empty:
            continue
        if "status" in chunk.columns and chunk["status"].notna().any():
            mask = chunk["status"].astype(str).str.lower().isin(["success","fail","v_success","v_fail"])
            s = chunk["status"].astype(str).str.lower().isin(["success","v_success"])
            succ += int(s[mask].sum()); total += int(mask.sum())
        elif "success" in chunk.columns:
            s = chunk["success"].astype(str).str.lower().isin(["true","1","success","v_success"])
            succ += int(s.sum()); total += int(len(chunk))
    if total < max(1, min_samples):
        return 0.0
    return round(succ / total, 4)

# -------------------------
# ê°ì‚¬ìš© ê°ì‚¬ ë¡œê·¸
# -------------------------
def log_audit_prediction(s, t, status, reason):
    row = {
        "timestamp": now_kst().isoformat(),
        "symbol": str(s or "UNKNOWN"),
        "strategy": str(t or "ì•Œìˆ˜ì—†ìŒ"),
        "status": str(status),
        "reason": str(reason)
    }
    try:
        with open(AUDIT_LOG, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(f, fieldnames=row.keys())
            if f.tell() == 0:
                w.writeheader()
            w.writerow(row)
    except:
        pass

# -------------------------
# ì˜ˆì¸¡ ë¡œê·¸ (í™•ì¥ ì¸ì ì§€ì› + ìë™ ì»¬ëŸ¼ë§ì¶¤ + íŒŒì¼ë½)
# -------------------------
def _align_row_to_header(row, header):
    if len(row) < len(header):
        row = row + [""] * (len(header) - len(row))
    elif len(row) > len(header):
        row = row[:len(header)]
    return row

def _clean_str(x):
    s = str(x).strip() if x is not None else ""
    if s.lower() in {"", "unknown", "none", "nan", "null"}:
        return ""
    return s

def _normalize_model_fields(model, model_name, symbol, strategy):
    m = _clean_str(model)
    mn = _clean_str(model_name)
    if not m and mn:
        m = mn
    if not mn and m:
        mn = m
    if not m and not mn:
        base = f"auto_{symbol}_{strategy}"
        m = mn = base
    return m, mn

def _extract_from_note(note_str: str):
    fields = {
        "position": "", "hint_allow_long": "", "hint_allow_short": "", "hint_slope": "",
        "used_minret_filter": "", "explore_used": "", "hint_ma_fast": "", "hint_ma_slow": ""
    }
    try:
        obj = json.loads(note_str) if isinstance(note_str, str) else {}
        if isinstance(obj, dict):
            for k in list(fields.keys()):
                if k in obj:
                    v = obj.get(k)
                    if isinstance(v, bool):
                        fields[k] = int(v)
                    else:
                        fields[k] = v if (v is not None) else ""
            if not fields.get("explore_used"):
                mc = obj.get("meta_choice", "")
                fields["explore_used"] = 1 if ("explore" in str(mc)) else 0
    except Exception:
        pass
    return fields

def log_prediction(
    symbol, strategy, direction=None, entry_price=0, target_price=0,
    timestamp=None, model=None, predicted_class=None, top_k=None,
    note="", success=False, reason="", rate=None, return_value=None,
    label=None, group_id=None, model_symbol=None, model_name=None,
    source="ì¼ë°˜", volatility=False, feature_vector=None,
    source_exchange="BYBIT",
    regime=None, meta_choice=None, raw_prob=None, calib_prob=None, calib_ver=None,
    class_return_min=None, class_return_max=None, class_return_text=None,
):
    from datetime import datetime as _dt
    try:
        from failure_db import insert_failure_record
    except Exception:
        insert_failure_record = None

    LOG_FILE = PREDICTION_LOG
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)

    with _FileLock(_PRED_LOCK_PATH, timeout=10.0):
        rotate_prediction_log_if_needed()
        ensure_prediction_log_exists()

        now = _dt.now(pytz.timezone("Asia/Seoul")).isoformat() if timestamp is None else timestamp
        top_k_str = ",".join(map(str, top_k)) if top_k else ""

        predicted_class = predicted_class if predicted_class is not None else -1
        label = label if label is not None else -1
        reason = (reason or "ì‚¬ìœ ì—†ìŒ").strip()
        rate = 0.0 if rate is None else float(rate)
        return_value = 0.0 if return_value is None else float(return_value)
        entry_price = float(entry_price or 0.0)
        target_price = float(target_price or 0.0)
        model, model_name = _normalize_model_fields(model, model_name, symbol, strategy)

        allowed_sources = ["ì¼ë°˜","ê¸°ë³¸","meta","evo_meta","baseline_meta","ì§„í™”í˜•","í‰ê°€","ë‹¨ì¼","ë³€ë™ì„±","train_loop","ì„€ë„ìš°"]
        if source not in allowed_sources:
            source = "ì¼ë°˜"

        dir_s = str(direction or "")
        src_s = str(source or "")
        ctx = "evaluation" if (src_s == "í‰ê°€" or dir_s.startswith("í‰ê°€")) else "prediction"

        fv_serial = ""
        try:
            if feature_vector is not None:
                if isinstance(feature_vector, (list, tuple)):
                    v = list(feature_vector)
                elif "numpy" in str(type(feature_vector)):
                    v = feature_vector.flatten().tolist()
                else:
                    v = []
                if len(v) > 64:
                    fv_serial = json.dumps({"head": v[:8], "tail": v[-8:]}, ensure_ascii=False)
                else:
                    fv_serial = json.dumps(v, ensure_ascii=False)
        except Exception:
            fv_serial = ""

        note_fields = _extract_from_note(note)

        row = [
            now, symbol, strategy, direction, entry_price, target_price,
            (model or ""), predicted_class, top_k_str, note,
            str(success), reason, rate, return_value, label,
            group_id, model_symbol, model_name, source, volatility, source_exchange,
            (regime or ""), (meta_choice or ""),
            (float(raw_prob) if raw_prob is not None else ""),
            (float(calib_prob) if calib_prob is not None else ""),
            (str(calib_ver) if calib_ver is not None else ""),
            fv_serial,
            (float(class_return_min) if class_return_min is not None else ""),
            (float(class_return_max) if class_return_max is not None else ""),
            (str(class_return_text) if class_return_text is not None else ""),
            note_fields.get("position",""),
            note_fields.get("hint_allow_long",""),
            note_fields.get("hint_allow_short",""),
            note_fields.get("hint_slope",""),
            note_fields.get("used_minret_filter",""),
            note_fields.get("explore_used",""),
            note_fields.get("hint_ma_fast",""),
            note_fields.get("hint_ma_slow",""),
        ]

        try:
            write_header = not os.path.exists(LOG_FILE) or os.path.getsize(LOG_FILE) == 0
            current_header = PREDICTION_HEADERS
            if not write_header:
                h = _read_csv_header(LOG_FILE)
                current_header = h if h else PREDICTION_HEADERS

            with open(LOG_FILE, "a", newline="", encoding="utf-8-sig") as f:
                w = csv.writer(f)
                if write_header:
                    w.writerow(PREDICTION_HEADERS)
                    current_header = PREDICTION_HEADERS
                w.writerow(_align_row_to_header(row, current_header))

            print(f"[âœ… ì˜ˆì¸¡ ë¡œê·¸ ê¸°ë¡ë¨] {symbol}-{strategy} class={predicted_class} | success={success} | src={source_exchange} | reason={reason}")

            should_record_failure = (
                insert_failure_record is not None
                and (ctx == "evaluation")
                and (not success)
                and (label not in (-1, "-1", None))
                and (entry_price not in (None, 0.0))
            )

            if should_record_failure:
                feature_hash = f"{symbol}-{strategy}-{model}-{predicted_class}-{label}-{rate}"
                safe_vector = []
                try:
                    import numpy as _np
                    if feature_vector is not None:
                        if isinstance(feature_vector, _np.ndarray):
                            safe_vector = feature_vector.flatten().tolist()
                        elif isinstance(feature_vector, list):
                            safe_vector = feature_vector
                except Exception:
                    safe_vector = []

                insert_failure_record(
                    {
                        "symbol": symbol, "strategy": strategy, "direction": direction,
                        "model": model, "predicted_class": predicted_class,
                        "rate": rate, "reason": reason, "label": label, "source": source,
                        "entry_price": entry_price, "target_price": target_price,
                        "return_value": return_value
                    },
                    feature_hash=feature_hash, label=label, feature_vector=safe_vector, context=ctx
                )
            else:
                if (insert_failure_record is None) or (ctx != "evaluation"):
                    log_audit_prediction(symbol, strategy, "skip_failure_db", f"ctx={ctx}, label={label}, entry_price={entry_price}")
        except Exception as e:
            print(f"[âš ï¸ ì˜ˆì¸¡ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨] {e}")

# -------------------------
# í•™ìŠµ ë¡œê·¸
# -------------------------
# note ë¬¸ìì—´ì—ì„œ ì—”ì§„/ìœˆë„/ìº¡/ë°ì´í„° í”Œë˜ê·¸ë¥¼ ìµœëŒ€í•œ ì¶”ì¶œ
_note_re_engine   = re.compile(r"engine=([a-zA-Z_]+)")
_note_re_window   = re.compile(r"window=(\d+)")
_note_re_cap      = re.compile(r"cap=(\d+)")
_note_re_flags    = re.compile(r"data_flags=\{?rows:(\d+),\s*limit:(\d+),\s*min:(\d+),\s*aug:(\d+),\s*enough_for_training:(\d+)\}?")

def _parse_train_note(note: str):
    s = str(note or "")
    eng = (_note_re_engine.search(s) or [None, ""])[1]
    win = (_note_re_window.search(s) or [None, ""])[1]
    cap = (_note_re_cap.search(s) or [None, ""])[1]
    mfl = _note_re_flags.search(s)
    rows = limit = minv = aug = enough = ""
    if mfl:
        rows, limit, minv, aug, enough = mfl.groups()
    return {
        "engine": eng, "window": win, "recent_cap": cap,
        "rows": rows, "limit": limit, "min": minv,
        "augment_needed": aug, "enough_for_training": enough
    }

def log_training_result(
    symbol, strategy, model="", accuracy=0.0, f1=0.0, loss=0.0,
    note="", source_exchange="BYBIT", status="success",
    y_true=None, y_pred=None
):
    LOG_FILE = TRAIN_LOG
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    now = datetime.datetime.now(pytz.timezone("Asia/Seoul")).isoformat()

    # í™•ì¥ í•„ë“œ íŒŒì‹±
    extras = _parse_train_note(note)

    row = [
        now, str(symbol), str(strategy), str(model or ""),
        float(accuracy) if accuracy is not None else "",
        float(f1) if f1 is not None else "",
        float(loss) if loss is not None else "",
        extras.get("engine",""), extras.get("window",""), extras.get("recent_cap",""),
        extras.get("rows",""), extras.get("limit",""), extras.get("min",""),
        extras.get("augment_needed",""), extras.get("enough_for_training",""),
        str(note or ""), str(source_exchange or "BYBIT"),
        str(status or "success")
    ]
    try:
        write_header = not os.path.exists(LOG_FILE) or os.path.getsize(LOG_FILE) == 0
        if write_header:
            ensure_train_log_exists()
        with open(LOG_FILE, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(TRAIN_HEADERS)
            w.writerow(_align_row_to_header(row, TRAIN_HEADERS))
        print(f"[âœ… í•™ìŠµ ë¡œê·¸ ê¸°ë¡] {symbol}-{strategy} {model} val_f1={f1:.4f} status={status}")
    except Exception as e:
        print(f"[âš ï¸ í•™ìŠµ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨] {e}")

    # ì´ë™í‰ê·  F1 & per-class F1 ëª¨ë‹ˆí„°ë§ (ì½˜ì†” ì¶œë ¥ ì „ìš©)
    try:
        N = int(os.getenv("LOG_F1_MA_N", "20"))
        df_ma = pd.read_csv(LOG_FILE, encoding="utf-8-sig")
        if "val_f1" in df_ma.columns:
            sub = df_ma[df_ma.get("strategy","") == strategy].tail(max(1, N))
            if not sub.empty:
                ma_f1 = float(pd.to_numeric(sub["val_f1"], errors="coerce").dropna().mean()) if "val_f1" in sub else float("nan")
                if ma_f1 == ma_f1:
                    print(f"[ğŸ“Š ì´ë™í‰ê·  F1] ì „ëµ={strategy} ìµœê·¼{len(sub)}íšŒ â†’ {ma_f1:.4f}")
    except Exception:
        pass
    if y_true is not None and y_pred is not None:
        try:
            rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)
            per_cls = {k: v["f1-score"] for k, v in rep.items() if k.isdigit()}
            print(f"[ğŸ“Š per-class F1] {symbol}-{strategy} {model} â†’ {json.dumps(per_cls, ensure_ascii=False)}")
        except Exception as e:
            print(f"[âš ï¸ per-class F1 ê³„ì‚° ì‹¤íŒ¨] {e}")

# -------------------------
# ìˆ˜ìµë¥  í´ë˜ìŠ¤ ê²½ê³„ ë¡œê·¸
# -------------------------
def log_class_ranges(symbol, strategy, group_id=None, class_ranges=None, note=""):
    import csv, os, math
    path = os.path.join(LOG_DIR, "class_ranges.csv")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    now = now_kst().isoformat()

    class_ranges = class_ranges or []
    write_header = not os.path.exists(path)
    try:
        with open(path, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(["timestamp","symbol","strategy","group_id","idx","low","high","note"])
            for i, rng in enumerate(class_ranges):
                try:
                    lo, hi = (float(rng[0]), float(rng[1]))
                except Exception:
                    lo, hi = (None, None)
                w.writerow([now, symbol, strategy, int(group_id) if group_id is not None else 0, i, lo, hi, str(note or "")])
        print(f"[ğŸ“ í´ë˜ìŠ¤ê²½ê³„ ë¡œê·¸] {symbol}-{strategy}-g{group_id} â†’ {len(class_ranges)}ê°œ ê¸°ë¡")
    except Exception as e:
        print(f"[âš ï¸ í´ë˜ìŠ¤ê²½ê³„ ë¡œê·¸ ì‹¤íŒ¨] {e}")

# -------------------------
# ìˆ˜ìµë¥  ë¶„í¬ ìš”ì•½ ë¡œê·¸
# -------------------------
def log_return_distribution(symbol, strategy, group_id=None, horizon_hours=None, summary: dict=None, note=""):
    path = os.path.join(LOG_DIR, "return_distribution.csv")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    now = now_kst().isoformat()

    s = summary or {}
    row = [
        now, str(symbol), str(strategy),
        int(group_id) if group_id is not None else 0,
        int(horizon_hours) if horizon_hours is not None else "",
        float(s.get("min", 0.0)), float(s.get("p25", 0.0)), float(s.get("p50", 0.0)),
        float(s.get("p75", 0.0)), float(s.get("p90", 0.0)), float(s.get("p95", 0.0)),
        float(s.get("p99", 0.0)), float(s.get("max", 0.0)), int(s.get("count", 0)),
        str(note or "")
    ]

    write_header = not os.path.exists(path)
    try:
        with open(path, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(["timestamp","symbol","strategy","group_id","horizon_hours",
                            "min","p25","p50","p75","p90","p95","p99","max","count","note"])
            w.writerow(row)
        print(f"[ğŸ“ˆ ìˆ˜ìµë¥ ë¶„í¬ ë¡œê·¸] {symbol}-{strategy}-g{group_id} count={s.get('count',0)}")
    except Exception as e:
        print(f"[âš ï¸ ìˆ˜ìµë¥ ë¶„í¬ ë¡œê·¸ ì‹¤íŒ¨] {e}")

# -------------------------
# ë¼ë²¨ ë¶„í¬ ë¡œê·¸
# -------------------------
def log_label_distribution(
    symbol, strategy, group_id=None,
    counts: dict=None, total: int=None, n_unique: int=None, entropy: float=None,
    labels=None, note=""
):
    import json, math

    path = os.path.join(LOG_DIR, "label_distribution.csv")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    now = now_kst().isoformat()

    if counts is None:
        from collections import Counter
        try:
            labels_list = list(map(int, list(labels or [])))
        except Exception:
            labels_list = []
        cnt = Counter(labels_list)
        total_calc = sum(cnt.values())
        probs = [c/total_calc for c in cnt.values()] if total_calc > 0 else []
        entropy_calc = -sum(p*math.log(p + 1e-12) for p in probs) if probs else 0.0
        counts = {int(k): int(v) for k, v in sorted(cnt.items())}
        total = total_calc
        n_unique = len(cnt)
        entropy = round(float(entropy_calc), 6)
    else:
        counts = {int(k): int(v) for k, v in sorted(counts.items())}
        total = int(total if total is not None else sum(counts.values()))
        n_unique = int(n_unique if n_unique is not None else len(counts))
        if entropy is None:
            probs = [c/total for c in counts.values()] if total > 0 else []
            entropy = round(float(-sum(p*math.log(p + 1e-12) for p in probs)) if probs else 0.0, 6)
        else:
            entropy = float(entropy)

    row = [
        now, str(symbol), str(strategy),
        int(group_id) if group_id is not None else 0,
        int(total),
        json.dumps(counts, ensure_ascii=False),
        int(n_unique),
        float(entropy),
        str(note or "")
    ]

    write_header = not os.path.exists(path)
    try:
        with open(path, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(["timestamp","symbol","strategy","group_id","total","counts_json","n_unique","entropy","note"])
            w.writerow(row)
        print(f"[ğŸ“Š ë¼ë²¨ë¶„í¬ ë¡œê·¸] {symbol}-{strategy}-g{group_id} â†’ total={total}, classes={n_unique}, H={entropy:.4f}")
    except Exception as e:
        print(f"[âš ï¸ ë¼ë²¨ë¶„í¬ ë¡œê·¸ ì‹¤íŒ¨] {e}")

# -------------------------
# (NEW) ê²€ì¦ í´ë˜ìŠ¤ ì»¤ë²„ ê²½ê³  + ë‹¨ì¼ í´ë˜ìŠ¤ ì˜ˆì¸¡ ê²½ê³ 
# -------------------------
def log_eval_coverage(symbol: str, strategy: str, counts: dict, num_classes: int, note: str = ""):
    path = os.path.join(LOG_DIR, "validation_coverage.csv")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    now = now_kst().isoformat()

    counts = {int(k): int(v) for k, v in sorted((counts or {}).items())}
    covered = sum(1 for v in counts.values() if int(v) > 0)
    total = int(sum(counts.values()))
    coverage = (covered / max(1, int(num_classes))) if num_classes else 0.0

    write_header = not os.path.exists(path)
    try:
        with open(path, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(["timestamp","symbol","strategy","num_classes","covered","coverage","total","counts_json","note"])
            w.writerow([now, symbol, strategy, int(num_classes), int(covered), float(round(coverage,4)), int(total), json.dumps(counts, ensure_ascii=False), str(note or "")])
        if covered <= 1:
            print(f"ğŸ”´ [ê²½ê³ ] ê²€ì¦ ë¼ë²¨ ë‹¨ì¼ í´ë˜ìŠ¤ ê°ì§€ â†’ {symbol}-{strategy} (covered={covered}/{num_classes})")
        elif coverage < 0.6:
            print(f"ğŸŸ  [ì£¼ì˜] ê²€ì¦ í´ë˜ìŠ¤ ì»¤ë²„ ë‚®ìŒ â†’ {symbol}-{strategy} (coverage={coverage:.2f})")
    except Exception as e:
        print(f"[âš ï¸ validation_coverage ë¡œê·¸ ì‹¤íŒ¨] {e}")

def alert_if_single_class_prediction(symbol: str, strategy: str, lookback_days: int = 3, min_rows: int = 100):
    try:
        ensure_prediction_log_exists()
        if not os.path.exists(PREDICTION_LOG) or os.path.getsize(PREDICTION_LOG) == 0:
            return False
        cutoff = now_kst() - datetime.timedelta(days=int(lookback_days))
        uniq = set(); total = 0
        usecols = ["timestamp","symbol","strategy","predicted_class"]
        for chunk in pd.read_csv(PREDICTION_LOG, encoding="utf-8-sig", usecols=[c for c in usecols if c in PREDICTION_HEADERS], chunksize=CHUNK):
            if "timestamp" not in chunk.columns or "predicted_class" not in chunk.columns:
                continue
            ts = pd.to_datetime(chunk["timestamp"], errors="coerce")
            try:
                ts = ts.dt.tz_localize("Asia/Seoul")
            except Exception:
                ts = ts.dt.tz_convert("Asia/Seoul")
            sub = chunk[(chunk.get("symbol","")==symbol) & (chunk.get("strategy","")==strategy)]
            sub = sub.loc[ts >= cutoff]
            if sub.empty:
                continue
            pcs = pd.to_numeric(sub["predicted_class"], errors="coerce").dropna().astype(int)
            uniq.update(pcs.unique().tolist())
            total += int(len(pcs))
        if total >= int(min_rows) and len(uniq) <= 1:
            print(f"ğŸ”´ [ê²½ê³ ] ìµœê·¼ ì˜ˆì¸¡ì´ ì‚¬ì‹¤ìƒ ë‹¨ì¼ í´ë˜ìŠ¤ â†’ {symbol}-{strategy} (rows={total}, uniq={len(uniq)})")
            log_audit_prediction(symbol, strategy, "single_class_pred", f"rows={total}, uniq={len(uniq)}")
            return True
        return False
    except Exception as e:
        print(f"[âš ï¸ ë‹¨ì¼í´ë˜ìŠ¤ ì˜ˆì¸¡ ì ê²€ ì‹¤íŒ¨] {e}")
        return False

# -------------------------
# ì •ë ¬ í‚¤
# -------------------------
def _model_sort_key(r):
    return (
        str(r.get("symbol", "")),
        str(r.get("strategy", "")),
        str(r.get("model", "")),
        int(r.get("group_id", 0)),
    )

# -------------------------
# ëª¨ë¸ ì¸ë²¤í† ë¦¬ ì¡°íšŒ
# -------------------------
def get_available_models(symbol: str = None, strategy: str = None):
    try:
        model_dir = "/persistent/models"
        if not os.path.isdir(model_dir):
            return []
        out = []
        for fn in os.listdir(model_dir):
            if not fn.endswith(".pt"):
                continue
            pt_path = os.path.join(model_dir, fn)
            meta_path = pt_path.replace(".pt", ".meta.json")
            if not os.path.exists(meta_path):
                continue
            try:
                with open(meta_path, "r", encoding="utf-8") as f:
                    meta = json.load(f)
            except Exception:
                meta = {}
            sym = meta.get("symbol") or fn.split("_", 1)[0]
            strat = meta.get("strategy") or ("ë‹¨ê¸°" if "_ë‹¨ê¸°_" in fn else "ì¤‘ê¸°" if "_ì¤‘ê¸°_" in fn else "ì¥ê¸°" if "_ì¥ê¸°_" in fn else "")
            if symbol and sym != symbol:
                continue
            if strategy and strat != strategy:
                continue
            out.append({
                "pt_file": fn,
                "meta_file": os.path.basename(meta_path),
                "symbol": sym,
                "strategy": strat,
                "model": meta.get("model", ""),
                "group_id": meta.get("group_id", 0),
                "num_classes": meta.get("num_classes", 0),
                "val_f1": float(meta.get("metrics", {}).get("val_f1", 0.0)),
                "timestamp": meta.get("timestamp", "")
            })
        out.sort(key=_model_sort_key)
        return out
    except Exception as e:
        print(f"[ì˜¤ë¥˜] get_available_models ì‹¤íŒ¨ â†’ {e}")
        return []

# -------------------------
# ìµœê·¼ ì˜ˆì¸¡ í†µê³„ ë‚´ë³´ë‚´ê¸° (ì²­í¬ ëˆ„ì‚°)
# -------------------------
def export_recent_model_stats(days: int = 7, out_path: str = None):
    try:
        ensure_prediction_log_exists()
        path = PREDICTION_LOG
        if out_path is None:
            os.makedirs(LOG_DIR, exist_ok=True)
            out_path = os.path.join(LOG_DIR, "recent_model_stats.csv")

        if not os.path.exists(path) or os.path.getsize(path) == 0:
            pd.DataFrame(columns=["symbol","strategy","model","total","success","fail","success_rate","last_ts"]).to_csv(out_path, index=False, encoding="utf-8-sig")
            return out_path

        cutoff = now_kst() - datetime.timedelta(days=int(days))

        agg = {}

        usecols = ["timestamp","symbol","strategy","model","status","success"]
        for chunk in pd.read_csv(
            path, encoding="utf-8-sig",
            usecols=[c for c in usecols if c in PREDICTION_HEADERS or c in ["status","success"]],
            chunksize=CHUNK
        ):
            if "timestamp" in chunk.columns:
                ts = pd.to_datetime(chunk["timestamp"], errors="coerce")
                try:
                    ts = ts.dt.tz_localize("Asia/Seoul")
                except Exception:
                    ts = ts.dt.tz_convert("Asia/Seoul")
                chunk = chunk.loc[ts >= cutoff]
                chunk = chunk.assign(_ts=ts)
            else:
                continue

            if chunk.empty or "model" not in chunk.columns:
                continue

            succ_mask = None
            if "status" in chunk.columns and chunk["status"].notna().any():
                ok_mask = chunk["status"].astype(str).str.lower().isin(["success","fail","v_success","v_fail"])
                succ_mask = chunk["status"].astype(str).str.lower().isin(["success","v_success"])
                chunk = chunk[ok_mask]
            elif "success" in chunk.columns:
                succ_mask = chunk["success"].astype(str).str.lower().isin(["true","1","success","v_success"])
            else:
                continue

            if chunk.empty:
                continue

            chunk = chunk.assign(_succ=succ_mask.astype(bool))

            for (sym, strat, mdl), sub in chunk.groupby(["symbol","strategy","model"], dropna=False):
                key = (str(sym), str(strat), str(mdl))
                d = agg.setdefault(key, {"success":0, "total":0, "last_ts":None})
                d["success"] += int(sub["_succ"].sum())
                d["total"]   += int(len(sub))
                last_ts = pd.to_datetime(sub["_ts"].max(), errors="coerce")
                if d["last_ts"] is None or (pd.notna(last_ts) and last_ts > d["last_ts"]):
                    d["last_ts"] = last_ts

        rows = []
        for (sym,strat,mdl), d in agg.items():
            total = int(d["total"]); succ = int(d["success"])
            rate = (succ/total) if total>0 else 0.0
            last_ts = d["last_ts"].isoformat() if d["last_ts"] is not None else ""
            rows.append({
                "symbol": sym, "strategy": strat, "model": mdl,
                "total": total, "success": succ, "fail": total - succ,
                "success_rate": round(rate, 4), "last_ts": last_ts
            })

        df_out = pd.DataFrame(rows, columns=["symbol","strategy","model","total","success","fail","success_rate","last_ts"])
        df_out = df_out.sort_values(["symbol","strategy","model","last_ts"])
        df_out.to_csv(out_path, index=False, encoding="utf-8-sig")
        print(f"[âœ… export_recent_model_stats] ì €ì¥: {out_path} (rows={len(df_out)})")
        return out_path
    except Exception as e:
        print(f"[âš ï¸ export_recent_model_stats ì‹¤íŒ¨] {e}")
        try:
            pd.DataFrame(columns=["symbol","strategy","model","total","success","fail","success_rate","last_ts"]).to_csv(
                out_path or os.path.join(LOG_DIR, "recent_model_stats.csv"), index=False, encoding="utf-8-sig")
        except Exception:
            pass
        return out_path or os.path.join(LOG_DIR, "recent_model_stats.csv")

