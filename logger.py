# === logger.py (í˜¸í™˜ ìµœì¢…ë³¸: í™•ì¥ ìŠ¤í‚¤ë§ˆÂ·ìë™ì •ë ¬ + ì¸ë²¤í† ë¦¬/í†µê³„ ë‚´ë³´ë‚´ê¸° + metaì „ìš© ì„±ê³µë¥ ) ===
import os, csv, json, datetime, pandas as pd, pytz, hashlib
import sqlite3
from collections import defaultdict
import threading, time  # â¬…ï¸ ë™ì‹œì„±/ì¬ì‹œë„

# -------------------------
# ê¸°ë³¸ ê²½ë¡œ/ë””ë ‰í† ë¦¬
# -------------------------
DIR = "/persistent"
LOG_DIR = os.path.join(DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# âœ… prediction_logëŠ” "ë£¨íŠ¸" ê²½ë¡œë¡œ í†µì¼
PREDICTION_LOG = f"{DIR}/prediction_log.csv"
WRONG = f"{DIR}/wrong_predictions.csv"  # (í˜¸í™˜ ëª©ì )
EVAL_RESULT = f"{LOG_DIR}/evaluation_result.csv"

# âœ… í•™ìŠµ ë¡œê·¸ íŒŒì¼ëª… í†µì¼
TRAIN_LOG = f"{LOG_DIR}/train_log.csv"
AUDIT_LOG = f"{LOG_DIR}/evaluation_audit.csv"

# âœ… ê³µìš© í—¤ë”(ê¸°ì¡´ + í™•ì¥ ì»¬ëŸ¼)
BASE_PRED_HEADERS = [
    "timestamp","symbol","strategy","direction",
    "entry_price","target_price",
    "model","predicted_class","top_k","note",
    "success","reason","rate","return_value",
    "label","group_id","model_symbol","model_name",
    "source","volatility","source_exchange"
]
EXTRA_PRED_HEADERS = ["regime","meta_choice","raw_prob","calib_prob","calib_ver"]
PREDICTION_HEADERS = BASE_PRED_HEADERS + EXTRA_PRED_HEADERS

now_kst = lambda: datetime.datetime.now(pytz.timezone("Asia/Seoul"))

# -------------------------
# ì•ˆì „í•œ ë¡œê·¸ íŒŒì¼ ë³´ì¥
# -------------------------
def _read_csv_header(path):
    try:
        with open(path, "r", encoding="utf-8-sig") as f:
            first = f.readline().strip()
        if not first:
            return []
        return [h.strip() for h in first.split(",")]
    except Exception:
        return []

def ensure_prediction_log_exists():
    try:
        os.makedirs(os.path.dirname(PREDICTION_LOG), exist_ok=True)
        if not os.path.exists(PREDICTION_LOG) or os.path.getsize(PREDICTION_LOG) == 0:
            with open(PREDICTION_LOG, "w", newline="", encoding="utf-8-sig") as f:
                csv.writer(f).writerow(PREDICTION_HEADERS)
            print("[âœ… ensure_prediction_log_exists] prediction_log.csv ìƒì„±(í™•ì¥ ìŠ¤í‚¤ë§ˆ)")
        else:
            existing = _read_csv_header(PREDICTION_LOG)
            if not existing or "timestamp" not in existing or "symbol" not in existing:
                bak = PREDICTION_LOG + ".bak"
                os.replace(PREDICTION_LOG, bak)
                with open(PREDICTION_LOG, "w", newline="", encoding="utf-8-sig") as out,\
                     open(bak, "r", encoding="utf-8-sig") as src:
                    w = csv.writer(out); w.writerow(PREDICTION_HEADERS)
                    reader = csv.reader(src)
                    try: next(reader)
                    except StopIteration: reader = []
                    for row in reader:
                        row = (row + [""]*len(PREDICTION_HEADERS))[:len(PREDICTION_HEADERS)]
                        w.writerow(row)
                print("[âœ… ensure_prediction_log_exists] ê¸°ì¡´ íŒŒì¼ í—¤ë” ë³´ì •(í™•ì¥) ì™„ë£Œ")
    except Exception as e:
        print(f"[âš ï¸ ensure_prediction_log_exists] ì˜ˆì™¸: {e}")

# -------------------------
# feature hash ìœ í‹¸
# -------------------------
def get_feature_hash(feature_row) -> str:
    try:
        import numpy as _np
        if feature_row is None:
            return "none"
        if "torch" in str(type(feature_row)):
            try:
                feature_row = feature_row.detach().cpu().numpy()
            except Exception:
                pass
        if isinstance(feature_row, _np.ndarray):
            arr = feature_row.flatten().astype(float)
        elif isinstance(feature_row, (list, tuple)):
            arr = _np.array(feature_row, dtype=float).flatten()
        else:
            arr = _np.array([float(feature_row)], dtype=float)
        rounded = [round(float(x), 2) for x in arr]
        joined = ",".join(map(str, rounded))
        return hashlib.sha1(joined.encode()).hexdigest()
    except Exception:
        return "hash_error"

# -------------------------
# SQLite: ëª¨ë¸ ì„±ê³µ/ì‹¤íŒ¨ ì§‘ê³„ (í•„ìš” ì‹œ ì‚¬ìš©)
# -------------------------
_db_conn = None
_DB_PATH = os.path.join(LOG_DIR, "failure_patterns.db")
_db_lock = threading.RLock()

def _apply_sqlite_pragmas(conn):
    try:
        cur = conn.cursor()
        cur.execute("PRAGMA journal_mode=WAL;")
        cur.execute("PRAGMA synchronous=NORMAL;")
        cur.execute("PRAGMA busy_timeout=5000;")
        cur.close()
    except Exception as e:
        print(f"[ê²½ê³ ] PRAGMA ì„¤ì • ì‹¤íŒ¨ â†’ {e}")

def _connect_sqlite():
    conn = sqlite3.connect(_DB_PATH, timeout=30, check_same_thread=False)
    _apply_sqlite_pragmas(conn)
    return conn

def get_db_connection():
    global _db_conn
    with _db_lock:
        if _db_conn is None:
            try:
                _db_conn = _connect_sqlite()
                print("[âœ… logger.py DB connection ìƒì„± ì™„ë£Œ]")
            except Exception as e:
                print(f"[ì˜¤ë¥˜] logger.py DB connection ìƒì„± ì‹¤íŒ¨ â†’ {e}")
                _db_conn = None
        return _db_conn

def _sqlite_exec_with_retry(sql, params=(), retries=5, sleep_base=0.2, commit=False):
    last_err = None
    for attempt in range(1, retries + 1):
        try:
            with _db_lock:
                conn = get_db_connection()
                if conn is None:
                    conn = _connect_sqlite()
                    globals()['_db_conn'] = conn
                cur = conn.cursor()
                cur.execute(sql, params)
                if commit:
                    conn.commit()
                try:
                    rows = cur.fetchall()
                except sqlite3.ProgrammingError:
                    rows = None
                cur.close()
                return rows
        except sqlite3.OperationalError as e:
            msg = str(e).lower()
            last_err = e
            if ("database is locked" in msg) or ("disk i/o error" in msg) or ("database is busy" in msg):
                with _db_lock:
                    try:
                        if globals().get("_db_conn"):
                            try:
                                globals()["_db_conn"].close()
                            except Exception:
                                pass
                        globals()["_db_conn"] = _connect_sqlite()
                    except Exception as ce:
                        last_err = ce
                time.sleep(sleep_base * attempt)
                continue
            else:
                raise
        except Exception as e:
            last_err = e
            time.sleep(sleep_base * attempt)
            continue
    raise last_err if last_err else RuntimeError("sqlite exec failed")

def ensure_success_db():
    try:
        _sqlite_exec_with_retry("""
            CREATE TABLE IF NOT EXISTS model_success (
                symbol TEXT,
                strategy TEXT,
                model TEXT,
                success INTEGER,
                fail INTEGER,
                PRIMARY KEY(symbol, strategy, model)
            )
        """, params=(), retries=5, commit=True)
        print("[âœ… ensure_success_db] model_success í…Œì´ë¸” í™•ì¸ ì™„ë£Œ")
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ensure_success_db ì‹¤íŒ¨ â†’ {e}")

def update_model_success(s, t, m, success):
    try:
        _sqlite_exec_with_retry("""
            INSERT INTO model_success (symbol, strategy, model, success, fail)
            VALUES (?, ?, ?, ?, ?)
            ON CONFLICT(symbol, strategy, model) DO UPDATE SET
                success = success + excluded.success,
                fail = fail + excluded.fail
        """, params=(s, t or "ì•Œìˆ˜ì—†ìŒ", m, int(success), int(not success)), retries=7, commit=True)
        print(f"[âœ… update_model_success] {s}-{t}-{m} ê¸°ë¡ ({'ì„±ê³µ' if success else 'ì‹¤íŒ¨'})")
    except Exception as e:
        print(f"[ì˜¤ë¥˜] update_model_success ì‹¤íŒ¨ â†’ {e}")

def get_model_success_rate(s, t, m):
    try:
        rows = _sqlite_exec_with_retry("""
            SELECT success, fail FROM model_success
            WHERE symbol=? AND strategy=? AND model=?
        """, params=(s, t or "ì•Œìˆ˜ì—†ìŒ", m), retries=5, commit=False)
        row = rows[0] if rows else None
        if row is None:
            return 0.0
        success_cnt, fail_cnt = row
        total = success_cnt + fail_cnt
        return (success_cnt / total) if total > 0 else 0.0
    except Exception as e:
        print(f"[ì˜¤ë¥˜] get_model_success_rate ì‹¤íŒ¨ â†’ {e}")
        return 0.0

# ì„œë²„ ì‹œì‘ ì‹œ ë³´ì¥
ensure_success_db()
ensure_prediction_log_exists()

# -------------------------
# íŒŒì¼ ë¡œë“œ/ìœ í‹¸
# -------------------------
def load_failure_count():
    path = os.path.join(LOG_DIR, "failure_count.csv")
    if not os.path.exists(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8-sig") as f:
            return {f"{r['symbol']}-{r['strategy']}": int(r["failures"]) for r in csv.DictReader(f)}
    except:
        return {}

def _normalize_status(df: pd.DataFrame) -> pd.DataFrame:
    if "status" in df.columns:
        df["status"] = df["status"].astype(str).str.lower().map(lambda x: "success" if x == "success" else "fail")
        return df
    if "success" in df.columns:
        s = df["success"].map(lambda x: str(x).strip().lower() in ["true","1","yes","y"])
        df["status"] = s.map(lambda b: "success" if b else "fail")
        return df
    df["status"] = ""
    return df

# âœ… [ë©”íƒ€ë§Œ] ì„±ê³µë¥ 
def get_meta_success_rate(strategy, min_samples: int = 1):
    """
    model == 'meta' ë§Œ ì§‘ê³„. status âˆˆ {success, fail, v_success, v_fail}
    min_samples ë¯¸ë§Œì´ë©´ 0.0 ë°˜í™˜.
    """
    try:
        df = pd.read_csv(PREDICTION_LOG, encoding="utf-8-sig", on_bad_lines="skip")
        if df.empty: return 0.0
        df = df[(df.get("strategy") == strategy) & (df.get("model") == "meta")].copy()
        if df.empty: return 0.0
        # ìƒíƒœ ì •ê·œí™”
        df["status"] = df.get("status")
        if "status" not in df.columns or df["status"].isna().all():
            # evaluate_predictionsê°€ status ì±„ìš°ê¸° ì „ì— í˜¸ì¶œë  ìˆ˜ ìˆìœ¼ë‹ˆ ë³´ìˆ˜ì ìœ¼ë¡œ ì œì™¸
            return 0.0
        df["sflag"] = df["status"].astype(str).str.lower().isin(["success","v_success"]).astype(int)
        df = df[df["status"].astype(str).str.lower().isin(["success","fail","v_success","v_fail"])]
        n = len(df)
        if n < max(1, min_samples): return 0.0
        return round(df["sflag"].mean(), 6)
    except Exception as e:
        print(f"[ì˜¤ë¥˜] get_meta_success_rate ì‹¤íŒ¨ â†’ {e}")
        return 0.0

# âœ… [ë©”íƒ€+ì„€ë„ìš°] í‰ê°€ ê±´ìˆ˜
def get_strategy_eval_count(strategy):
    try:
        df = pd.read_csv(PREDICTION_LOG, encoding="utf-8-sig", on_bad_lines="skip")
        # statusê°€ ì±„ì›Œì§„(ì„±ê³µ/ì‹¤íŒ¨) ëª¨ë“  ì˜ˆì¸¡ í¬í•¨ â†’ ì„€ë„ìš° í¬í•¨
        return len(df[(df.get("strategy")==strategy) & (df.get("status").astype(str).str.lower().isin(['success','fail','v_success','v_fail']))])
    except Exception as e:
        print(f"[ì˜¤ë¥˜] get_strategy_eval_count ì‹¤íŒ¨ â†’ {e}")
        return 0

# (ì°¸ê³ ) ì „ì²´ ì„±ê³µë¥ (ë©”íƒ€+ì„€ë„ìš° í˜¼í•©) â€” í•„ìš” ì‹œë§Œ ì‚¬ìš©
def get_actual_success_rate(strategy, min_samples: int = 1):
    try:
        df = pd.read_csv(PREDICTION_LOG, encoding="utf-8-sig", on_bad_lines="skip")
        df = df[df["strategy"] == strategy]
        df = _normalize_status(df)
        df = df[df["status"].isin(["success","fail"])]
        n = len(df)
        if n < max(1, min_samples):
            return 0.0
        return round(len(df[df["status"]=="success"]) / n, 4)
    except Exception as e:
        print(f"[ì˜¤ë¥˜] get_actual_success_rate ì‹¤íŒ¨ â†’ {e}")
        return 0.0

def log_audit_prediction(s, t, status, reason):
    row = {
        "timestamp": now_kst().isoformat(),
        "symbol": str(s or "UNKNOWN"),
        "strategy": str(t or "ì•Œìˆ˜ì—†ìŒ"),
        "status": str(status),
        "reason": str(reason)
    }
    try:
        with open(AUDIT_LOG, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(f, fieldnames=row.keys())
            if f.tell() == 0:
                w.writeheader()
            w.writerow(row)
    except:
        pass

# -------------------------
# ì˜ˆì¸¡ ë¡œê·¸ (í™•ì¥ ì¸ì ì§€ì› + ìë™ ì»¬ëŸ¼ë§ì¶¤)
# -------------------------
def _align_row_to_header(row, header):
    if len(row) < len(header):
        row = row + [""] * (len(header) - len(row))
    elif len(row) > len(header):
        row = row[:len(header)]
    return row

def log_prediction(
    symbol, strategy, direction=None, entry_price=0, target_price=0,
    timestamp=None, model=None, predicted_class=None, top_k=None,
    note="", success=False, reason="", rate=None, return_value=None,
    label=None, group_id=None, model_symbol=None, model_name=None,
    source="ì¼ë°˜", volatility=False, feature_vector=None,
    source_exchange="BYBIT",
    # --- í™•ì¥ í•„ë“œ ---
    regime=None, meta_choice=None, raw_prob=None, calib_prob=None, calib_ver=None
):
    from datetime import datetime as _dt
    from failure_db import insert_failure_record

    LOG_FILE = PREDICTION_LOG
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)

    now = _dt.now(pytz.timezone("Asia/Seoul")).isoformat() if timestamp is None else timestamp
    top_k_str = ",".join(map(str, top_k)) if top_k else ""

    predicted_class = predicted_class if predicted_class is not None else -1
    label = label if label is not None else -1
    reason = reason or "ì‚¬ìœ ì—†ìŒ"
    rate = 0.0 if rate is None else float(rate)
    return_value = 0.0 if return_value is None else float(return_value)
    entry_price = entry_price or 0.0
    target_price = target_price or 0.0

    allowed_sources = ["ì¼ë°˜","meta","evo_meta","baseline_meta","ì§„í™”í˜•","í‰ê°€","ë‹¨ì¼","ë³€ë™ì„±","train_loop","ì„€ë„ìš°"]
    if source not in allowed_sources:
        source = "ì¼ë°˜"

    row = [
        now, symbol, strategy, direction, entry_price, target_price,
        (model or ""), predicted_class, top_k_str, note,
        str(success), reason, rate, return_value, label,
        group_id, model_symbol, model_name, source, volatility, source_exchange,
        (regime or ""), (meta_choice or ""), 
        (float(raw_prob) if raw_prob is not None else ""),
        (float(calib_prob) if calib_prob is not None else ""),
        (str(calib_ver) if calib_ver is not None else "")
    ]

    try:
        write_header = not os.path.exists(LOG_FILE) or os.path.getsize(LOG_FILE) == 0
        current_header = PREDICTION_HEADERS
        if not write_header:
            h = _read_csv_header(LOG_FILE)
            current_header = h if h else PREDICTION_HEADERS

        with open(LOG_FILE, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(PREDICTION_HEADERS)
                current_header = PREDICTION_HEADERS
            w.writerow(_align_row_to_header(row, current_header))

        print(f"[âœ… ì˜ˆì¸¡ ë¡œê·¸ ê¸°ë¡ë¨] {symbol}-{strategy} class={predicted_class} | success={success} | src={source_exchange} | reason={reason}")

        if not success:
            feature_hash = f"{symbol}-{strategy}-{model or ''}-{predicted_class}-{label}-{rate}"
            safe_vector = []
            try:
                import numpy as _np
                if feature_vector is not None:
                    if isinstance(feature_vector, _np.ndarray):
                        safe_vector = feature_vector.flatten().tolist()
                    elif isinstance(feature_vector, list):
                        safe_vector = feature_vector
            except:
                safe_vector = []

            insert_failure_record(
                {
                    "symbol": symbol, "strategy": strategy, "direction": direction,
                    "model": model or "", "predicted_class": predicted_class,
                    "rate": rate, "reason": reason, "label": label, "source": source,
                    "entry_price": entry_price, "target_price": target_price,
                    "return_value": return_value
                },
                feature_hash=feature_hash, label=label, feature_vector=safe_vector
            )

    except Exception as e:
        print(f"[âš ï¸ ì˜ˆì¸¡ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨] {e}")

# -------------------------
# í•™ìŠµ ë¡œê·¸ (í›ˆë ¨ ì„±ê³µ/ì‹¤íŒ¨ëŠ” ì„±ê³µë¥  ì§‘ê³„ì—ì„œ ì œì™¸)
# -------------------------
def log_training_result(
    symbol, strategy, model="", accuracy=0.0, f1=0.0, loss=0.0,
    note="", source_exchange="BYBIT", status="success",
):
    LOG_FILE = TRAIN_LOG
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    now = datetime.datetime.now(pytz.timezone("Asia/Seoul")).isoformat()
    row = [
        now, str(symbol), str(strategy), str(model or ""),
        float(accuracy) if accuracy is not None else 0.0,
        float(f1) if f1 is not None else 0.0,
        float(loss) if loss is not None else 0.0,
        str(note or ""), str(source_exchange or "BYBIT"),
        str(status or "success")
    ]
    try:
        write_header = not os.path.exists(LOG_FILE)
        with open(LOG_FILE, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(["timestamp","symbol","strategy","model","accuracy","f1","loss","note","source_exchange","status"])
            w.writerow(row)
        print(f"[âœ… í•™ìŠµ ë¡œê·¸ ê¸°ë¡] {symbol}-{strategy} {model} status={status}")
    except Exception as e:
        print(f"[âš ï¸ í•™ìŠµ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨] {e}")
    # âš ï¸ ì£¼ì˜: í›ˆë ¨ ì„±ê³µ/ì‹¤íŒ¨ë¥¼ model_success ì§‘ê³„ì— ë„£ì§€ ì•ŠëŠ”ë‹¤(ì˜ˆì¸¡ ì„±ëŠ¥ê³¼ ë¬´ê´€).
    # í•„ìš” ì‹œ ë³„ë„ ì§‘ê³„ê°€ í•„ìš”í•˜ë©´ ë³„ë„ í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤.

# -------------------------
# ìˆ˜ìµë¥  í´ë˜ìŠ¤ ê²½ê³„ ë¡œê·¸
# -------------------------
def log_class_ranges(symbol, strategy, group_id=None, class_ranges=None, note=""):
    import csv, os
    path = os.path.join(LOG_DIR, "class_ranges.csv")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    now = now_kst().isoformat()

    class_ranges = class_ranges or []
    write_header = not os.path.exists(path)
    try:
        with open(path, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(["timestamp","symbol","strategy","group_id","idx","low","high","note"])
            for i, rng in enumerate(class_ranges):
                try:
                    lo, hi = (float(rng[0]), float(rng[1]))
                except Exception:
                    lo, hi = (None, None)
                w.writerow([now, symbol, strategy, int(group_id) if group_id is not None else 0, i, lo, hi, str(note or "")])
        print(f"[ğŸ“ í´ë˜ìŠ¤ê²½ê³„ ë¡œê·¸] {symbol}-{strategy}-g{group_id} â†’ {len(class_ranges)}ê°œ ê¸°ë¡")
    except Exception as e:
        print(f"[âš ï¸ í´ë˜ìŠ¤ê²½ê³„ ë¡œê·¸ ì‹¤íŒ¨] {e}")

# -------------------------
# ìˆ˜ìµë¥  ë¶„í¬ ìš”ì•½ ë¡œê·¸
# -------------------------
def log_return_distribution(symbol, strategy, group_id=None, horizon_hours=None, summary: dict=None, note=""):
    path = os.path.join(LOG_DIR, "return_distribution.csv")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    now = now_kst().isoformat()

    s = summary or {}
    row = [
        now, str(symbol), str(strategy),
        int(group_id) if group_id is not None else 0,
        int(horizon_hours) if horizon_hours is not None else "",
        float(s.get("min", 0.0)), float(s.get("p25", 0.0)), float(s.get("p50", 0.0)),
        float(s.get("p75", 0.0)), float(s.get("p90", 0.0)), float(s.get("p95", 0.0)),
        float(s.get("p99", 0.0)), float(s.get("max", 0.0)), int(s.get("count", 0)),
        str(note or "")
    ]

    write_header = not os.path.exists(path)
    try:
        with open(path, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(["timestamp","symbol","strategy","group_id","horizon_hours",
                            "min","p25","p50","p75","p90","p95","p99","max","count","note"])
            w.writerow(row)
        print(f"[ğŸ“ˆ ìˆ˜ìµë¥ ë¶„í¬ ë¡œê·¸] {symbol}-{strategy}-g{group_id} count={s.get('count',0)}")
    except Exception as e:
        print(f"[âš ï¸ ìˆ˜ìµë¥ ë¶„í¬ ë¡œê·¸ ì‹¤íŒ¨] {e}")

# -------------------------
# ë¼ë²¨ ë¶„í¬ ë¡œê·¸
# -------------------------
def log_label_distribution(
    symbol, strategy, group_id=None,
    counts: dict=None, total: int=None, n_unique: int=None, entropy: float=None,
    labels=None, note=""
):
    import json, math

    path = os.path.join(LOG_DIR, "label_distribution.csv")
    os.makedirs(os.path.dirname(path), exist_ok=True)
    now = now_kst().isoformat()

    if counts is None:
        from collections import Counter
        try:
            labels_list = list(map(int, list(labels or [])))
        except Exception:
            labels_list = []
        cnt = Counter(labels_list)
        total_calc = sum(cnt.values())
        probs = [c/total_calc for c in cnt.values()] if total_calc > 0 else []
        entropy_calc = -sum(p*math.log(p + 1e-12) for p in probs) if probs else 0.0
        counts = {int(k): int(v) for k, v in sorted(cnt.items())}
        total = total_calc
        n_unique = len(cnt)
        entropy = round(float(entropy_calc), 6)
    else:
        counts = {int(k): int(v) for k, v in sorted(counts.items())}
        total = int(total if total is not None else sum(counts.values()))
        n_unique = int(n_unique if n_unique is not None else len(counts))
        if entropy is None:
            probs = [c/total for c in counts.values()] if total > 0 else []
            entropy = round(float(-sum(p*math.log(p + 1e-12) for p in probs)) if probs else 0.0, 6)
        else:
            entropy = float(entropy)

    row = [
        now, str(symbol), str(strategy),
        int(group_id) if group_id is not None else 0,
        int(total),
        json.dumps(counts, ensure_ascii=False),
        int(n_unique),
        float(entropy),
        str(note or "")
    ]

    write_header = not os.path.exists(path)
    try:
        with open(path, "a", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(["timestamp","symbol","strategy","group_id","total","counts_json","n_unique","entropy","note"])
            w.writerow(row)
        print(f"[ğŸ“Š ë¼ë²¨ë¶„í¬ ë¡œê·¸] {symbol}-{strategy}-g{group_id} â†’ total={total}, classes={n_unique}, H={entropy:.4f}")
    except Exception as e:
        print(f"[âš ï¸ ë¼ë²¨ë¶„í¬ ë¡œê·¸ ì‹¤íŒ¨] {e}")

# -------------------------
# ëª¨ë¸ ì¸ë²¤í† ë¦¬ ì¡°íšŒ
# -------------------------
def get_available_models(symbol: str = None, strategy: str = None):
    try:
        model_dir = "/persistent/models"
        if not os.path.isdir(model_dir):
            return []
        out = []
        for fn in os.listdir(model_dir):
            if not fn.endswith(".pt"):
                continue
            pt_path = os.path.join(model_dir, fn)
            meta_path = pt_path.replace(".pt", ".meta.json")
            if not os.path.exists(meta_path):
                continue
            try:
                with open(meta_path, "r", encoding="utf-8") as f:
                    meta = json.load(f)
            except Exception:
                meta = {}
            sym = meta.get("symbol") or fn.split("_", 1)[0]
            strat = meta.get("strategy") or ("ë‹¨ê¸°" if "_ë‹¨ê¸°_" in fn else "ì¤‘ê¸°" if "_ì¤‘ê¸°_" in fn else "ì¥ê¸°" if "_ì¥ê¸°_" in fn else "")
            if symbol and sym != symbol:
                continue
            if strategy and strat != strategy:
                continue
            out.append({
                "pt_file": fn,
                "meta_file": os.path.basename(meta_path),
                "symbol": sym,
                "strategy": strat,
                "model": meta.get("model", ""),
                "group_id": meta.get("group_id", 0),
                "num_classes": meta.get("num_classes", 0),
                "val_f1": float(meta.get("metrics", {}).get("val_f1", 0.0)),
                "timestamp": meta.get("timestamp", "")
            })
        out.sort(key=lambda r: (r["symbol"], r["strategy"], r["model"], r["group_id"]))
        return out
    except Exception as e:
        print(f"[ì˜¤ë¥˜] get_available_models ì‹¤íŒ¨ â†’ {e}")
        return []

# -------------------------
# ìµœê·¼ ì˜ˆì¸¡ í†µê³„ ë‚´ë³´ë‚´ê¸°
# -------------------------
def export_recent_model_stats(days: int = 7, out_path: str = None):
    try:
        ensure_prediction_log_exists()
        path = PREDICTION_LOG
        if out_path is None:
            os.makedirs(LOG_DIR, exist_ok=True)
            out_path = os.path.join(LOG_DIR, "recent_model_stats.csv")

        df = pd.read_csv(path, encoding="utf-8-sig", on_bad_lines="skip")
        if df.empty:
            pd.DataFrame(columns=["symbol","strategy","model","total","success","fail","success_rate","last_ts"]).to_csv(out_path, index=False, encoding="utf-8-sig")
            return out_path

        # ìƒíƒœ ì •ê·œí™”
        df = df.copy()
        if "status" in df.columns:
            df["status"] = df["status"].astype(str).str.lower()
            df = df[df["status"].isin(["success","fail","v_success","v_fail"])]
        else:
            return out_path

        # ìµœê·¼ Nì¼ë§Œ
        ts = pd.to_datetime(df["timestamp"], errors="coerce")
        try:
            ts = ts.dt.tz_localize("Asia/Seoul")
        except Exception:
            ts = ts.dt.tz_convert("Asia/Seoul")
        cutoff = now_kst() - datetime.timedelta(days=int(days))
        df["timestamp"] = ts
        df = df[df["timestamp"] >= cutoff]

        if df.empty:
            pd.DataFrame(columns=["symbol","strategy","model","total","success","fail","success_rate","last_ts"]).to_csv(out_path, index=False, encoding="utf-8-sig")
            return out_path

        # ì§‘ê³„
        df["ok_flag"] = df["status"].isin(["success","v_success"]).astype(int)
        grp_cols = [c for c in ["symbol","strategy","model"] if c in df.columns]
        g = df.groupby(grp_cols, dropna=False).agg(
            total=("ok_flag","count"),
            success=("ok_flag","sum"),
            last_ts=("timestamp","max")
        ).reset_index()
        g["fail"] = g["total"] - g["success"]
        g["success_rate"] = (g["success"] / g["total"]).round(4)
        g = g.sort_values(["symbol","strategy","model","last_ts"]).copy()
        g.to_csv(out_path, index=False, encoding="utf-8-sig")
        print(f"[âœ… export_recent_model_stats] ì €ì¥: {out_path} (rows={len(g)})")
        return out_path
    except Exception as e:
        print(f"[âš ï¸ export_recent_model_stats ì‹¤íŒ¨] {e}")
        try:
            pd.DataFrame(columns=["symbol","strategy","model","total","success","fail","success_rate","last_ts"]).to_csv(
                out_path or os.path.join(LOG_DIR, "recent_model_stats.csv"), index=False, encoding="utf-8-sig")
        except Exception:
            pass
        return out_path or os.path.join(LOG_DIR, "recent_model_stats.csv")
