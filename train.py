import os, json, torch, torch.nn as nn, numpy as np, datetime, pytz, sys, pandas as pd, time, traceback, hashlib, sqlite3
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import MinMaxScaler
from collections import Counter

from data.utils import SYMBOLS, get_kline_by_strategy, compute_features, create_dataset
from model.base_model import get_model
from model_weight_loader import get_model_weight
from feature_importance import compute_feature_importance, save_feature_importance
from wrong_data_loader import load_training_prediction_data
from failure_db import load_existing_failure_hashes, insert_failure_record, ensure_failure_db
from window_optimizer import find_best_window
from data_augmentation import balance_classes
from config import get_NUM_CLASSES, get_FEATURE_INPUT_SIZE, get_class_groups, get_class_ranges, set_NUM_CLASSES
from logger import log_training_result, strategy_stats, load_failure_count, update_model_success

NUM_CLASSES = get_NUM_CLASSES()
FEATURE_INPUT_SIZE = get_FEATURE_INPUT_SIZE()

training_in_progress = {"ë‹¨ê¸°": False, "ì¤‘ê¸°": False, "ì¥ê¸°": False}

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_DIR = "/persistent/models"
os.makedirs(MODEL_DIR, exist_ok=True)
now_kst = lambda: datetime.datetime.now(pytz.timezone("Asia/Seoul"))
STRATEGY_WRONG_REP = {"ë‹¨ê¸°": 4, "ì¤‘ê¸°": 6, "ì¥ê¸°": 8}

def get_feature_hash_from_tensor(x, use_full=False, precision=3):
    """
    âœ… ë§ˆì§€ë§‰ timestep ë˜ëŠ” ì „ì²´ featureë¥¼ ë°˜ì˜¬ë¦¼ í›„ sha1 í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜
    """
    if x.ndim != 2 or x.shape[0] == 0:
        return "invalid"
    try:
        flat = x.flatten() if use_full else x[-1]
        rounded = [round(float(val), precision) for val in flat]
        return hashlib.sha1(",".join(map(str, rounded)).encode()).hexdigest()
    except Exception as e:
        print(f"[get_feature_hash_from_tensor ì˜¤ë¥˜] {e}")
        return "invalid"

def get_frequent_failures(min_count=5):
    """
    âœ… failure_patterns.dbì—ì„œ ë™ì¼ ì‹¤íŒ¨ê°€ min_count ì´ìƒì´ë©´ ë°˜í™˜
    """
    counter = Counter()
    try:
        with sqlite3.connect("/persistent/logs/failure_patterns.db") as conn:
            rows = conn.execute("SELECT hash FROM failure_patterns").fetchall()
            for row in rows:
                counter[row[0]] += 1
    except:
        pass
    return {h for h, cnt in counter.items() if cnt >= min_count}

def save_model_metadata(symbol, strategy, model_type, acc, f1, loss, input_size=None, class_counts=None, used_feature_columns=None):
    """
    âœ… ëª¨ë¸ ë©”íƒ€ì •ë³´ ì €ì¥
    """
    meta = {
        "symbol": symbol, "strategy": strategy, "model": model_type or "unknown",
        "input_size": int(input_size) if input_size else FEATURE_INPUT_SIZE,
        "accuracy": float(round(acc, 4)), "f1_score": float(round(f1, 4)),
        "loss": float(round(loss, 6)),
        "timestamp": now_kst().strftime("%Y-%m-%d %H:%M:%S")
    }
    if class_counts:
        meta["class_counts"] = {str(k): int(v) for k, v in class_counts.items()}
    if used_feature_columns:
        meta["used_feature_columns"] = used_feature_columns

    path = f"{MODEL_DIR}/{symbol}_{strategy}_{model_type}.meta.json"
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)
        print(f"[ë©”íƒ€ì €ì¥] {model_type} ({symbol}-{strategy}) acc={acc:.4f}")
    except Exception as e:
        print(f"[ERROR] meta ì €ì¥ ì‹¤íŒ¨: {e}")

def train_one_model(symbol, strategy, group_id=None, max_epochs=20):
    """
    âœ… ë‹¨ì¼ ì‹¬ë³¼Â·ì „ëµ (ì˜µì…˜: ê·¸ë£¹) í•™ìŠµ
    - ì‹¤íŒ¨ìƒ˜í”Œ ë³‘í•©
    - ë°ì´í„° ë¶€ì¡± ì‹œ ì¦ê°•
    - RangerAdaBelief ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
    """
    from ssl_pretrain import masked_reconstruction
    from ranger_adabelief import RangerAdaBelief as Ranger

    ensure_failure_db()
    input_size = get_FEATURE_INPUT_SIZE()
    group_ids = [group_id] if group_id is not None else [0]

    for gid in group_ids:
        model_saved = False
        try:
            print(f"âœ… [train_one_model ì‹œì‘] {symbol}-{strategy}-group{gid}")

            # 0) SSL ì‚¬ì „í•™ìŠµ(ê°€ëŠ¥í•˜ë©´)
            try:
                masked_reconstruction(symbol, strategy, input_size)
            except Exception as e:
                print(f"[âš ï¸ SSL ì‚¬ì „í•™ìŠµ ì‹¤íŒ¨] {e}")

            # 1) ë°ì´í„° ë¡œë“œ
            df = get_kline_by_strategy(symbol, strategy)
            if df is None or df.empty:
                print(f"[â© ìŠ¤í‚µ] {symbol}-{strategy}-group{gid} â†’ ë°ì´í„° ì—†ìŒ")
                log_training_result(symbol, strategy, model="all", accuracy=0.0, f1=0.0, loss=0.0, note="ë°ì´í„° ì—†ìŒ", status="skipped")
                insert_failure_record({"symbol": symbol, "strategy": strategy, "model": "all",
                                       "predicted_class": -1, "success": False, "rate": "", "reason": "ë°ì´í„° ì—†ìŒ"}, feature_vector=[])
                return

            feat = compute_features(symbol, df, strategy)
            if feat is None or feat.empty:
                print(f"[â© ìŠ¤í‚µ] {symbol}-{strategy}-group{gid} â†’ í”¼ì²˜ ì—†ìŒ")
                log_training_result(symbol, strategy, model="all", accuracy=0.0, f1=0.0, loss=0.0, note="í”¼ì²˜ ì—†ìŒ", status="skipped")
                insert_failure_record({"symbol": symbol, "strategy": strategy, "model": "all",
                                       "predicted_class": -1, "success": False, "rate": "", "reason": "í”¼ì²˜ ì—†ìŒ"}, feature_vector=[])
                return

            # 2) ìŠ¤ì¼€ì¼ë§
            features_only = feat.drop(columns=["timestamp", "strategy"], errors="ignore")
            feat_scaled = MinMaxScaler().fit_transform(features_only)

            # 3) í´ë˜ìŠ¤ ê²½ê³„
            try:
                class_ranges = get_class_ranges(symbol=symbol, strategy=strategy, group_id=gid)
            except Exception as e:
                print(f"[âŒ í´ë˜ìŠ¤ ë²”ìœ„ ê³„ì‚° ì‹¤íŒ¨] {e}")
                log_training_result(symbol, strategy, model=f"group{gid}", accuracy=0.0, f1=0.0, loss=0.0, note="í´ë˜ìŠ¤ ê³„ì‚° ì‹¤íŒ¨", status="failed")
                insert_failure_record({"symbol": symbol, "strategy": strategy, "model": "all",
                                       "predicted_class": -1, "success": False, "rate": "", "reason": "í´ë˜ìŠ¤ ê³„ì‚° ì‹¤íŒ¨"}, feature_vector=[])
                return

            num_classes = len(class_ranges)
            set_NUM_CLASSES(num_classes)

            # 4) ë¼ë²¨ ë§Œë“¤ê¸° (ë‹¤ìŒ ìˆ˜ìµë¥ )
            returns = df["close"].pct_change().fillna(0).values
            labels = []
            for r in returns:
                for i, (low, high) in enumerate(class_ranges):
                    if low <= r <= high:
                        labels.append(i); break
                else:
                    labels.append(0)

            # 5) ìœˆë„ìš° & ì‹œí€€ìŠ¤
            window = 60
            X, y = [], []
            for i in range(len(feat_scaled) - window):
                X.append(feat_scaled[i:i+window])
                y.append(labels[i + window] if i + window < len(labels) else 0)
            X, y = np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)
            print(f"[ğŸ“Š ì´ˆê¸° ìƒ˜í”Œ ìˆ˜] {len(X)}ê±´")

            # 6) ë°ì´í„° ë¶€ì¡± â†’ ì¦ê°•
            if len(X) < 50:
                print(f"[âš ï¸ ë°ì´í„° ë¶€ì¡± â†’ ì¦ê°•] {symbol}-{strategy}")
                try:
                    X, y = balance_classes(X, y, num_classes=num_classes)
                    print(f"[âœ… ì¦ê°• ì™„ë£Œ] ì´ ìƒ˜í”Œ ìˆ˜: {len(X)}")
                except Exception as e:
                    print(f"[âŒ ì¦ê°• ì‹¤íŒ¨] {e}")
                    insert_failure_record({"symbol": symbol, "strategy": strategy, "model": "all",
                                           "predicted_class": -1, "success": False, "rate": "", "reason": "ì¦ê°• ì‹¤íŒ¨"}, feature_vector=[])
                    return

            # 7) ì‹¤íŒ¨ìƒ˜í”Œ ë³‘í•©
            fail_X, fail_y = load_training_prediction_data(symbol, strategy, input_size, window, group_id=gid)
            if fail_X is not None and len(fail_X) > 0:
                print(f"[ğŸ“Œ ì‹¤íŒ¨ ìƒ˜í”Œ ë³‘í•©] {len(fail_X)}ê±´")
                unique_hashes, merged_X, merged_y = {}, [], []
                for i in range(len(fail_X)):
                    h = get_feature_hash_from_tensor(torch.tensor(fail_X[i:i+1], dtype=torch.float32))
                    if h not in unique_hashes:
                        unique_hashes[h] = True; merged_X.append(fail_X[i]); merged_y.append(fail_y[i])
                for i in range(len(X)):
                    h = get_feature_hash_from_tensor(torch.tensor(X[i:i+1], dtype=torch.float32))
                    if h not in unique_hashes:
                        unique_hashes[h] = True; merged_X.append(X[i]); merged_y.append(y[i])
                X, y = np.array(merged_X, dtype=np.float32), np.array(merged_y, dtype=np.int64)
                print(f"[ğŸ“Š ë³‘í•© í›„ ìƒ˜í”Œ ìˆ˜] {len(X)}ê±´")

            if len(X) < 10:
                print(f"[â© ìŠ¤í‚µ] {symbol}-{strategy}-group{gid} â†’ ìµœì¢… ìƒ˜í”Œ ë¶€ì¡± ({len(X)})")
                log_training_result(symbol, strategy, model=f"group{gid}", accuracy=0.0, f1=0.0, loss=0.0, note="ìµœì¢… ìƒ˜í”Œ ë¶€ì¡±", status="skipped")
                insert_failure_record({"symbol": symbol, "strategy": strategy, "model": "all",
                                       "predicted_class": -1, "success": False, "rate": "", "reason": "ìµœì¢… ìƒ˜í”Œ ë¶€ì¡±"}, feature_vector=[])
                return

            # 8) ëª¨ë¸ë³„ í•™ìŠµ
            for model_type in ["lstm", "cnn_lstm", "transformer"]:
                print(f"[ğŸ§  í•™ìŠµ ì‹œì‘] {model_type} ëª¨ë¸")
                model = get_model(model_type, input_size=input_size, output_size=num_classes).to(DEVICE)
                model_name = f"{symbol}_{strategy}_{model_type}_group{gid}_cls{num_classes}.pt"
                model_path = os.path.join(MODEL_DIR, model_name)

                optimizer = Ranger(model.parameters(), lr=0.001)
                criterion = torch.nn.CrossEntropyLoss()

                ratio = int(len(X) * 0.8)
                X_train = torch.tensor(X[:ratio], dtype=torch.float32)
                y_train = torch.tensor(y[:ratio], dtype=torch.long)
                X_val = torch.tensor(X[ratio:], dtype=torch.float32)
                y_val = torch.tensor(y[ratio:], dtype=torch.long)

                train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)
                val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64)

                total_loss = 0.0
                for epoch in range(max_epochs):
                    model.train()
                    for xb, yb in train_loader:
                        xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                        optimizer.zero_grad()
                        loss = criterion(model(xb), yb)
                        loss.backward(); optimizer.step()
                        total_loss += loss.item()
                    if (epoch + 1) % 5 == 0 or epoch == max_epochs - 1:
                        print(f"[ğŸ“ˆ Epoch {epoch+1}/{max_epochs}] Loss: {loss.item():.4f}")

                # 9) ê²€ì¦
                model.eval()
                all_preds, all_labels = [], []
                with torch.no_grad():
                    for xb, yb in val_loader:
                        preds = torch.argmax(model(xb.to(DEVICE)), dim=1).cpu().numpy()
                        all_preds.extend(preds); all_labels.extend(yb.numpy())

                acc = accuracy_score(all_labels, all_preds) if all_labels else 0.0
                f1 = f1_score(all_labels, all_preds, average='macro') if all_labels else 0.0
                print(f"[ğŸ¯ {model_type}] acc={acc:.4f}, f1={f1:.4f}")

                # 10) ì €ì¥ + ë©”íƒ€
                os.makedirs(MODEL_DIR, exist_ok=True)
                torch.save(model.state_dict(), model_path)
                with open(model_path.replace(".pt", ".meta.json"), "w", encoding="utf-8") as f:
                    json.dump({
                        "symbol": symbol, "strategy": strategy, "model": model_type,
                        "group_id": gid, "num_classes": num_classes,
                        "input_size": input_size, "timestamp": now_kst().isoformat(),
                        "fail_data_merged": bool(fail_X is not None and len(fail_X) > 0),
                        "model_name": model_name
                    }, f, ensure_ascii=False, indent=2)

                log_training_result(symbol, strategy, model=model_path, accuracy=acc, f1=f1, loss=total_loss, status="success")
                update_model_success(symbol, strategy, model_type, success=(acc > 0.6 and f1 > 0.55))
                print(f"[âœ… {model_type} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ] acc={acc:.4f}, f1={f1:.4f}")
                model_saved = True

        except Exception as e:
            print(f"[âŒ train_one_model ì‹¤íŒ¨] {symbol}-{strategy}-group{gid} â†’ {e}")
            traceback.print_exc()
            insert_failure_record({"symbol": symbol, "strategy": strategy, "model": "all",
                                   "predicted_class": -1, "success": False, "rate": "", "reason": str(e)}, feature_vector=[])

        # 11) ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë³´ê´€
        if not model_saved:
            print(f"[âš ï¸ {symbol}-{strategy}-group{gid}] í•™ìŠµ ì‹¤íŒ¨ â†’ ë”ë¯¸ ì €ì¥")
            for model_type in ["lstm", "cnn_lstm", "transformer"]:
                model_name = f"{symbol}_{strategy}_{model_type}_group{gid}_cls3.pt"
                model_path = os.path.join(MODEL_DIR, model_name)
                dummy = get_model(model_type, input_size=input_size, output_size=3).to("cpu")
                torch.save(dummy.state_dict(), model_path)
                with open(model_path.replace(".pt", ".meta.json"), "w", encoding="utf-8") as f:
                    json.dump({"symbol": symbol, "strategy": strategy, "model": model_type, "model_name": model_name}, f)
            log_training_result(symbol, strategy, model="dummy", accuracy=0.0, f1=0.0, loss=0.0, status="failed")
            insert_failure_record({"symbol": symbol, "strategy": strategy, "model": "all",
                                   "predicted_class": -1, "success": False, "rate": "", "reason": "ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨"}, feature_vector=[])

def augment_and_expand(X_train_group, y_train_group, repeat_factor, group_classes, target_count):
    """
    âœ… í´ë˜ìŠ¤ ê· í˜• ì¦ê°• ìœ í‹¸ (í•„ìš” ì‹œ ì‚¬ìš©)
    """
    import random
    from data_augmentation import add_gaussian_noise, apply_scaling, apply_shift, apply_dropout_mask

    X_aug, y_aug = [], []
    class_counts = {cls: np.sum(y_train_group == cls) for cls in group_classes}
    max_count = max(class_counts.values()) if class_counts else 1
    per_class_target = int(max_count * 0.8)

    for cls in group_classes:
        cls_indices = np.where(y_train_group == cls)[0]
        if len(cls_indices) == 0:
            dummy = np.random.normal(0, 1, (per_class_target, X_train_group.shape[1], X_train_group.shape[2])).astype(np.float32)
            X_cls_aug = dummy; y_cls_aug = np.array([cls] * per_class_target, dtype=np.int64)
        else:
            X_cls = X_train_group[cls_indices]; y_cls = y_train_group[cls_indices]
            n_repeat = int(np.ceil(per_class_target / len(cls_indices)))
            X_cls_oversampled = np.tile(X_cls, (n_repeat, 1, 1))[:per_class_target]
            y_cls_oversampled = np.tile(y_cls, n_repeat)[:per_class_target]
            X_cls_aug = []
            for x in X_cls_oversampled:
                x1 = add_gaussian_noise(x)
                x2 = apply_scaling(x1)
                x3 = apply_shift(x2)
                x4 = apply_dropout_mask(x3)
                mixup = np.random.uniform(0.7, 1.0)
                x4 = x4 * mixup + np.random.normal(0, 0.05, x4.shape).astype(np.float32) * (1 - mixup)
                X_cls_aug.append(x4)
            X_cls_aug = np.array(X_cls_aug, dtype=np.float32)
            y_cls_aug = y_cls_oversampled

        X_aug.append(X_cls_aug); y_aug.append(y_cls_aug)

    X_aug = np.concatenate(X_aug, axis=0)
    y_aug = np.concatenate(y_aug, axis=0)

    if len(X_aug) < target_count:
        idx = np.random.choice(len(X_aug), target_count - len(X_aug))
        X_aug = np.concatenate([X_aug, X_aug[idx]], axis=0)
        y_aug = np.concatenate([y_aug, y_aug[idx]], axis=0)
    else:
        X_aug = X_aug[:target_count]; y_aug = y_aug[:target_count]

    y_encoded, X_encoded = [], []
    for i, y in enumerate(y_aug):
        try:
            encoded = group_classes.index(y)
            y_encoded.append(encoded); X_encoded.append(X_aug[i])
        except ValueError:
            print(f"[âŒ ë¼ë²¨ ì¬ì¸ì½”ë”© ì˜¤ë¥˜] {y} not in group_classes â†’ ì œê±°")
            continue

    X_encoded = np.array(X_encoded, dtype=np.float32)
    y_encoded = np.array(y_encoded, dtype=np.int64)

    print(f"[âœ… augment_and_expand] ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(y_encoded)}, ë¶„í¬: {Counter(y_encoded)}")
    return X_encoded, y_encoded

def train_all_models():
    """
    âœ… SYMBOLS ì „ì²´ì— ëŒ€í•´ ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° í•™ìŠµ ìˆ˜í–‰
    """
    global training_in_progress
    from telegram_bot import send_message
    strategies = ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]

    for strategy in strategies:
        if training_in_progress.get(strategy, False):
            print(f"âš ï¸ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: {strategy}")
            continue

        print(f"ğŸš€ {strategy} í•™ìŠµ ì‹œì‘")
        training_in_progress[strategy] = True

        try:
            for symbol in SYMBOLS:
                try:
                    train_one_model(symbol, strategy)
                except Exception as e:
                    print(f"[ì˜¤ë¥˜] {symbol}-{strategy} í•™ìŠµ ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"[ì¹˜ëª… ì˜¤ë¥˜] {strategy} í•™ìŠµ ì¤‘ë‹¨: {e}")
        finally:
            training_in_progress[strategy] = False
            print(f"âœ… {strategy} í•™ìŠµ ì™„ë£Œ")

        time.sleep(5)

    send_message("âœ… ì „ì²´ í•™ìŠµ ì™„ë£Œ. ì˜ˆì¸¡ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

def train_models(symbol_list):
    """
    âœ… ê° ì‹¬ë³¼ì— ëŒ€í•´ ì „ëµë³„ ì „ì²´ ê·¸ë£¹ í•™ìŠµ â†’ ë©”íƒ€ ë³´ì • â†’ ì‹¤íŒ¨í•™ìŠµ â†’ ì§„í™”í˜• ë©”íƒ€ëŸ¬ë„ˆ
    """
    global training_in_progress
    from telegram_bot import send_message
    import maintenance_fix_meta

    strategies = ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]
    print(f"ğŸš€ [train_models] ì‹¬ë³¼ í•™ìŠµ ì‹œì‘: {symbol_list}")

    for symbol in symbol_list:
        print(f"\nğŸ” [ì‹¬ë³¼ ì‹œì‘] {symbol}")
        for strategy in strategies:
            if training_in_progress.get(strategy, False):
                print(f"âš ï¸ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: {strategy}")
                continue

            training_in_progress[strategy] = True
            try:
                train_one_model(symbol, strategy, group_id=None)
            except Exception as e:
                print(f"[âŒ í•™ìŠµ ì‹¤íŒ¨] {symbol}-{strategy} â†’ {e}")
            finally:
                training_in_progress[strategy] = False
                print(f"âœ… {symbol}-{strategy} ì „ì²´ ê·¸ë£¹ í•™ìŠµ ì™„ë£Œ")
                time.sleep(2)

    # ë©”íƒ€ì •ë³´ ë³´ì •
    try:
        maintenance_fix_meta.fix_all_meta_json()
        print(f"âœ… meta ë³´ì • ì™„ë£Œ: {symbol_list}")
    except Exception as e:
        print(f"[âš ï¸ meta ë³´ì • ì‹¤íŒ¨] {e}")

    # ì‹¤íŒ¨ í•™ìŠµ ë£¨í”„
    try:
        import failure_trainer
        failure_trainer.run_failure_training()
        print(f"âœ… ì‹¤íŒ¨í•™ìŠµ ë£¨í”„ ì™„ë£Œ")
    except Exception as e:
        print(f"[âŒ ì‹¤íŒ¨í•™ìŠµ ë£¨í”„ ì˜ˆì™¸] {e}")

    # ì§„í™”í˜• ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ
    try:
        from evo_meta_learner import train_evo_meta_loop
        train_evo_meta_loop()
        print(f"âœ… ì§„í™”í˜• ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ ì™„ë£Œ")
    except Exception as e:
        print(f"[âŒ ì§„í™”í˜• ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ ì‹¤íŒ¨] {e}")

    send_message(f"âœ… ì „ì²´ ì‹¬ë³¼ í•™ìŠµ ì™„ë£Œ: {symbol_list}")

def train_model_loop(strategy):
    """
    âœ… íŠ¹ì • strategy ë¬´í•œ ë£¨í”„ í•™ìŠµ
    """
    global training_in_progress
    if training_in_progress.get(strategy, False):
        print(f"âš ï¸ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: {strategy}")
        return

    training_in_progress[strategy] = True
    print(f"ğŸš€ {strategy} ë¬´í•œ í•™ìŠµ ë£¨í”„ ì‹œì‘")
    try:
        for symbol in SYMBOLS:
            try:
                train_one_model(symbol, strategy)
            except Exception as e:
                print(f"[ì˜¤ë¥˜] {symbol}-{strategy} í•™ìŠµ ì‹¤íŒ¨: {e}")
    finally:
        training_in_progress[strategy] = False
        print(f"âœ… {strategy} ë£¨í”„ ì¢…ë£Œ")

def train_symbol_group_loop(delay_minutes=5):
    """
    âœ… ì‹¬ë³¼ ê·¸ë£¹ ìˆœíšŒ í•™ìŠµ + ì˜ˆì¸¡ + í›„ì²˜ë¦¬ ë£¨í”„
    """
    import json, traceback
    from datetime import datetime as _dt
    import pytz
    import maintenance_fix_meta
    from data.utils import SYMBOL_GROUPS, _kline_cache, _feature_cache
    from predict import predict
    import safe_cleanup
    from evo_meta_learner import train_evo_meta_loop

    def _now():
        return _dt.now(pytz.timezone("Asia/Seoul"))

    ensure_failure_db()
    done_path = "/persistent/train_done.json"

    FORCE_TRAINING = True
    loop_count = 0
    group_count = len(SYMBOL_GROUPS)
    print(f"ğŸš€ ì „ì²´ {group_count}ê°œ ê·¸ë£¹ í•™ìŠµ ë£¨í”„ ì‹œì‘ ({_now().isoformat()})")

    while True:
        loop_count += 1
        print(f"\nğŸ”„ ê·¸ë£¹ ìˆœíšŒ ë£¨í”„ #{loop_count} ì‹œì‘ ({_now().isoformat()})")
        train_done = {}

        for group_id, group in enumerate(SYMBOL_GROUPS):
            print(f"\nğŸ“‚ [ê·¸ë£¹ {group_id+1}/{group_count}] ì§„ì…")

            if not group:
                print(f"[âš ï¸ ê·¸ë£¹ {group_id+1}] ì‹¬ë³¼ ì—†ìŒ â†’ ê±´ë„ˆëœ€")
                continue

            group_sorted = sorted(group)
            print(f"ğŸ“Š [ê·¸ë£¹ {group_id+1}] í•™ìŠµ ì‹œì‘ | ì‹¬ë³¼ ìˆ˜: {len(group_sorted)}")
            _kline_cache.clear(); _feature_cache.clear()

            for symbol in group_sorted:
                for strategy in ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]:
                    train_done.setdefault(symbol, {}).setdefault(strategy, {})
                    try:
                        class_ranges = get_class_ranges(symbol=symbol, strategy=strategy)
                        if not class_ranges or len(class_ranges) == 0:
                            raise ValueError("ë¹ˆ í´ë˜ìŠ¤ ê²½ê³„ ë°˜í™˜ë¨")
                        num_classes = len(class_ranges)
                        class_groups = get_class_groups(num_classes=num_classes)
                        MAX_GROUP_ID = len(class_groups) - 1
                    except Exception as e:
                        print(f"[âŒ í´ë˜ìŠ¤ ê²½ê³„ ê³„ì‚° ì‹¤íŒ¨] {symbol}-{strategy} â†’ {e}")
                        log_training_result(symbol, strategy, model="all", accuracy=0.0, f1=0.0, loss=0.0, note=f"í´ë˜ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}", status="failed")
                        continue

                    for gid in range(MAX_GROUP_ID + 1):
                        if not FORCE_TRAINING and train_done[symbol][strategy].get(str(gid), False):
                            print(f"[â„¹ï¸ ì¬í•™ìŠµ ìƒëµ] {symbol}-{strategy}-group{gid}")
                            continue
                        try:
                            print(f"[â–¶ í•™ìŠµ ì‹œì‘] {symbol}-{strategy}-group{gid}")
                            train_one_model(symbol, strategy, group_id=gid)
                            train_done[symbol][strategy][str(gid)] = True
                            with open(done_path, "w", encoding="utf-8") as f:
                                json.dump(train_done, f, ensure_ascii=False, indent=2)
                            print(f"[âœ… í•™ìŠµ ì™„ë£Œ] {symbol}-{strategy}-group{gid}")
                            log_training_result(symbol, strategy, model=f"group{gid}", accuracy=0.0, f1=0.0, loss=0.0, note="í•™ìŠµ ì™„ë£Œ", status="success")
                        except Exception as e:
                            print(f"[âŒ í•™ìŠµ ì‹¤íŒ¨] {symbol}-{strategy}-group{gid} â†’ {e}")
                            traceback.print_exc()
                            log_training_result(symbol, strategy, model=f"group{gid}", accuracy=0.0, f1=0.0, loss=0.0, note=str(e), status="failed")

            # ê·¸ë£¹ í•™ìŠµ í›„ ì˜ˆì¸¡
            try:
                print(f"ğŸ”® [ê·¸ë£¹ {group_id+1}] ì˜ˆì¸¡ ì‹œì‘")
                for symbol in group_sorted:
                    for strategy in ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]:
                        try:
                            print(f"[ğŸ”® ì˜ˆì¸¡] {symbol}-{strategy}")
                            predict(symbol=symbol, strategy=strategy, source="train_loop")
                        except Exception as e:
                            print(f"[âŒ ì˜ˆì¸¡ ì‹¤íŒ¨] {symbol}-{strategy} â†’ {e}")
                            traceback.print_exc()
            except Exception as e:
                print(f"[âš ï¸ ì˜ˆì¸¡ ìˆ˜í–‰ ì˜¤ë¥˜] ê·¸ë£¹ {group_id+1} â†’ {e}")

        # í›„ì²˜ë¦¬
        try:
            maintenance_fix_meta.fix_all_meta_json()
            safe_cleanup.auto_delete_old_logs()
        except Exception as e:
            print(f"[âš ï¸ í›„ì²˜ë¦¬ ì‹¤íŒ¨] â†’ {e}")

        FORCE_TRAINING = False
        time.sleep(delay_minutes * 60)

        # ì§„í™”í˜• ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ
        try:
            train_evo_meta_loop()
        except Exception as e:
            print(f"[âš ï¸ ì§„í™”í˜• ë©”íƒ€ëŸ¬ë„ˆ í•™ìŠµ ì‹¤íŒ¨] â†’ {e}")

def pretrain_ssl_features(symbol, strategy, pretrain_epochs=5):
    """
    âœ… Self-Supervised Learning pretraining (ì˜µì…˜)
    """
    print(f"â–¶ SSL Pretraining ì‹œì‘: {symbol}-{strategy}")

    df = get_kline_by_strategy(symbol, strategy)
    if df is None or df.empty:
        print("â›” ì¤‘ë‹¨: ì‹œì„¸ ë°ì´í„° ì—†ìŒ"); return

    df_feat = compute_features(symbol, df, strategy)
    if df_feat is None or df_feat.empty or df_feat.isnull().any().any():
        print("â›” ì¤‘ë‹¨: í”¼ì²˜ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” NaN"); return

    features_only = df_feat.drop(columns=["timestamp"], errors="ignore")
    feat_scaled = MinMaxScaler().fit_transform(features_only)
    X = np.expand_dims(feat_scaled, axis=1)  # (samples, 1, features)

    input_size = X.shape[2]
    model = get_model("autoencoder", input_size=input_size, output_size=input_size).to(DEVICE).train()

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    lossfn = nn.MSELoss()

    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(X, dtype=torch.float32))
    loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)

    for epoch in range(pretrain_epochs):
        total_loss = 0.0
        for xb, yb in loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            out = model(xb)
            loss = lossfn(out, yb)
            optimizer.zero_grad(); loss.backward(); optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / max(1, len(loader))
        print(f"[SSL Pretrain {epoch+1}/{pretrain_epochs}] loss={avg_loss:.6f}")

    torch.save(model.state_dict(), f"{MODEL_DIR}/{symbol}_{strategy}_ssl_pretrain.pt")
    print(f"âœ… SSL Pretraining ì™„ë£Œ: {symbol}-{strategy}")
