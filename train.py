# === train.py (FINAL, speed-tuned + SSL cache + CPU thread cap + Lightning Trainer) ===
# ‚úÖ Ï∂îÍ∞Ä: CPU Ïä§Î†àÎìú ÏÉÅÌïú(Í∏∞Î≥∏ 2). Í∏∞Ï°¥ ÌôòÍ≤ΩÎ≥ÄÏàò ÏÑ§Ï†ïÏù¥ ÏûàÏúºÎ©¥ Í∑∏ÎåÄÎ°ú Îë†.
import os
def _set_default_thread_env(name: str, val: int):
    if os.getenv(name) is None:
        os.environ[name] = str(val)
for _n in ("OMP_NUM_THREADS", "MKL_NUM_THREADS", "OPENBLAS_NUM_THREADS",
           "NUMEXPR_NUM_THREADS", "VECLIB_MAXIMUM_THREADS", "BLIS_NUM_THREADS",
           "TORCH_NUM_THREADS"):
    _set_default_thread_env(_n, int(os.getenv("CPU_THREAD_CAP", "2")))

import json, time, traceback, tempfile, io, errno
from datetime import datetime
import pytz
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import MinMaxScaler
from collections import Counter
import shutil  # ‚Üê Ï∂îÍ∞Ä
import gc      # ‚Üê Ï∂îÍ∞Ä
import threading  # ‚Üê Ï∂îÍ∞Ä (Î£®ÌîÑ Ï†úÏñ¥)

# ‚úÖ torch ÎÇ¥Î∂Ä Ïä§Î†àÎìúÎèÑ Ï†úÌïú
try:
    torch.set_num_threads(int(os.getenv("TORCH_NUM_THREADS", "2")))
except Exception:
    pass

# (ÏÑ†ÌÉù) Lightning ÏÇ¨Ïö©: ÏÑ§Ïπò Ïïà ÎêòÏñ¥ ÏûàÏúºÎ©¥ Ìè¥Î∞±
_HAS_LIGHTNING = False
try:
    import pytorch_lightning as pl
    _HAS_LIGHTNING = True
except Exception:
    _HAS_LIGHTNING = False

# ‚¨áÔ∏è Î∂àÌïÑÏöîÌïú SYMBOLS/SYMBOLS_GROUPS ÏùòÏ°¥ Ï†úÍ±∞
from data.utils import get_kline_by_strategy, compute_features, create_dataset, SYMBOL_GROUPS

from model.base_model import get_model
from feature_importance import compute_feature_importance, save_feature_importance  # Ìò∏ÌôòÏö©
from failure_db import insert_failure_record, ensure_failure_db
import logger  # log_* Î∞è ensure_prediction_log_exists ÏÇ¨Ïö©
from config import (
    get_NUM_CLASSES, get_FEATURE_INPUT_SIZE, get_class_groups,
    get_class_ranges, set_NUM_CLASSES, STRATEGY_CONFIG  # üîß Î≥ÄÍ≤Ω: STRATEGY_CONFIG Ï∂îÍ∞Ä ÏûÑÌè¨Ìä∏
)
from data_augmentation import balance_classes

# --- window_optimizer ---
from window_optimizer import find_best_window

# --- ssl_pretrain (ÏòµÏÖò) ---
try:
    from ssl_pretrain import masked_reconstruction, get_ssl_ckpt_path   # ‚úÖ Ï∂îÍ∞Ä ÏûÑÌè¨Ìä∏
except Exception:
    def masked_reconstruction(symbol, strategy, input_size):
        return None
    # ‚úÖ Ìè¥Î∞± Í≤ΩÎ°ú Ìó¨Ìçº(ssl_pretrain.py Î∂ÄÏû¨ ÏãúÏóêÎèÑ ÏïàÏ†Ñ)
    def get_ssl_ckpt_path(symbol: str, strategy: str) -> str:
        base = os.getenv("SSL_CACHE_DIR", "/persistent/ssl_models")
        os.makedirs(base, exist_ok=True)
        return f"{base}/{symbol}_{strategy}_ssl.pt"

# --- evo meta learner (ÏòµÏÖò) ---
try:
    from evo_meta_learner import train_evo_meta_loop
except Exception:
    def train_evo_meta_loop(*args, **kwargs):
        return None

# === (6Î≤à) ÏûêÎèô ÌõÑÏ≤òÎ¶¨ ÌõÖ: ÌïôÏäµ ÏßÅÌõÑ Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò/Ïã§Ìå®ÌïôÏäµ ===
def _safe_print(msg):
    try:
        print(msg, flush=True)
    except Exception:
        pass

def _try_auto_calibration(symbol, strategy, model_name):
    try:
        import calibration  # 4Î≤àÏóêÏÑú Ï∂îÍ∞ÄÎêòÎäî ÌååÏùº(ÏóÜÏúºÎ©¥ Ïä§ÌÇµ)
    except Exception as e:
        _safe_print(f"[CALIB] Î™®Îìà ÏóÜÏùå/Î°úÎìú Ïã§Ìå® ‚Üí Ïä§ÌÇµ ({e})")
        return
    for fn_name in ("learn_and_save_from_checkpoint", "learn_and_save"):
        try:
            fn = getattr(calibration, fn_name, None)
            if callable(fn):
                fn(symbol=symbol, strategy=strategy, model_name=model_name)
                _safe_print(f"[CALIB] {symbol}-{strategy}-{model_name} ‚Üí {fn_name} Ïã§Ìñâ")
                return
        except Exception as ce:
            _safe_print(f"[CALIB] {fn_name} ÏòàÏô∏ ‚Üí {ce}")
    _safe_print("[CALIB] ÏÇ¨Ïö©Í∞ÄÎä•Ìïú API ÏóÜÏùå ‚Üí Ïä§ÌÇµ")

try:
    _orig_log_training_result = logger.log_training_result
    def _wrapped_log_training_result(symbol, strategy, model="", accuracy=0.0, f1=0.0, loss=0.0,
                                     note="", source_exchange="BYBIT", status="success"):
        try:
            _orig_log_training_result(symbol, strategy, model, accuracy, f1, loss, note, source_exchange, status)
        finally:
            try:
                _try_auto_calibration(symbol, strategy, model or "")
            except Exception as e:
                _safe_print(f"[HOOK] Ï∫òÎ¶¨Î∏åÎ†àÏù¥ÏÖò ÌõÖ ÏòàÏô∏ ‚Üí {e}")
    logger.log_training_result = _wrapped_log_training_result
    _safe_print("[HOOK] logger.log_training_result ‚Üí Ï∫òÎ¶¨ ÌõÖ Ïû•Ï∞© ÏôÑÎ£å")
except Exception as _e:
    _safe_print(f"[HOOK] Ïû•Ï∞© Ïã§Ìå®(ÏõêÎ≥∏ ÎØ∏ÌÉê) ‚Üí {_e}")

def _maybe_run_failure_learn(background=True):
    import threading
    def _job():
        try:
            import failure_learn  # 7Î≤à Îã®Í≥Ñ(ÏóÜÏúºÎ©¥ Ïä§ÌÇµ)
        except Exception as e:
            _safe_print(f"[FAIL-LEARN] Î™®Îìà ÏóÜÏùå/Î°úÎìú Ïã§Ìå® ‚Üí Ïä§ÌÇµ ({e})")
            return
        for name in ("mini_retrain", "run_once", "run"):
            try:
                fn = getattr(failure_learn, name, None)
                if callable(fn):
                    fn()
                    _safe_print(f"[FAIL-LEARN] {name} Ïã§Ìñâ ÏôÑÎ£å")
                    return
            except Exception as e:
                _safe_print(f"[FAIL-LEARN] {name} ÏòàÏô∏ ‚Üí {e}")
        _safe_print("[FAIL-LEARN] Ïã§Ìñâ Í∞ÄÎä•Ìïú API ÏóÜÏùå ‚Üí Ïä§ÌÇµ")
    if background:
        threading.Thread(target=_job, daemon=True).start()
    else:
        _job()

# Ï¥àÍ∏∞ 1Ìöå Ï°∞Ïö©Ìûà ÏãúÎèÑ(ÏûàÏúºÎ©¥ ÏàòÌñâ/ÏóÜÏúºÎ©¥ Ïä§ÌÇµ)
try:
    _maybe_run_failure_learn(background=True)
except Exception as _e:
    _safe_print(f"[FAIL-LEARN] Ï¥àÍ∏∞ ÏãúÎèÑ ÏòàÏô∏ ‚Üí {_e}")
# === ÏûêÎèô ÌõÑÏ≤òÎ¶¨ ÌõÖ ÎÅù ===

NUM_CLASSES = get_NUM_CLASSES()
FEATURE_INPUT_SIZE = get_FEATURE_INPUT_SIZE()

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_DIR = "/persistent/models"
os.makedirs(MODEL_DIR, exist_ok=True)

# ‚úÖ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÏµúÍ∑º Íµ¨Í∞Ñ ÏÉÅÌïú (Ï†ïÌôïÎèÑ ÏòÅÌñ• ÏµúÏÜå / ÏÜçÎèÑ Ìñ•ÏÉÅ)
_MAX_ROWS_FOR_TRAIN = int(os.getenv("TRAIN_MAX_ROWS", "1200"))

# ‚úÖ DataLoader ÌäúÎãù(ÏïàÏ†Ñ): CPU Í∏∞Ï§Ä
_BATCH_SIZE = int(os.getenv("TRAIN_BATCH_SIZE", "128"))
_NUM_WORKERS = int(os.getenv("TRAIN_NUM_WORKERS", "0"))   # CPU Í≤ΩÎüâ ÌååÏù¥ÌîÑÎùºÏù∏: 0 Í∂åÏû•
_PIN_MEMORY = False
_PERSISTENT = False

now_kst = lambda: datetime.now(pytz.timezone("Asia/Seoul"))
training_in_progress = {"Îã®Í∏∞": False, "Ï§ëÍ∏∞": False, "Ïû•Í∏∞": False}

# --------------------------------------------------
# Ïú†Ìã∏
# --------------------------------------------------
def _atomic_write(path: str, bytes_or_str, mode: str = "wb"):
    """Ïì∞Í∏∞ Ïã§Ìå®/Ï§ëÎã® ÎåÄÎπÑ ÏõêÏûêÏ†Å Ï†ÄÏû•."""
    dirpath = os.path.dirname(path)
    os.makedirs(dirpath, exist_ok=True)
    fd, tmppath = tempfile.mkstemp(dir=dirpath, prefix=".tmp_", suffix=".swap")
    try:
        with os.fdopen(fd, mode) as f:
            if "b" in mode:
                data = bytes_or_str if isinstance(bytes_or_str, (bytes, bytearray)) else bytes_or_str.encode("utf-8")
                f.write(data)
            else:
                f.write(bytes_or_str)
            f.flush()
            os.fsync(f.fileno())
        os.replace(tmppath, path)
    finally:
        try:
            if os.path.exists(tmppath):
                os.remove(tmppath)
        except Exception:
            pass

def _log_skip(symbol, strategy, reason):
    logger.log_training_result(symbol, strategy, model="all", accuracy=0.0, f1=0.0,
                               loss=0.0, note=reason, status="skipped")
    insert_failure_record({
        "symbol": symbol, "strategy": strategy, "model": "all",
        "predicted_class": -1, "success": False, "rate": 0.0,
        "reason": reason
    }, feature_vector=[])

def _log_fail(symbol, strategy, reason):
    logger.log_training_result(symbol, strategy, model="all", accuracy=0.0, f1=0.0,
                               loss=0.0, note=reason, status="failed")
    insert_failure_record({
        "symbol": symbol, "strategy": strategy, "model": "all",
        "predicted_class": -1, "success": False, "rate": 0.0,
        "reason": reason
    }, feature_vector=[])

def _strategy_horizon_hours(strategy: str) -> int:
    return {"Îã®Í∏∞": 4, "Ï§ëÍ∏∞": 24, "Ïû•Í∏∞": 168}.get(strategy, 24)

def _future_returns_by_timestamp(df: pd.DataFrame, horizon_hours: int) -> np.ndarray:
    if df is None or len(df) == 0 or "timestamp" not in df.columns:
        return np.zeros(0 if df is None else len(df), dtype=np.float32)

    ts = pd.to_datetime(df["timestamp"], errors="coerce")
    close = df["close"].astype(float).values
    high = (df["high"] if "high" in df.columns else df["close"]).astype(float).values

    if ts.dt.tz is None:
        ts = ts.dt.tz_localize("UTC").dt.tz_convert("Asia/Seoul")
    else:
        ts = ts.dt.tz_convert("Asia/Seoul")

    out = np.zeros(len(df), dtype=np.float32)
    horizon = pd.Timedelta(hours=horizon_hours)

    j_start = 0
    for i in range(len(df)):
        t0 = ts.iloc[i]; t1 = t0 + horizon
        j = max(j_start, i)
        max_h = high[i]
        while j < len(df) and ts.iloc[j] <= t1:
            if high[j] > max_h: max_h = high[j]
            j += 1
        j_start = max(j_start, i)
        base = close[i] if close[i] > 0 else (close[i] + 1e-6)
        out[i] = float((max_h - base) / (base + 1e-12))
    return out.astype(np.float32)

def _save_model_and_meta(model: nn.Module, path_pt: str, meta: dict):
    buffer = io.BytesIO()
    torch.save(model.state_dict(), buffer)
    _atomic_write(path_pt, buffer.getvalue(), mode="wb")
    meta_json = json.dumps(meta, ensure_ascii=False, indent=2)
    _atomic_write(path_pt.replace(".pt", ".meta.json"), meta_json, mode="w")

# ‚¨áÔ∏è Ï∂îÍ∞Ä: ÏòàÏ∏°/Í¥ÄÏö∞ Ìò∏Ìôò Î≥ÑÏπ≠ Ïú†Ìã∏
def _safe_alias(src: str, dst: str):
    os.makedirs(os.path.dirname(dst), exist_ok=True)
    try:
        if os.path.islink(dst) or os.path.exists(dst):
            os.remove(dst)
    except Exception:
        pass
    try:
        os.link(src, dst)  # ÌïòÎìúÎßÅÌÅ¨ ÏãúÎèÑ
    except Exception:
        shutil.copyfile(src, dst)  # Ïã§Ìå® Ïãú Î≥µÏÇ¨

def _emit_aliases(model_path: str, meta_path: str, symbol: str, strategy: str, model_type: str):
    # 1) ÌèâÌÉÑ(legacy) Î≥ÑÏπ≠
    flat_pt   = os.path.join(MODEL_DIR, f"{symbol}_{strategy}_{model_type}.pt")
    flat_meta = flat_pt.replace(".pt", ".meta.json")
    _safe_alias(model_path, flat_pt)
    _safe_alias(meta_path, flat_meta)
    # 2) ÎîîÎ†âÌÑ∞Î¶¨ Íµ¨Ï°∞ Î≥ÑÏπ≠
    dir_pt   = os.path.join(MODEL_DIR, symbol, strategy, f"{model_type}.pt")
    dir_meta = dir_pt.replace(".pt", ".meta.json")
    _safe_alias(model_path, dir_pt)
    _safe_alias(meta_path, dir_meta)

# --------------------------------------------------
# (Ï∂îÍ∞Ä) Lightning Î™®Îìà ÎûòÌçº
# --------------------------------------------------
if _HAS_LIGHTNING:
    class LitSeqModel(pl.LightningModule):
        def __init__(self, base_model: nn.Module, lr: float = 1e-3):
            super().__init__()
            self.model = base_model
            self.criterion = nn.CrossEntropyLoss()
            self.lr = lr

        def forward(self, x):
            return self.model(x)

        def training_step(self, batch, batch_idx):
            xb, yb = batch
            logits = self(xb)
            loss = self.criterion(logits, yb)
            return loss

        def configure_optimizers(self):
            return torch.optim.Adam(self.parameters(), lr=self.lr)

# --------------------------------------------------
# Îã®Ïùº Î™®Îç∏ ÌïôÏäµ
# --------------------------------------------------
def train_one_model(symbol, strategy, group_id=None, max_epochs=12):
    result = {
        "symbol": symbol, "strategy": strategy, "group_id": int(group_id or 0),
        "models": []
    }
    try:
        print(f"‚úÖ [train_one_model ÏãúÏûë] {symbol}-{strategy}-group{group_id}")
        ensure_failure_db()

        # ‚úÖ SSL ÏÇ¨Ï†ÑÌïôÏäµ Ï∫êÏãú Ïä§ÌÇµ
        try:
            ssl_ckpt = get_ssl_ckpt_path(symbol, strategy)   # ‚úÖ Ìó¨Ìçº ÏÇ¨Ïö©(Í≤ΩÎ°ú ÌÜµÏùº)
            if not os.path.exists(ssl_ckpt):
                masked_reconstruction(symbol, strategy, FEATURE_INPUT_SIZE)
            else:
                print(f"[SSL] cache found ‚Üí skip: {ssl_ckpt}")
        except Exception as e:
            print(f"[‚ö†Ô∏è SSL ÏÇ¨Ï†ÑÌïôÏäµ Ïã§Ìå®] {e}")

        df = get_kline_by_strategy(symbol, strategy)
        if df is None or df.empty:
            _log_skip(symbol, strategy, "Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå"); return result

        # üîß Î≥ÄÍ≤Ω: Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°± Ïã†Ìò∏(augment/enough) Ïù∏ÏßÄ + Î°úÍ∑∏Ïóê ÌëúÌòÑ
        try:
            cfg = STRATEGY_CONFIG.get(strategy, {})
            _limit = int(cfg.get("limit", 300))
        except Exception:
            _limit = 300
        _min_required = max(60, int(_limit * 0.90))

        _attrs = getattr(df, "attrs", {}) if df is not None else {}
        augment_needed = bool(_attrs.get("augment_needed", len(df) < _limit))
        enough_for_training = bool(_attrs.get("enough_for_training", len(df) >= _min_required))
        print(f"[DATA] {symbol}-{strategy} rows={len(df)} limit={_limit} "
              f"min_required={_min_required} augment_needed={augment_needed} "
              f"enough_for_training={enough_for_training}")

        feat = compute_features(symbol, df, strategy)
        if feat is None or feat.empty or feat.isnull().any().any():
            _log_skip(symbol, strategy, "ÌîºÏ≤ò ÏóÜÏùå"); return result

        # 1) ÎèôÏ†Å ÌÅ¥ÎûòÏä§ Í≤ΩÍ≥Ñ
        try:
            class_ranges = get_class_ranges(symbol=symbol, strategy=strategy, group_id=group_id)
        except Exception as e:
            _log_fail(symbol, strategy, "ÌÅ¥ÎûòÏä§ Í≥ÑÏÇ∞ Ïã§Ìå®"); return result

        num_classes = len(class_ranges)
        set_NUM_CLASSES(num_classes)

        if not class_ranges or len(class_ranges) < 2:
            try:
                logger.log_class_ranges(symbol, strategy, group_id=group_id,
                                        class_ranges=class_ranges or [], note="train_skip(<2 classes)")
                logger.log_training_result(symbol, strategy, model="all", accuracy=0.0, f1=0.0, loss=0.0,
                                           note=f"Ïä§ÌÇµ: group_id={group_id}, ÌÅ¥ÎûòÏä§<2", status="skipped")
            except Exception:
                pass
            return result

        # Í≤ΩÍ≥Ñ Î°úÍ∑∏
        try:
            logger.log_class_ranges(symbol, strategy, group_id=group_id,
                                    class_ranges=class_ranges, note="train_one_model")
            print(f"[üìè ÌÅ¥ÎûòÏä§Í≤ΩÍ≥Ñ Î°úÍ∑∏] {symbol}-{strategy}-g{group_id} ‚Üí {class_ranges}")
        except Exception as e:
            print(f"[‚ö†Ô∏è log_class_ranges Ïã§Ìå®/ÎØ∏Íµ¨ÌòÑ] {e}")

        # 2) ÎØ∏Îûò ÏàòÏùµÎ•† + ÏöîÏïΩ Î°úÍ∑∏
        horizon_hours = _strategy_horizon_hours(strategy)
        future_gains = _future_returns_by_timestamp(df, horizon_hours=horizon_hours)
        try:
            fg = future_gains[np.isfinite(future_gains)]
            if fg.size > 0:
                q = np.nanpercentile(fg, [0,25,50,75,90,95,99])
                print(f"[üìà ÏàòÏùµÎ•†Î∂ÑÌè¨] {symbol}-{strategy}-g{group_id} "
                      f"min={q[0]:.4f}, p25={q[1]:.4f}, p50={q[2]:.4f}, p75={q[3]:.4f}, "
                      f"p90={q[4]:.4f}, p95={q[5]:.4f}, p99={q[6]:.4f}, max={np.nanmax(fg):.4f}")
                try:
                    logger.log_return_distribution(symbol, strategy, group_id=group_id,
                        horizon_hours=int(horizon_hours),
                        summary={"min":float(q[0]),"p25":float(q[1]),"p50":float(q[2]),
                                 "p75":float(q[3]),"p90":float(q[4]),"p95":float(q[5]),
                                 "p99":float(q[6]),"max":float(np.nanmax(fg)),"count":int(fg.size)},
                        note="train_one_model")
                except Exception as le:
                    print(f"[‚ö†Ô∏è log_return_distribution Ïã§Ìå®/ÎØ∏Íµ¨ÌòÑ] {le}")
        except Exception as e:
            print(f"[‚ö†Ô∏è ÏàòÏùµÎ•†Î∂ÑÌè¨ ÏöîÏïΩ Ïã§Ìå®] {e}")

        # 3) ÎùºÎ≤®ÎßÅ + Î∂ÑÌè¨ Î°úÍ∑∏  ‚îÄ‚îÄ ‚òÖ Í≤ΩÍ≥Ñ Ïù¥ÌÉà Î≥¥Ï†ï(ÌÅ¥Î¶¨Ìïë) Ï∂îÍ∞Ä
        labels = []
        clipped_low, clipped_high, unmatched = 0, 0, 0

        lo0 = class_ranges[0][0]
        hi_last = class_ranges[-1][1]

        for r in future_gains:
            if not np.isfinite(r):
                r = lo0
            if r < lo0:
                labels.append(0); clipped_low += 1; continue
            if r > hi_last:
                labels.append(len(class_ranges) - 1); clipped_high += 1; continue
            idx = None
            for i, (lo, hi) in enumerate(class_ranges):
                if lo <= r <= hi:
                    idx = i; break
            if idx is None:
                idx = len(class_ranges) - 1 if r > hi_last else 0
                unmatched += 1
            labels.append(idx)

        if clipped_low or clipped_high or unmatched:
            print(f"[üîß ÎùºÎ≤® Î≥¥Ï†ï] {symbol}-{strategy}-g{group_id} "
                  f"low_clip={clipped_low}, high_clip={clipped_high}, unmatched={unmatched}")

        labels = np.array(labels, dtype=np.int64)

        features_only = feat.drop(columns=["timestamp", "strategy"], errors="ignore")
        feat_scaled = MinMaxScaler().fit_transform(features_only)

        # ‚úÖ ÏÜçÎèÑ Í∞úÏÑ†: ÏµúÍ∑º Íµ¨Í∞ÑÎßå ÏÇ¨Ïö©(ÎùºÎ≤®Í≥º Ï†ïÎ†¨ Ïú†ÏßÄ)
        if len(feat_scaled) > _MAX_ROWS_FOR_TRAIN or len(labels) > _MAX_ROWS_FOR_TRAIN:
            cut = min(_MAX_ROWS_FOR_TRAIN, len(feat_scaled), len(labels))
            feat_scaled = feat_scaled[-cut:]
            labels = labels[-cut:]

        # 4) ÏµúÏ†Å ÏúàÎèÑÏö∞(ÌÉêÏÉâ Ìè≠ Ï∂ïÏÜå)
        try:
            best_window = find_best_window(symbol, strategy, window_list=[20, 40], group_id=group_id)
        except Exception:
            best_window = 40
        window = int(max(5, best_window))
        # üîí Îç∞Ïù¥ÌÑ∞ Í∏∏Ïù¥Î•º ÎÑòÏßÄ ÏïäÎèÑÎ°ù ÌÅ¥Îû®ÌîÑ
        window = int(min(window, max(6, len(feat_scaled) - 1)))

        # 5) ÏãúÌÄÄÏä§ ÏÉùÏÑ±
        X, y = [], []
        for i in range(len(feat_scaled) - window):
            X.append(feat_scaled[i:i+window])
            y_idx = i + window - 1
            y.append(labels[y_idx] if 0 <= y_idx < len(labels) else 0)
        X, y = np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)

        # ÎùºÎ≤®/Î∂ÑÌè¨ Î°úÍ∑∏
        try:
            label_counts = Counter(y.tolist())
            total_labels = int(len(y))
            probs = np.array(list(label_counts.values()), dtype=np.float64)
            entropy = float(-(probs / max(1, probs.sum()) * np.log2((probs / max(1, probs.sum())) + 1e-12)).sum()) if probs.sum() > 0 else 0.0
            logger.log_label_distribution(symbol, strategy, group_id=group_id,
                                          counts=dict(label_counts), total=total_labels,
                                          n_unique=int(len(label_counts)), entropy=float(entropy),
                                          note=f"window={window}, recent_cap={len(feat_scaled)}")
            print(f"[üßÆ ÎùºÎ≤®Î∂ÑÌè¨ Î°úÍ∑∏] {symbol}-{strategy}-g{group_id} total={total_labels}, "
                  f"classes={len(label_counts)}, H={entropy:.4f}")
        except Exception as e:
            print(f"[‚ö†Ô∏è log_label_distribution Ïã§Ìå®/ÎØ∏Íµ¨ÌòÑ] {e}")

        # üîß Î≥ÄÍ≤Ω: Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±(enough_for_training=False)Ïùº Îïå, Í∏∞Ï°¥ fallback/Î∞∏Îü∞Ïã± Í≤ΩÎ°úÎ•º Ïö∞ÏÑ† ÏãúÎèÑ.
        if len(X) < 20:
            try:
                res = create_dataset(feat.to_dict(orient="records"), window=window, strategy=strategy, input_size=FEATURE_INPUT_SIZE)
                if isinstance(res, tuple) and len(res) >= 2:
                    X_fb, y_fb = res[0], res[1]
                else:
                    X_fb, y_fb = res
                if isinstance(X_fb, np.ndarray) and len(X_fb) > 0:
                    X, y = X_fb.astype(np.float32), y_fb.astype(np.int64)
            except Exception as e:
                print(f"[‚ö†Ô∏è fallback Ïã§Ìå®] {e}")

        if len(X) < 10:
            # Îç∞Ïù¥ÌÑ∞Í∞Ä Ï∂©Î∂ÑÏπò ÏïäÏúºÎ©¥ ÏïàÏ†Ñ Ïä§ÌÇµ(Ïã§Ìå® Í∏∞Î°ù Ìè¨Ìï®)
            _log_skip(symbol, strategy, f"ÏµúÏ¢Ö ÏÉòÌîå Î∂ÄÏ°± (rows={len(df)}, limit={_limit}, min_required={_min_required})")
            return result

        try:
            if len(X) < 200:
                X, y = balance_classes(X, y, num_classes=num_classes)
        except Exception as e:
            print(f"[‚ö†Ô∏è Î∞∏Îü∞Ïã± Ïã§Ìå®] {e}")

        # 6) ÌïôÏäµ/ÌèâÍ∞Ä/Ï†ÄÏû•
        for model_type in ["lstm", "cnn_lstm", "transformer"]:
            base_model = get_model(model_type, input_size=FEATURE_INPUT_SIZE, output_size=num_classes).to(DEVICE)

            val_len = max(1, int(len(X) * 0.2))
            if len(X) - val_len < 1: val_len = len(X) - 1
            train_X, val_X = X[:-val_len], X[-val_len:]
            train_y, val_y = y[:-val_len], y[-val_len:]

            train_loader = DataLoader(
                TensorDataset(torch.tensor(train_X), torch.tensor(train_y)),
                batch_size=_BATCH_SIZE, shuffle=True,
                num_workers=_NUM_WORKERS, pin_memory=_PIN_MEMORY, persistent_workers=_PERSISTENT
            )
            val_loader = DataLoader(
                TensorDataset(torch.tensor(val_X), torch.tensor(val_y)),
                batch_size=_BATCH_SIZE,
                num_workers=_NUM_WORKERS, pin_memory=_PIN_MEMORY, persistent_workers=_PERSISTENT
            )

            total_loss = 0.0

            if _HAS_LIGHTNING:
                # ‚úÖ Lightning Í∏∞Î∞ò ÌïôÏäµ(Í∏∞Î≥∏ Í≤ΩÎüâ ÏòµÏÖò, Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏/Î°úÍ±∞ ÎπÑÌôúÏÑ±)
                lit = LitSeqModel(base_model, lr=1e-3)
                trainer = pl.Trainer(
                    max_epochs=max_epochs,
                    accelerator="gpu" if torch.cuda.is_available() else "cpu",
                    devices=1,
                    enable_checkpointing=False,
                    logger=False,
                    enable_model_summary=False,
                    enable_progress_bar=False,
                )
                # LightningÏùÄ Ïä§ÌÖù ÏÜêÏã§Îßå Î∞òÌôòÌïòÎØÄÎ°ú Ï¥ùÌï©ÏùÄ ÏàòÎèô Ï∂îÏ†ÅÌïòÏßÄ ÏïäÏùå(ÌèâÍ∞Ä ÌõÑ Ï†ÄÏû•)
                trainer.fit(lit, train_dataloaders=train_loader, val_dataloaders=val_loader)
                model = lit.model.to(DEVICE)
            else:
                # üîÅ Í∏∞Ï°¥ ÏàòÎèô Î£®ÌîÑ Ìè¥Î∞±
                model = base_model
                optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
                criterion = nn.CrossEntropyLoss()

                for epoch in range(max_epochs):
                    model.train()
                    for xb, yb in train_loader:
                        xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                        logits = model(xb)
                        loss = criterion(logits, yb)
                        if not torch.isfinite(loss): continue
                        optimizer.zero_grad(); loss.backward(); optimizer.step()
                        total_loss += float(loss.item())

            # Í≤ÄÏ¶ù ÌèâÍ∞Ä(Í≥µÌÜµ)
            model.eval()
            all_preds, all_labels = [], []
            with torch.no_grad():
                for xb, yb in val_loader:
                    preds = torch.argmax(model(xb.to(DEVICE)), dim=1).cpu().numpy()
                    all_preds.extend(preds); all_labels.extend(yb.numpy())
            acc = float(accuracy_score(all_labels, all_preds))
            f1 = float(f1_score(all_labels, all_preds, average="macro"))

            model_name = f"{symbol}_{strategy}_{model_type}_group{int(group_id) if group_id is not None else 0}_cls{int(num_classes)}.pt"
            model_path = os.path.join(MODEL_DIR, model_name)
            meta = {
                "symbol": symbol, "strategy": strategy, "model": model_type,
                "group_id": int(group_id) if group_id is not None else 0,
                "num_classes": int(num_classes), "input_size": int(FEATURE_INPUT_SIZE),
                "metrics": {"val_acc": acc, "val_f1": f1, "train_loss_sum": float(total_loss)},
                "timestamp": now_kst().isoformat(), "model_name": model_name,
                "window": int(window), "recent_cap": int(len(feat_scaled)),
                "engine": "lightning" if _HAS_LIGHTNING else "manual",
                # üîß Î≥ÄÍ≤Ω: Îç∞Ïù¥ÌÑ∞ ÏÉÅÌÉú ÌîåÎûòÍ∑∏Î•º Î©îÌÉÄÏóê Ï†ÄÏû•(Í¥ÄÏö∞/ÌõÑÏÜç ÏßÑÎã® Ïö©Ïù¥)
                "data_flags": {
                    "rows": int(len(df)), "limit": int(_limit), "min_required": int(_min_required),
                    "augment_needed": bool(augment_needed), "enough_for_training": bool(enough_for_training)
                }
            }
            _save_model_and_meta(model, model_path, meta)

            # ‚¨áÔ∏è Ï∂îÍ∞Ä: ÏòàÏ∏°/Î™®ÎãàÌÑ∞ Ìò∏Ìôò Î≥ÑÏπ≠ ÏÉùÏÑ±
            _emit_aliases(model_path, model_path.replace(".pt", ".meta.json"),
                          symbol, strategy, model_type)

            # üîß Î≥ÄÍ≤Ω: noteÏóê data_flags ÏöîÏïΩ Ìè¨Ìï®
            logger.log_training_result(
                symbol, strategy, model=model_name, accuracy=acc, f1=f1,
                loss=float(total_loss),
                note=(f"train_one_model(window={window}, cap={len(feat_scaled)}, "
                      f"engine={'lightning' if _HAS_LIGHTNING else 'manual'}, "
                      f"data_flags={{rows:{len(df)},limit:{_limit},min:{_min_required},"
                      f"aug:{int(augment_needed)},enough:{int(enough_for_training)}}})"),
                source_exchange="BYBIT", status="success"
            )
            result["models"].append({
                "type": model_type, "acc": acc, "f1": f1,
                "loss_sum": float(total_loss), "pt": model_path,
                "meta": model_path.replace(".pt", ".meta.json")
            })

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        return result

    except Exception as e:
        _log_fail(symbol, strategy, str(e))
        return result

# --------------------------------------------------
# (Ï∂îÍ∞Ä) Í≤ΩÎüâ Ï†ïÎ¶¨ Ïú†Ìã∏ ‚Äî Í∑∏Î£π Ï¢ÖÎ£å Ïãú Ìò∏Ï∂ú
# --------------------------------------------------
def _prune_caches_and_gc():
    # cache Î™®ÎìàÏù¥ ÏûàÏúºÎ©¥ prune
    try:
        from cache import CacheManager as _CM
        try:
            before = _CM.stats()
        except Exception:
            before = None
        pruned = _CM.prune()
        try:
            after = _CM.stats()
        except Exception:
            after = None
        print(f"[CACHE] prune ok: before={before}, after={after}, pruned={pruned}")
    except Exception as e:
        print(f"[CACHE] Î™®Îìà ÏóÜÏùå/Ïä§ÌÇµ ({e})")
    # safe_cleanupÏùò Í≤ΩÎüâ Ï†ïÎ¶¨ Ìä∏Î¶¨Í±∞(ÏûàÏúºÎ©¥ ÏÇ¨Ïö©)
    try:
        from safe_cleanup import trigger_light_cleanup
        trigger_light_cleanup()
    except Exception:
        pass
    try:
        gc.collect()
    except Exception:
        pass

# --------------------------------------------------
# ‚úÖ Í∑∏Î£π Ï†ïÎ†¨ Ìó¨Ìçº (Ïã†Í∑ú): BTCUSDTÍ∞Ä Ìè¨Ìï®Îêú Í∑∏Î£πÏùÑ Îß® ÏïûÏúºÎ°ú ÌöåÏ†Ñ,
#    Í∑∏ Í∑∏Î£π ÎÇ¥Î∂ÄÏóêÏÑúÎèÑ BTCUSDTÎ•º Ï≤´ ÏõêÏÜåÎ°ú Í≥†Ï†ï. ÎÇòÎ®∏ÏßÄ ÏàúÏÑúÎäî Î≥¥Ï°¥.
# --------------------------------------------------
def _rotate_groups_starting_with(groups, anchor_symbol="BTCUSDT"):
    norm = [list(g) for g in groups]
    anchor_gid = None
    for i, g in enumerate(norm):
        if anchor_symbol in g:
            anchor_gid = i
            break
    if anchor_gid is not None and anchor_gid != 0:
        norm = norm[anchor_gid:] + norm[:anchor_gid]
    if norm and anchor_symbol in norm[0]:
        norm[0] = [anchor_symbol] + [s for s in norm[0] if s != anchor_symbol]
    return norm

# --------------------------------------------------
# Ï†ÑÏ≤¥ ÌïôÏäµ Î£®Ìã¥  (‚úÖ stop_event ÏßÄÏõê)
# --------------------------------------------------
def train_models(symbol_list, stop_event: threading.Event | None = None):
    strategies = ["Îã®Í∏∞", "Ï§ëÍ∏∞", "Ïû•Í∏∞"]
    for symbol in symbol_list:
        if stop_event is not None and stop_event.is_set():
            print("[STOP] train_models: stop_event Í∞êÏßÄ ‚Üí Ï°∞Í∏∞ Ï¢ÖÎ£å"); return
        for strategy in strategies:
            if stop_event is not None and stop_event.is_set():
                print("[STOP] train_models: stop_event Í∞êÏßÄ(strategy loop) ‚Üí Ï°∞Í∏∞ Ï¢ÖÎ£å"); return
            try:
                class_ranges = get_class_ranges(symbol=symbol, strategy=strategy)
                if not class_ranges:
                    raise ValueError("Îπà ÌÅ¥ÎûòÏä§ Í≤ΩÍ≥Ñ")
                num_classes = len(class_ranges)
                groups = get_class_groups(num_classes=num_classes)
                max_gid = len(groups) - 1
            except Exception as e:
                _log_fail(symbol, strategy, f"ÌÅ¥ÎûòÏä§ Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
                continue

            for gid in range(max_gid + 1):
                if stop_event is not None and stop_event.is_set():
                    print("[STOP] train_models: stop_event Í∞êÏßÄ(group loop) ‚Üí Ï°∞Í∏∞ Ï¢ÖÎ£å"); return
                # Í∞Å Í∑∏Î£πÎ≥Ñ ÌÅ¥ÎûòÏä§ Ïàò 2Í∞ú ÎØ∏ÎßåÏù¥Î©¥ Ïä§ÌÇµ
                try:
                    grp_ranges = get_class_ranges(symbol=symbol, strategy=strategy, group_id=gid)
                    if not grp_ranges or len(grp_ranges) < 2:
                        try:
                            logger.log_class_ranges(symbol, strategy, group_id=gid,
                                                    class_ranges=grp_ranges or [], note="train_skip(<2 classes)")
                            logger.log_training_result(symbol, strategy, model=f"group{gid}",
                                                       accuracy=0.0, f1=0.0, loss=0.0,
                                                       note=f"Ïä§ÌÇµ: group_id={gid}, ÌÅ¥ÎûòÏä§<2", status="skipped")
                        except Exception:
                            pass
                        continue
                except Exception as e:
                    try:
                        logger.log_training_result(symbol, strategy, model=f"group{gid}",
                                                   accuracy=0.0, f1=0.0, loss=0.0,
                                                   note=f"Ïä§ÌÇµ: group_id={gid}, Í≤ΩÍ≥ÑÍ≥ÑÏÇ∞Ïã§Ìå® {e}", status="skipped")
                    except Exception:
                        pass
                    continue

                train_one_model(symbol, strategy, group_id=gid)
                if stop_event is not None and stop_event.is_set():
                    print("[STOP] train_models: stop_event Í∞êÏßÄ(after one model) ‚Üí Ï°∞Í∏∞ Ï¢ÖÎ£å"); return
                time.sleep(0.5)

    try:
        import maintenance_fix_meta
        maintenance_fix_meta.fix_all_meta_json()
    except Exception as e:
        print(f"[‚ö†Ô∏è meta Î≥¥Ï†ï Ïã§Ìå®] {e}")

    try:
        import failure_trainer
        failure_trainer.run_failure_training()
    except Exception as e:
        print(f"[‚ö†Ô∏è Ïã§Ìå®ÌïôÏäµ Î£®ÌîÑ ÏòàÏô∏] {e}")

    try:
        train_evo_meta_loop()
    except Exception as e:
        print(f"[‚ö†Ô∏è ÏßÑÌôîÌòï Î©îÌÉÄÎü¨ÎÑà ÌïôÏäµ Ïã§Ìå®] {e}")

# --------------------------------------------------
# Í∑∏Î£π Î£®ÌîÑ(Í∑∏Î£π ÏôÑÎ£å ÌõÑ ÏòàÏ∏° 1Ìöå)  (‚úÖ stop_event ÏßÄÏõê)
# --------------------------------------------------
def train_symbol_group_loop(sleep_sec: int = 0, stop_event: threading.Event | None = None):
    try:
        from predict import predict  # ÏòàÏ∏° Ìï®Ïàò Î∂àÎü¨Ïò§Í∏∞

        # ‚úÖ ÌïôÏäµ/ÏòàÏ∏° Î°úÍ∑∏ ÌååÏùº/Ìó§Îçî Î≥¥Ïû•(Ï°¥Ïû¨ ÏãúÎßå)
        try:
            if hasattr(logger, "ensure_train_log_exists"):
                logger.ensure_train_log_exists()
        except Exception:
            pass
        try:
            if hasattr(logger, "ensure_prediction_log_exists"):
                logger.ensure_prediction_log_exists()
        except Exception:
            pass

        # ÏõêÎ≥∏ Í∑∏Î£π ‚Üí BTCUSDT Í∑∏Î£πÏùÑ Îß® ÏïûÏúºÎ°ú ÌöåÏ†Ñ
        groups = _rotate_groups_starting_with(SYMBOL_GROUPS, anchor_symbol="BTCUSDT")

        for idx, group in enumerate(groups):
            if stop_event is not None and stop_event.is_set():
                print("[STOP] train_symbol_group_loop: stop_event Í∞êÏßÄ(group idx) ‚Üí Ï¢ÖÎ£å"); break

            print(f"üöÄ [train_symbol_group_loop] Í∑∏Î£π #{idx+1}/{len(groups)} ‚Üí {group} | mode=per_symbol_all_horizons")

            # 1) Í∑∏Î£π ÌïôÏäµ
            train_models(group, stop_event=stop_event)
            if stop_event is not None and stop_event.is_set():
                print("[STOP] train_symbol_group_loop: stop_event Í∞êÏßÄ(after train_models) ‚Üí Ï¢ÖÎ£å"); break

            # ‚úÖ Î™®Îç∏ Ï†ÄÏû• ÏßÅÌõÑ I/O ÏïàÏ†ïÌôî
            time.sleep(0.2)

            # 2) Í∑∏Î£π ÌïôÏäµ ÏôÑÎ£å ÌõÑ Îã® Ìïú Î≤àÏî© ÏòàÏ∏°
            for symbol in group:
                if stop_event is not None and stop_event.is_set():
                    print("[STOP] train_symbol_group_loop: stop_event Í∞êÏßÄ(pred loop) ‚Üí Ï¢ÖÎ£å"); break
                for strategy in ["Îã®Í∏∞", "Ï§ëÍ∏∞", "Ïû•Í∏∞"]:
                    if stop_event is not None and stop_event.is_set():
                        print("[STOP] train_symbol_group_loop: stop_event Í∞êÏßÄ(pred inner) ‚Üí Ï¢ÖÎ£å"); break
                    try:
                        print(f"üîÆ [Ï¶âÏãúÏòàÏ∏°] {symbol}-{strategy}")
                        predict(symbol, strategy, source="Í∑∏Î£πÏßÅÌõÑ", model_type=None)
                    except Exception as e:
                        print(f"[‚ö†Ô∏è ÏòàÏ∏° Ïã§Ìå®] {symbol}-{strategy}: {e}")

            # 3) Í∑∏Î£π Ï¢ÖÎ£å Ï†ïÎ¶¨
            _prune_caches_and_gc()

            if sleep_sec > 0:
                for _ in range(sleep_sec):
                    if stop_event is not None and stop_event.is_set():
                        print("[STOP] train_symbol_group_loop: stop_event Í∞êÏßÄ(sleep) ‚Üí Ï¢ÖÎ£å"); break
                    time.sleep(1)
                if stop_event is not None and stop_event.is_set():
                    break

        print("‚úÖ train_symbol_group_loop ÏôÑÎ£å")
    except Exception as e:
        print(f"[‚ùå train_symbol_group_loop ÏòàÏô∏] {e}")

# --------------------------------------------------
# ‚úÖ Î£®ÌîÑ Ï†úÏñ¥ Ïú†Ìã∏: Ï§ëÎ≥µ Î∞©ÏßÄÏö© (Îã®Ïùº Î£®ÌîÑ Î≥¥Ïû•)
# --------------------------------------------------
_TRAIN_LOOP_THREAD: threading.Thread | None = None
_TRAIN_LOOP_STOP: threading.Event | None = None
_TRAIN_LOOP_LOCK = threading.Lock()

def start_train_loop(force_restart: bool = False, sleep_sec: int = 0):
    """ÌïôÏäµ Î£®ÌîÑÎ•º 1Í∞úÎßå Ïã§Ìñâ. force_restart=TrueÎ©¥ Í∏∞Ï°¥ Î£®ÌîÑÎ•º Î®ºÏ†Ä Ï†ïÏßÄ."""
    global _TRAIN_LOOP_THREAD, _TRAIN_LOOP_STOP
    with _TRAIN_LOOP_LOCK:
        # Ïù¥ÎØ∏ ÎèåÍ≥† ÏûàÏúºÎ©¥‚Ä¶
        if _TRAIN_LOOP_THREAD is not None and _TRAIN_LOOP_THREAD.is_alive():
            if not force_restart:
                print("‚ÑπÔ∏è start_train_loop: Í∏∞Ï°¥ Î£®ÌîÑÍ∞Ä Ïã§Ìñâ Ï§ë ‚Üí Ïû¨ÏãúÏûë ÏÉùÎûµ"); return False
            # Í∞ïÏ†ú Ïû¨ÏãúÏûë: Î®ºÏ†Ä Ï†ïÏßÄ
            print("üõë start_train_loop: Í∏∞Ï°¥ Î£®ÌîÑ Ï†ïÏßÄ ÏãúÎèÑ")
            stop_train_loop(timeout=30)

        # ÏÉà Ïù¥Î≤§Ìä∏/Ïä§Î†àÎìú Ï§ÄÎπÑ
        _TRAIN_LOOP_STOP = threading.Event()
        def _runner():
            try:
                train_symbol_group_loop(sleep_sec=sleep_sec, stop_event=_TRAIN_LOOP_STOP)
            finally:
                # Ïä§Î†àÎìú Ï¢ÖÎ£å Ïãú ÌÅ¥Î¶¨Ïñ¥
                print("‚ÑπÔ∏è train loop thread Ï¢ÖÎ£å")
        _TRAIN_LOOP_THREAD = threading.Thread(target=_runner, daemon=True)
        _TRAIN_LOOP_THREAD.start()
        print("‚úÖ train loop ÏãúÏûëÎê® (Îã®Ïùº Ïù∏Ïä§ÌÑ¥Ïä§ Î≥¥Ïû•)")
        return True

def stop_train_loop(timeout: int | float | None = 30):
    """Ïã§Ìñâ Ï§ë Î£®ÌîÑÎ•º ÏïàÏ†ÑÌïòÍ≤å Ï§ëÎã® ÏöîÏ≤≠ÌïòÍ≥† ÎåÄÍ∏∞."""
    global _TRAIN_LOOP_THREAD, _TRAIN_LOOP_STOP
    with _TRAIN_LOOP_LOCK:
        if _TRAIN_LOOP_THREAD is None or not _TRAIN_LOOP_THREAD.is_alive():
            print("‚ÑπÔ∏è stop_train_loop: Ïã§Ìñâ Ï§ëÏù∏ Î£®ÌîÑ ÏóÜÏùå"); return True
        if _TRAIN_LOOP_STOP is None:
            print("‚ö†Ô∏è stop_train_loop: stop_event ÏóÜÏùå(ÎπÑÏ†ïÏÉÅ ÏÉÅÌÉú)"); return False
        # Ï§ëÎã® ÏöîÏ≤≠
        _TRAIN_LOOP_STOP.set()
        _TRAIN_LOOP_THREAD.join(timeout=timeout)
        if _TRAIN_LOOP_THREAD.is_alive():
            print("‚ö†Ô∏è stop_train_loop: ÌÉÄÏûÑÏïÑÏõÉ ‚Äî Ïó¨Ï†ÑÌûà Ïã§Ìñâ Ï§ë")
            return False
        _TRAIN_LOOP_THREAD = None
        _TRAIN_LOOP_STOP = None
        print("‚úÖ stop_train_loop: Ï†ïÏÉÅ Ï¢ÖÎ£å")
        return True

if __name__ == "__main__":
    # ÌïÑÏöî Ïãú Í∞ÑÎã® Ïã§Ìñâ ÏßÑÏûÖÏ†ê
    try:
        # Îã®Ïùº Î£®ÌîÑ Î≥¥Ïû• Î∞©ÏãùÏúºÎ°ú ÏãúÏûë
        start_train_loop(force_restart=True, sleep_sec=0)
    except Exception as e:
        print(f"[MAIN] ÏòàÏô∏: {e}")
