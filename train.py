# train.py â€” SPEED v2.6 FINAL (í¬ì†Œí´ë˜ìŠ¤ ì œê±° ì—†ì´ ê·¸ë£¹ í´ë˜ìŠ¤ ì „ë¶€ í•™ìŠµ ë²„ì „)
# -*- coding: utf-8 -*-
import sitecustomize
import os, time, glob, shutil, json, random, traceback, threading, gc, csv, re
from datetime import datetime
from typing import Optional, List, Tuple, Dict, Any
from logger import ensure_train_log_exists

import numpy as np, pandas as pd, pytz, torch, torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import StratifiedShuffleSplit
from collections import Counter
from logger import (
    log_prediction,
    ensure_prediction_log_exists,
    extract_candle_returns,      # â† ë°©ê¸ˆ ë§Œë“  ê±°
    make_return_histogram,       # â† ë°©ê¸ˆ ë§Œë“  ê±°
)

# ---------- ê³µìš© ë©”ëª¨ë¦¬ ìœ í‹¸ ----------
def _safe_empty_cache():
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except Exception:
        pass


def _release_memory(*objs):
    for o in objs:
        try:
            del o
        except Exception:
            pass
    gc.collect()
    _safe_empty_cache()


# ---------- ê¸°ë³¸ í™˜ê²½/ì‹œë“œ ----------
def _set_default_thread_env(n: str, v: int):
    if os.getenv(n) is None:
        os.environ[n] = str(v)


for _n in (
    "OMP_NUM_THREADS",
    "MKL_NUM_THREADS",
    "OPENBLAS_NUM_THREADS",
    "NUMEXPR_NUM_THREADS",
    "VECLIB_MAXIMUM_THREADS",
    "BLIS_NUM_THREADS",
    "TORCH_NUM_THREADS",
):
    _set_default_thread_env(_n, int(os.getenv("CPU_THREAD_CAP", "1")))
try:
    torch.set_num_threads(int(os.getenv("TORCH_NUM_THREADS", "1")))
except:
    pass

# --- CPU ìµœì í™” ---
try:
    torch.set_num_interop_threads(1)
except:
    pass
try:
    torch.backends.mkldnn.enabled = True
except:
    pass
try:
    torch.use_deterministic_algorithms(False)
    torch.set_deterministic_debug(False)
except:
    pass


def set_global_seed(s: int = 20240101):
    os.environ["PYTHONHASHSEED"] = str(s)
    random.seed(s)
    np.random.seed(s)
    torch.manual_seed(s)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(s)
    try:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    except:
        pass


set_global_seed(int(os.getenv("GLOBAL_SEED", "20240101")))

# ---------- ì™¸ë¶€ ì˜ì¡´ ----------
from model_io import save_model

# [í’€ë°±] data.utils â†’ utils
try:
    from data.utils import (
        get_kline_by_strategy,
        compute_features,
        SYMBOL_GROUPS,
        should_train_symbol,
        mark_symbol_trained,
        ready_for_group_predict,
        mark_group_predicted,
        reset_group_order,
        CacheManager as DataCacheManager,
        compute_features_multi,
    )
except Exception:
    from utils import (
        get_kline_by_strategy,
        compute_features,
        SYMBOL_GROUPS,
        should_train_symbol,
        mark_symbol_trained,
        ready_for_group_predict,
        mark_group_predicted,
        reset_group_order,
        CacheManager as DataCacheManager,
        compute_features_multi,
    )

    # ì„ íƒì  ì‹ ê·œ API í´ë°±
    def compute_features_multi(symbol: str, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        out = {}
        for s in ("ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"):
            try:
                out[s] = compute_features(symbol, df, s)
            except Exception:
                out[s] = None
        return out

# ===== [ADD] ë³´ê°• ì„í¬íŠ¸: data.utils ì—ì„œ í˜„ì¬ ê·¸ë£¹ ì¡°íšŒ =====
try:
    from data.utils import get_current_group_index, get_current_group_symbols
except Exception:
    try:
        from utils import get_current_group_index, get_current_group_symbols
    except Exception:
        def get_current_group_index():
            return 0

        def get_current_group_symbols():
            return SYMBOL_GROUPS[0] if SYMBOL_GROUPS else []

# NOTE: ë¦¬í¬ êµ¬ì¡°ì— ë§ì¶° ê²½ë¡œ ì •ì • (robust dual import)
try:
    from model.base_model import get_model, freeze_backbone, unfreeze_last_k_layers
except Exception:
    from base_model import get_model

    def freeze_backbone(model):
        return None

    def unfreeze_last_k_layers(model, k: int = 1):
        return None

from feature_importance import compute_feature_importance, save_feature_importance
from failure_db import insert_failure_record, ensure_failure_db
import logger
from logger import log_prediction, ensure_prediction_log_exists  # <<< [ADD] ìš´ì˜ë¡œê·¸ ì°ê¸°ìš©

ensure_prediction_log_exists()  # <<< prediction_log.csv ì—†ìœ¼ë©´ ë§Œë“¤ì–´ ë‘”ë‹¤
from config import (
    get_NUM_CLASSES,
    get_FEATURE_INPUT_SIZE,
    get_class_groups,
    get_class_ranges,
    set_NUM_CLASSES,
    STRATEGY_CONFIG,
    get_QUALITY,
    get_LOSS,
    BOUNDARY_BAND,
    get_TRAIN_LOG_PATH,
)

# ===================== [ADD] í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • ìœ í‹¸ =====================
# YOPO ì² í•™: ë¼ë²¨ "ë³‘í•©/ì‚­ì œ" ì—†ìŒ. í¬ì†Œ í´ë˜ìŠ¤ë„ ê·¸ëŒ€ë¡œ ìœ ì§€.
try:
    from data_augmentation import balance_classes, compute_class_weights, make_weighted_sampler
except Exception:
    # ì•ˆì „ í´ë°±: ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•ŠìŒ
    def balance_classes(X, y, *a, **k): return (X, y)
    def compute_class_weights(y, *a, **k): return np.ones((int(np.max(y))+1 if len(y)>0 else 0,), dtype=np.float32)
    def make_weighted_sampler(*a, **k): return None
# ======================================================================

# ================================================
# âœ… ê²½ë¡œ í†µì¼(í•µì‹¬)
# - train_log.csvê°€ ìˆëŠ” "persistent"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ logs/models/run ëª¨ë‘ ê³ ì •
# - /tmp/persistent ê°™ì€ ë‹¤ë¥¸ ë£¨íŠ¸ë¡œ ê°ˆë¼ì§€ì§€ ì•Šê²Œ ë§‰ìŒ
# ================================================
TRAIN_LOG = get_TRAIN_LOG_PATH()  # /persistent/logs/train_log.csv ê°™ì€ "ì§„ì§œ" ê²½ë¡œ

# train_log.csv -> .../persistent/logs/train_log.csv
# BASEëŠ” .../persistent
_base_from_trainlog = os.path.dirname(os.path.dirname(TRAIN_LOG))

BASE_PERSIST_DIR = (
    os.getenv("PERSIST_DIR")
    or os.getenv("PERSISTENT_DIR")
    or _base_from_trainlog
)

try:
    os.makedirs(BASE_PERSIST_DIR, exist_ok=True)
except Exception:
    pass

# logsëŠ” train_log.csvê°€ ìˆëŠ” í´ë”ë¡œ ê³ ì •
LOG_DIR = os.path.dirname(TRAIN_LOG)
try:
    os.makedirs(LOG_DIR, exist_ok=True)
except Exception:
    pass

MODEL_DIR = os.getenv("MODEL_DIR", os.path.join(BASE_PERSIST_DIR, "models"))
RUN_DIR   = os.getenv("RUN_DIR",   os.path.join(BASE_PERSIST_DIR, "run"))

for _d in (MODEL_DIR, RUN_DIR):
    try:
        os.makedirs(_d, exist_ok=True)
    except Exception:
        pass

GROUP_ACTIVE_PATH = os.path.join(BASE_PERSIST_DIR, "GROUP_ACTIVE")
PERSIST_DIR = BASE_PERSIST_DIR
# ================================================


# ==== [ADD] í•™ìŠµ ë•Œ ìº”ë“¤ ìˆ˜ìµë¶„í¬ ìš´ì˜ë¡œê·¸ë¡œ ë‚¨ê¸°ëŠ” í•¨ìˆ˜ ====
def log_return_distribution_for_train(symbol: str, strategy: str, df: pd.DataFrame, max_rows: int = 1000):
    """
    í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ìº”ë“¤ì˜ ìˆ˜ìµë¥  ë¶„í¬ë¥¼ ìš´ì˜ë¡œê·¸ì™€ 'ë˜‘ê°™ì€ ë°©ì‹'ìœ¼ë¡œ ë‚¨ê¸´ë‹¤.
    ì¦‰, ê³µí†µ í•¨ìˆ˜ë§Œ ì“´ë‹¤.
    """
    try:
        if df is None or df.empty:
            return

        # 1) ê³µí†µí•¨ìˆ˜ë¡œ ìˆ˜ìµë¥  ì „ë¶€ ë½‘ê¸°
        rets = extract_candle_returns(df, max_rows=max_rows, strategy=strategy)

        if not rets:
            return

        # 2) ê³µí†µí•¨ìˆ˜ë¡œ íˆìŠ¤í† ê·¸ë¨ ë§Œë“¤ê¸°
        hist_info = make_return_histogram(rets, bins=20)

        # 3) ì½˜ì†”ì—ë„ ì°ì–´ì£¼ê³ 
        print(f"[ìˆ˜ìµë¶„í¬: {symbol}-{strategy}] sample={len(rets)}", flush=True)
        edges = hist_info["bin_edges"]
        counts = hist_info["bin_counts"]
        for i, cnt in enumerate(counts):
            if cnt == 0:
                continue
            lo = edges[i]
            hi = edges[i + 1]
            print(f"  {lo:.4f} ~ {hi:.4f} : {cnt}", flush=True)

        # 4) ìš´ì˜ë¡œê·¸(csv)ì—ë„ ë‚¨ê¸°ê¸°
        log_prediction(
            symbol=symbol,
            strategy=strategy,
            direction="í•™ìŠµìˆ˜ìµë¶„í¬",
            entry_price=0.0,
            target_price=0.0,
            model="trainer",
            model_name="trainer",
            predicted_class=-1,
            label=-1,
            note=json.dumps(
                {
                    "sample_size": len(rets),
                    "bin_edges": hist_info["bin_edges"],
                    "bin_counts": hist_info["bin_counts"],
                },
                ensure_ascii=False,
            ),
            top_k=[],
            success=True,
            reason="train_return_distribution",
            rate=0.0,
            expected_return=0.0,
            position="neutral",
            return_value=0.0,
            source="train",
        )
    except Exception as e:
        print(f"[train.return-dist warn] {e}", flush=True)

DEFAULT_TRAIN_HEADERS = [
    "timestamp",
    "symbol",
    "strategy",
    "model",
    "val_acc",
    "val_f1",
    "val_loss",
    "engine",
    "window",
    "recent_cap",
    "rows",
    "limit",
    "min",
    "augment_needed",
    "enough_for_training",
    "note",
    "source_exchange",
    "status",

    # âœ… train.py íŒ¨ì¹˜ê°€ ì‹¤ì œë¡œ ì“°ëŠ” í•„ë“œ (ëˆ„ë½ë˜ë©´ CSVì— ì•ˆ ì¨ì§)
    "group",
    "epoch",
    "train_loss",
    "notes",

    # ì¶”ê°€ ë©”íŠ¸ë¦­
    "accuracy",
    "f1",
    "loss",
    "y_true",
    "y_pred",
    "num_classes",

    # âœ… ë¼ë²¨/ë¶„í¬ ê´€ë ¨ (íŒ¨ì¹˜ê°€ ì±„ìš°ëŠ” í‚¤)
    "label_total",
    "label_masked",
    "label_masked_ratio",
    "near_zero",
    "boundary_band",
    "val_coverage",

    # === ì§„ë‹¨ 5ì¢… ===
    "NUM_CLASSES",
    "class_counts_label_freeze",
    "usable_samples",
    "class_counts_after_assemble",
    "batch_stratified_ok",

    # === ìˆ˜ìµë¥ /í´ë˜ìŠ¤ êµ¬ê°„ ìš”ì•½ ===
    "near_zero_band",
    "near_zero_count",
    "masked_count",
    "class_ranges",
    "bin_edges",
    "bin_counts",
    "bin_spans",

    # ğŸ”¥ í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ í•„ë“œ
    "class_edges",      # = bin_edges
    "class_counts",     # = bin_counts
    "bins",             # = len(bin_edges) - 1

    # ğŸ”¥ per-class ì„±ëŠ¥ ì €ì¥ìš©
    "per_class_f1",

    # ğŸ”¥ í•™ìŠµë¡œê·¸ ì¹´ë“œ UIìš© í•„ìˆ˜ 6ê°œ
    "ui_data_summary",
    "ui_dist_summary",
    "ui_class_range_summary",
    "ui_class_count_summary",
    "ui_usable_summary",
    "ui_performance_summary",

    # âœ… _log_training_result_patched()ê°€ ì‹¤ì œë¡œ ì±„ìš°ëŠ” "êµ¬í˜• UI ìš”ì•½"
    "ui_status",
    "ui_data_amount",
    "ui_return_summary",
    "ui_coverage_summary",
]

try:
    from logger import TRAIN_HEADERS

    # logger ìª½ ê¸°ë³¸ í—¤ë”ì™€ ìš°ë¦¬ê°€ ì¶”ê°€í•œ í—¤ë”ë¥¼ í•©ì¹¨(ì¤‘ë³µ ì œê±°)
    TRAIN_HEADERS = list(dict.fromkeys(list(TRAIN_HEADERS) + DEFAULT_TRAIN_HEADERS))
except Exception:
    TRAIN_HEADERS = DEFAULT_TRAIN_HEADERS

# âœ…âœ…âœ… í•µì‹¬ ìˆ˜ì •: train_log ê²½ë¡œëŠ” ì ˆëŒ€ ì¬ì‘ì„±í•˜ì§€ ë§ê³ , config.get_TRAIN_LOG_PATH() ê·¸ëŒ€ë¡œ ì“´ë‹¤.
TRAIN_LOG = get_TRAIN_LOG_PATH()
try:
    os.makedirs(os.path.dirname(TRAIN_LOG), exist_ok=True)
except Exception:
    TRAIN_LOG = get_TRAIN_LOG_PATH()


def _ensure_train_log():
    """
    âœ… train.pyì—ì„œëŠ” í—¤ë”ë¥¼ 'ì§ì ‘' ìƒˆë¡œ ë§Œë“¤ì§€ ì•ŠëŠ”ë‹¤.
    âœ… ì˜¤ì§ logger.pyì˜ ensure_train_log_exists()ë§Œ ì‚¬ìš©í•œë‹¤.
    """
    try:
        ensure_train_log_exists()
    except Exception as e:
        print(f"[FATAL] ensure_train_log_exists() failed: {e}", flush=True)
        raise


def _normalize_train_row(row: dict) -> dict:
    # ëª¨ë“  í—¤ë”ì— ëŒ€í•´ ê¸°ë³¸ê°’ ì±„ìš°ê¸°
    r = {k: row.get(k, None) for k in TRAIN_HEADERS}

    # ì˜›ë‚  í‚¤ ì´ë¦„ê³¼ í˜¸í™˜
    if r.get("val_acc") is None and row.get("accuracy") is not None:
        r["val_acc"] = row.get("accuracy")
    if r.get("val_f1") is None and row.get("f1") is not None:
        r["val_f1"] = row.get("f1")
    if r.get("val_loss") is None and row.get("loss") is not None:
        r["val_loss"] = row.get("loss")

    if r.get("engine") in (None, ""):
        r["engine"] = row.get("engine", "manual")
    if r.get("source_exchange") in (None, ""):
        r["source_exchange"] = row.get("source_exchange", "BYBIT")

    be = row.get("bin_edges") or r.get("bin_edges")
    bc = row.get("bin_counts") or r.get("bin_counts")

    if be is not None and r.get("class_edges") is None:
        r["class_edges"] = be
    if bc is not None and r.get("class_counts") is None:
        r["class_counts"] = bc
    if r.get("bins") is None:
        try:
            if isinstance(be, (list, tuple)) and len(be) >= 2:
                r["bins"] = len(be) - 1
        except Exception:
            pass

    return r


def _compute_bin_info_from_labels(
    labels: np.ndarray,
    class_ranges: list,
    to_local: dict,
    bin_edges: list | None = None,
):
    """
    - y(ì „ì²´ labels) ê¸°ì¤€ìœ¼ë¡œ í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚° âœ…
    """
    num_global = len(class_ranges)
    if num_global <= 0:
        return {"bin_edges": [], "bin_counts": [], "bin_spans": [], "bins": 0}

    counts_global = np.zeros(num_global, dtype=int)
    for g, l in to_local.items():
        if g < num_global:
            counts_global[g] = int((labels == g).sum())

    if not bin_edges:
        bin_edges = [float(lo) for (lo, _) in class_ranges]
        bin_edges.append(float(class_ranges[-1][1]))

    bin_spans = [float(bin_edges[i + 1] - bin_edges[i]) for i in range(len(bin_edges) - 1)]

    return {
        "bin_edges": list(bin_edges),
        "bin_counts": counts_global.tolist(),
        "bin_spans": bin_spans,
        "bins": len(bin_edges) - 1,
    }


def _append_train_log(row: dict):
    """
    âœ…âœ…âœ… í•µì‹¬: ë¬´ì¡°ê±´ get_TRAIN_LOG_PATH()ë¡œë§Œ ê¸°ë¡í•œë‹¤.
    """
    try:
        _ensure_train_log()

        _path = get_TRAIN_LOG_PATH()
        try:
            os.makedirs(os.path.dirname(_path), exist_ok=True)
        except Exception:
            pass

        with open(_path, "a", encoding="utf-8-sig", newline="") as f:
            w = csv.DictWriter(f, fieldnames=TRAIN_HEADERS, extrasaction="ignore")
            w.writerow(_normalize_train_row(row))

        try:
            print(f"[TRAIN_LOG APPEND OK] path={_path} symbol={row.get('symbol')} strategy={row.get('strategy')} status={row.get('status')}", flush=True)
        except Exception:
            pass

    except Exception as e:
        print(f"[FATAL] train_log append failed: {e}", flush=True)
        raise


# =========================================================
# âœ…âœ…âœ… [í•µì‹¬ ìˆ˜ì •] logger íŒ¨ì¹˜ëŠ” "ì „ì—­ì—ì„œ 1ë²ˆë§Œ" ì ìš©ë˜ì–´ì•¼ í•œë‹¤
# - ì›ë˜ í•¨ìˆ˜ ë°±ì—…(_orig_ltr) ì—†ìœ¼ë©´ íŒ¨ì¹˜ í•¨ìˆ˜ê°€ ê¹¨ì§
# - íŒ¨ì¹˜ ì ìš© ì½”ë“œê°€ í•¨ìˆ˜ ì•ˆ(ë“¤ì—¬ì“°ê¸°)ìœ¼ë¡œ ë“¤ì–´ê°€ë©´ ì ìš©ì´ ë¶ˆì•ˆì •í•¨
# =========================================================

# 1) ì›ë³¸ ë°±ì—… (ë”± 1ë²ˆ)
if not hasattr(logger, "_orig_log_training_result"):
    logger._orig_log_training_result = logger.log_training_result

_orig_ltr = logger._orig_log_training_result


def _log_training_result_patched(*args, **kw):
    """
    logger.log_training_result ë¥¼ ê°€ë¡œì±„ì„œ:
    1) ì›ë˜ logger ë¡œê¹… ì‹¤í–‰
    2) train_log.csv ì— ê¸°ë¡í•  row ìƒì„±(ìš´ì˜ë¡œê·¸ ìš”ì•½ + ë¶„í¬/êµ¬ê°„/ì„±ëŠ¥ í¬í•¨)
    """
    # 1) ì›ë˜ logger í˜¸ì¶œ
    if callable(_orig_ltr):
        try:
            _orig_ltr(*args, **kw)
        except Exception as e:
            print(f"[ê²½ê³ ] logger.log_training_result ì‹¤íŒ¨: {e}", flush=True)

    # 2) symbol/strategy/model ë³µêµ¬
    symbol = args[0] if len(args) > 0 else kw.get("symbol")
    strategy = args[1] if len(args) > 1 else kw.get("strategy")
    model = args[2] if len(args) > 2 else kw.get("model", "")

    result_dict = None
    if "result" in kw and isinstance(kw["result"], dict):
        result_dict = kw["result"]

    row: Dict[str, Any] = {
        "timestamp": datetime.now(pytz.timezone("Asia/Seoul")).isoformat(),
        "symbol": symbol,
        "strategy": strategy,
        "model": model,

        "note": kw.get("note", ""),
        "notes": kw.get("notes", ""),

        "status": kw.get("status", ""),
        "window": kw.get("window", ""),
        "group": kw.get("group", ""),
        "epoch": kw.get("epoch", ""),

        "val_acc": kw.get("val_acc", ""),
        "val_f1": kw.get("val_f1", ""),
        "val_loss": kw.get("val_loss", ""),
        "train_loss": kw.get("train_loss", ""),

        "engine": kw.get("engine", "manual"),
        "source_exchange": kw.get("source_exchange", "BYBIT"),
        "rows": kw.get("rows", ""),
        "limit": kw.get("limit", ""),
        "min": kw.get("min", ""),
        "augment_needed": kw.get("augment_needed", ""),
        "enough_for_training": kw.get("enough_for_training", ""),

        "label_total": kw.get("label_total", ""),
        "label_masked": kw.get("label_masked", ""),
        "label_masked_ratio": kw.get("label_masked_ratio", ""),
        "near_zero": kw.get("near_zero", ""),
        "near_zero_band": kw.get("near_zero_band", ""),
        "near_zero_count": kw.get("near_zero_count", ""),
        "boundary_band": kw.get("boundary_band", ""),
        "masked_count": kw.get("masked_count", ""),
        "bin_edges": kw.get("bin_edges", ""),
        "bin_counts": kw.get("bin_counts", ""),
        "bin_spans": kw.get("bin_spans", ""),
        "class_ranges": kw.get("class_ranges", ""),
        "class_edges": kw.get("class_edges", ""),
        "class_counts": kw.get("class_counts", ""),
        "bins": kw.get("bins", ""),
        "val_coverage": kw.get("val_coverage", ""),

        "num_classes": kw.get("num_classes", ""),
        "NUM_CLASSES": kw.get("NUM_CLASSES", ""),
        "usable_samples": kw.get("usable_samples", ""),
        "class_counts_label_freeze": kw.get("class_counts_label_freeze", ""),
        "class_counts_after_assemble": kw.get("class_counts_after_assemble", ""),
        "batch_stratified_ok": kw.get("batch_stratified_ok", ""),

        "per_class_f1": kw.get("per_class_f1", ""),

        "ui_status": kw.get("ui_status", ""),
        "ui_data_amount": kw.get("ui_data_amount", ""),
        "ui_return_summary": kw.get("ui_return_summary", ""),
        "ui_coverage_summary": kw.get("ui_coverage_summary", ""),

        "ui_data_summary": kw.get("ui_data_summary", ""),
        "ui_dist_summary": kw.get("ui_dist_summary", ""),
        "ui_class_range_summary": kw.get("ui_class_range_summary", ""),
        "ui_class_count_summary": kw.get("ui_class_count_summary", ""),
        "ui_usable_summary": kw.get("ui_usable_summary", ""),
        "ui_performance_summary": kw.get("ui_performance_summary", ""),
    }

    if isinstance(result_dict, dict):
        for k, v in result_dict.items():
            if k in row:
                row[k] = v

    for k, v in kw.items():
        if k in row and v not in (None, ""):
            row[k] = v

    def _jsonify_if_needed(v):
        if v is None:
            return None
        if isinstance(v, (dict, list, tuple)):
            try:
                return json.dumps(v, ensure_ascii=False)
            except Exception:
                return str(v)
        return v

    def _restore_json(val):
        if val in (None, "", "null"):
            return None
        if isinstance(val, (list, dict)):
            return val
        if isinstance(val, str):
            s = val.strip()
            if (s.startswith("[") and s.endswith("]")) or (s.startswith("{") and s.endswith("}")):
                try:
                    return json.loads(s)
                except Exception:
                    return val
        return val

    for k in ("bin_edges", "bin_counts", "bin_spans", "class_ranges", "class_edges", "class_counts", "val_coverage", "per_class_f1"):
        row[k] = _restore_json(row.get(k))

    try:
        be = row.get("bin_edges")
        bc = row.get("bin_counts")
        if row.get("bins") in (None, "", 0):
            if isinstance(be, list) and len(be) >= 2:
                row["bins"] = len(be) - 1
            elif isinstance(bc, list):
                row["bins"] = len(bc)
    except Exception:
        pass

    try:
        if row.get("num_classes") in (None, "", 0):
            cr = row.get("class_ranges")
            be2 = row.get("bin_edges")
            cc2 = row.get("class_counts")
            if isinstance(cr, list) and cr:
                row["num_classes"] = int(len(cr))
            elif isinstance(be2, list) and len(be2) >= 2:
                row["num_classes"] = int(len(be2) - 1)
            elif isinstance(cc2, list) and cc2:
                row["num_classes"] = int(len(cc2))
    except Exception:
        pass

    try:
        status = (row.get("status") or "").strip()
        if not row.get("ui_status"):
            if status in ("success", "best"):
                row["ui_status"] = f"âœ… ì •ìƒ í•™ìŠµ (status={status})"
            elif status in ("info",):
                row["ui_status"] = f"â„¹ï¸ ì°¸ê³ ìš© ë¡œê·¸ (status={status})"
            elif status in ("warn", "warning"):
                row["ui_status"] = f"ğŸŸ  ê²½ê³  (status={status})"
            elif status in ("error", "fail", "failed"):
                row["ui_status"] = f"ğŸ”´ ì‹¤íŒ¨/ì˜¤ë¥˜ (status={status})"
            else:
                row["ui_status"] = f"ìƒíƒœ: {status or 'ë¯¸ìƒ'}"

        need_new_ui = any(not row.get(k) for k in (
            "ui_data_summary", "ui_dist_summary", "ui_class_range_summary",
            "ui_class_count_summary", "ui_usable_summary", "ui_performance_summary"
        ))

        if need_new_ui:
            rows = int(row.get("rows") or 0)
            usable = int(row.get("usable_samples") or 0)
            acc = float(row.get("val_acc") or row.get("accuracy") or 0.0)
            f1v = float(row.get("val_f1") or row.get("f1") or 0.0)
            be = row.get("bin_edges") or []
            bc = row.get("bin_counts") or []
            cr = row.get("class_ranges") or []

            summary = make_training_summary_fields(
                rows=rows,
                bin_edges=be if isinstance(be, list) else [],
                bin_counts=bc if isinstance(bc, list) else [],
                class_ranges=cr if isinstance(cr, list) else [],
                usable_samples=usable,
                acc=acc,
                f1=f1v,
            )
            for k, v in summary.items():
                if not row.get(k):
                    row[k] = v

        if not row.get("ui_data_amount"):
            rows_v = row.get("rows")
            row["ui_data_amount"] = f"í•™ìŠµ ë°ì´í„°: {rows_v}ê°œ" if rows_v not in (None, "", 0) else "í•™ìŠµ ë°ì´í„° ì •ë³´ ì—†ìŒ"
        if not row.get("ui_return_summary"):
            be = row.get("bin_edges")
            if isinstance(be, list) and len(be) >= 2:
                row["ui_return_summary"] = f"ìˆ˜ìµë¥  ë²”ìœ„: {float(be[0]):.4f} ~ {float(be[-1]):.4f}"
            else:
                row["ui_return_summary"] = "ìˆ˜ìµë¥  ë¶„í¬ ì •ë³´ ì—†ìŒ"
        if not row.get("ui_coverage_summary"):
            cov = row.get("val_coverage")
            if isinstance(cov, dict) and cov.get("total"):
                covered = int(cov.get("covered", 0))
                total = int(cov.get("total", 0))
                row["ui_coverage_summary"] = f"ê²€ì¦ ì»¤ë²„ë¦¬ì§€: {covered}/{total}"
            else:
                row["ui_coverage_summary"] = "ê²€ì¦ ì»¤ë²„ë¦¬ì§€ ì •ë³´ ì—†ìŒ"
    except Exception as e:
        print(f"[train_log summary warn] {e}", flush=True)

    for k in list(row.keys()):
        row[k] = _jsonify_if_needed(row[k])

    _append_train_log(row)

    try:
        logger.update_train_dashboard(
            symbol=row.get("symbol"),
            strategy=row.get("strategy"),
            model=row.get("model", "")
        )
    except Exception as e:
        print(f"[ê²½ê³ ] train_dashboard ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}", flush=True)


# âœ… íŒ¨ì¹˜ ì ìš©ì€ ì—¬ê¸°ì„œ "ì „ì—­ 1ë²ˆ"ë§Œ
logger.log_training_result = _log_training_result_patched
logger._patched_train_log = True


# âœ… ì˜ˆì¸¡ ê²Œì´íŠ¸: ì•ˆì „ ì„í¬íŠ¸(ì—†ìœ¼ë©´ no-op)
try:
    from predict import close_predict_gate
except Exception:
    def close_predict_gate(*a, **k):
        return None

# âœ… í•™ìŠµ ì§í›„ ìë™ ì˜ˆì¸¡ íŠ¸ë¦¬ê±° (ì—†ìœ¼ë©´ no-op)
try:
    from predict_trigger import run_after_training
except Exception:
    def run_after_training(symbol, strategy, *a, **k):
        return False

# [ê°€ë“œ] focal_loss (ì—†ìœ¼ë©´ CE Loss ëŒ€ì²´)
try:
    from focal_loss import FocalLoss
except Exception:
    class FocalLoss(nn.Module):
        def __init__(self, gamma: float = 2.0, weight: Optional[torch.Tensor] = None):
            super().__init__()
            self.ce = nn.CrossEntropyLoss(weight=weight)

        def forward(self, logits, targets):
            return self.ce(logits, targets)

# [í’€ë°±] data.labels â†’ labels (Renderìš© ì—„ê²© fallback)
try:
    from data.labels import make_labels, make_all_horizon_labels
except ModuleNotFoundError:
    from labels import make_labels, make_all_horizon_labels

try:
    from window_optimizer import find_best_window, find_best_windows
except Exception:
    from window_optimizer import find_best_window

    def find_best_windows(symbol, strategy, window_list, top_k=3, group_id=None):
        return [find_best_window(symbol, strategy, window_list=window_list, group_id=group_id)]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ predict_lock: per-key ë½ ì‚¬ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from predict_lock import (
        clear_stale_predict_lock as pl_clear_stale,
        wait_until_free as pl_wait_free,
    )
except Exception:
    def pl_clear_stale(lock_key=None):
        return None

    def pl_wait_free(max_wait_sec: int, lock_key=None):
        return True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒì¸íŠœë‹ ë¡œë”(ì„ íƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from model_io import load_for_finetune as _load_for_finetune
except Exception:
    _load_for_finetune = None

# ---------- ì „ì—­ ìƒìˆ˜ ----------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_CLASSES = get_NUM_CLASSES()
FEATURE_INPUT_SIZE = get_FEATURE_INPUT_SIZE()

_MAX_ROWS_FOR_TRAIN = int(os.getenv("TRAIN_MAX_ROWS", "1200"))
_BATCH_SIZE = int(os.getenv("TRAIN_BATCH_SIZE", "128"))
_NUM_WORKERS = int(os.getenv("TRAIN_NUM_WORKERS", "0"))
_PIN_MEMORY = False
_PERSISTENT = False
SMART_TRAIN = os.getenv("SMART_TRAIN", "1") == "1"
LABEL_SMOOTH = float(os.getenv("LABEL_SMOOTH", "0.05"))
GRAD_CLIP = float(os.getenv("GRAD_CLIP_NORM", "1.0"))
FOCAL_GAMMA = float(os.getenv("FOCAL_GAMMA", "2.0"))
EARLY_STOP_PATIENCE = int(os.getenv("EARLY_STOP_PATIENCE", "4"))
EARLY_STOP_MIN_DELTA = float(os.getenv("EARLY_STOP_MIN_DELTA", "0.0001"))

USE_AMP = os.getenv("USE_AMP", "1") == "1"
TRAIN_CUDA_EMPTY_EVERY_EP = os.getenv("TRAIN_CUDA_EMPTY_EVERY_EP", "1") == "1"

# ===== [ADD] í´ë˜ìŠ¤ ë¶ˆê· í˜• ì œì–´ìš© ENV =====
BALANCE_CLASSES_FLAG = os.getenv("BALANCE_CLASSES", "1") == "1"
WEIGHTED_SAMPLER_FLAG = os.getenv("WEIGHTED_SAMPLER", "0") == "1"

MIN_CLASS_SAMPLES = int(os.getenv("MIN_CLASS_SAMPLES", "4"))
MAX_CLASS_UPSAMPLE = int(os.getenv("MAX_CLASS_UPSAMPLE", "64"))
# ===================================================================

def _as_bool_env(name: str, default: bool) -> bool:
    v = os.getenv(name)
    return default if v is None else v.strip().lower() in ("1", "true", "yes", "on")


COST_SENSITIVE_ARGMAX = _as_bool_env("COST_SENSITIVE_ARGMAX", True)
CS_ARG_BETA = float(os.getenv("CS_ARG_BETA", "1.0"))


def _epochs_for(strategy: str) -> int:
    if strategy == "ë‹¨ê¸°":
        return int(os.getenv("EPOCHS_SHORT", "36"))
    if strategy == "ì¤‘ê¸°":
        return int(os.getenv("EPOCHS_MID", "24"))
    if strategy == "ì¥ê¸°":
        return int(os.getenv("EPOCHS_LONG", "12"))
    return int(os.getenv("EPOCHS_DEFAULT", "24"))


EVAL_MIN_F1_SHORT = float(os.getenv("EVAL_MIN_F1_SHORT", "0.10"))
EVAL_MIN_F1_MID = float(os.getenv("EVAL_MIN_F1_MID", "0.50"))
EVAL_MIN_F1_LONG = float(os.getenv("EVAL_MIN_F1_LONG", "0.45"))
_SHORT_RETRY = int(os.getenv("SHORT_STRATEGY_RETRY", "3"))


def _min_f1_for(strategy: str) -> float:
    return (
        EVAL_MIN_F1_SHORT
        if strategy == "ë‹¨ê¸°"
        else (EVAL_MIN_F1_MID if strategy == "ì¤‘ê¸°" else EVAL_MIN_F1_LONG)
    )


now_kst = lambda: datetime.now(pytz.timezone("Asia/Seoul"))

PREDICT_OVERRIDE_ON_GROUP_END = _as_bool_env("PREDICT_OVERRIDE_ON_GROUP_END", True)

PREDICT_FORCE_AFTER_GROUP = _as_bool_env("PREDICT_FORCE_AFTER_GROUP", True)
PREDICT_TIMEOUT_SEC = float(os.getenv("PREDICT_TIMEOUT_SEC", "180"))

IMPORTANCE_ENABLE = os.getenv("IMPORTANCE_ENABLE", "1") == "1"

GROUP_TRAIN_LOCK = os.path.join(RUN_DIR, "group_training.lock")


def _set_group_active(active: bool, group_idx: int | None = None, symbols: list | None = None):
    try:
        if active:
            with open(GROUP_ACTIVE_PATH, "w", encoding="utf-8") as f:
                ts = datetime.utcnow().isoformat()
                syms = ",".join(symbols or [])
                f.write(f"ts={ts}\n")
                if group_idx is not None:
                    f.write(f"group={int(group_idx)}\n")
                f.write(f"symbols={syms}\n")
        else:
            if os.path.exists(GROUP_ACTIVE_PATH):
                os.remove(GROUP_ACTIVE_PATH)
    except Exception as e:
        try:
            print(f"[GROUP_ACTIVE warn] {e}", flush=True)
        except:
            pass


def _set_group_train_lock(active: bool, group_idx: int | None = None, symbols: list | None = None):
    try:
        if active:
            with open(GROUP_TRAIN_LOCK, "w", encoding="utf-8") as f:
                f.write(f"group={int(group_idx) if group_idx is not None else -1}\n")
                f.write(f"ts={datetime.utcnow().isoformat()}\n")
                f.write(f"symbols={','.join(symbols or [])}\n")
        else:
            if os.path.exists(GROUP_TRAIN_LOCK):
                os.remove(GROUP_TRAIN_LOCK)
    except Exception as e:
        try:
            print(f"[GROUP_LOCK warn] {e}", flush=True)
        except:
            pass


def _is_group_active_file() -> bool:
    try:
        return os.path.exists(GROUP_ACTIVE_PATH)
    except Exception:
        return False


def _is_group_lock_file() -> bool:
    try:
        return os.path.exists(GROUP_TRAIN_LOCK)
    except Exception:
        return False


def _maybe_insert_failure(payload: dict, feature_vector: Optional[List[Any]] = None):
    try:
        if not ready_for_group_predict():
            return
        insert_failure_record(payload, feature_vector=(feature_vector or []))
    except Exception as e:
        try:
            print(f"[FAILREC skip] {e}", flush=True)
        except:
            pass


def _safe_print(msg):
    try:
        print(msg, flush=True)
    except:
        pass


def _stem(p: str) -> str:
    return os.path.splitext(p)[0]


def _save_model_and_meta(model: nn.Module, path_pt: str, meta: dict):
    os.makedirs(os.path.dirname(path_pt), exist_ok=True)
    weight = _stem(path_pt) + ".ptz"
    save_model(weight, model.state_dict())
    meta_path = _stem(path_pt) + ".meta.json"
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=None, separators=(",", ":"))
    return weight, meta_path


def coverage_split_indices(
    y,
    val_frac=0.20,
    min_coverage=0.60,
    stride=50,
    max_windows=200,
    num_classes=None,
):
    y = np.asarray(y).astype(int)
    n = len(y)
    val_len = max(1, int(round(n * val_frac)))

    if num_classes is None:
        uniq = np.unique(y)
        num_classes = (
            max(len(uniq), int(uniq.max()) + 1)
            if (uniq.size and uniq.min() >= 0)
            else len(uniq)
        )

    tried = 0
    best = None
    end = n

    while end - val_len >= 0 and tried < max_windows:
        start = end - val_len
        yv = y[start:end]
        cnt = Counter(yv.tolist())
        covered = len([1 for v in cnt.values() if v > 0])
        coverage = covered / max(1, int(num_classes))

        if best is None or coverage > best[0]:
            best = (coverage, start, end, cnt, covered)

        if coverage >= float(min_coverage):
            break

        end -= int(max(1, stride))
        tried += 1

    if best is None:
        start, end = max(0, n - val_len), n
        cnt = Counter(y[start:end].tolist())
        covered = len(cnt)
        coverage = covered / max(1, int(num_classes))
    else:
        coverage, start, end, cnt, covered = best

    val_idx = np.arange(start, end)
    train_idx = np.concatenate([np.arange(0, start), np.arange(end, n)], axis=0)

    # âœ… ì›í•˜ëŠ” ì¶œë ¥ í¬ë§·ìœ¼ë¡œ ë³€ê²½
    _safe_print(
        f"[VAL][COVER] num_classes={int(num_classes)} covered={int(covered)} coverage={coverage*100:.1f}%"
    )

    return train_idx, val_idx



def _log_skip(symbol, strategy, reason):
    logger.log_training_result(
        symbol,
        strategy,
        model="all",
        accuracy=0.0,
        f1=0.0,
        loss=0.0,
        val_acc=0.0,
        val_f1=0.0,
        val_loss=0.0,
        engine="manual",
        window=None,
        recent_cap=None,
        rows=None,
        limit=None,
        min=None,
        augment_needed=None,
        enough_for_training=None,
        note=reason,
        source_exchange="BYBIT",
        status="skipped",
    )
    _maybe_insert_failure(
        {
            "symbol": symbol,
            "strategy": strategy,
            "model": "all",
            "predicted_class": -1,
            "success": False,
            "rate": 0.0,
            "reason": reason,
        },
        feature_vector=[],
    )


def _log_fail(symbol, strategy, reason):
    logger.log_training_result(
        symbol,
        strategy,
        model="all",
        accuracy=0.0,
        f1=0.0,
        loss=0.0,
        val_acc=0.0,
        val_f1=0.0,
        val_loss=0.0,
        engine="manual",
        window=None,
        recent_cap=None,
        rows=None,
        limit=None,
        min=None,
        augment_needed=None,
        enough_for_training=None,
        note=reason,
        source_exchange="BYBIT",
        status="failed",
    )
    _maybe_insert_failure(
        {
            "symbol": symbol,
            "strategy": strategy,
            "model": "all",
            "predicted_class": -1,
            "success": False,
            "rate": 0.0,
            "reason": reason,
        },
        feature_vector=[],
    )


def _has_any_model_for_symbol(symbol: str) -> bool:
    exts = (".ptz", ".safetensors", ".pt")
    try:
        if any(glob.glob(os.path.join(MODEL_DIR, f"{symbol}_*{e}")) for e in exts):
            return True
        d = os.path.join(MODEL_DIR, symbol)
        return (
            any(glob.glob(os.path.join(d, "*", "*" + e)) for e in exts)
            if os.path.isdir(d)
            else False
        )
    except:
        return False


def _has_model_for(symbol: str, strategy: str) -> bool:
    exts = (".ptz", ".safetensors", ".pt")
    try:
        if any(glob.glob(os.path.join(MODEL_DIR, f"{symbol}_{strategy}_*{e}")) for e in exts):
            return True
        d = os.path.join(MODEL_DIR, symbol, strategy)
        return (
            any(glob.glob(os.path.join(d, "*" + e)) for e in exts)
            if os.path.isdir(d)
            else False
        )
    except:
        return False


# ---------- ì „ëµ ê°„ í”¼ì²˜/ë¼ë²¨ íŒ¨ìŠ¤ë‹¤ìš´ (ìˆ˜ì •ë³¸) ----------
def _build_precomputed(symbol: str) -> tuple[Dict[str, Optional[pd.DataFrame]], Dict[str, Any], Dict[str, Any]]:
    """
    ì „ëµë§ˆë‹¤ ìê¸° dfë¥¼ ë¶ˆëŸ¬ì„œ â†’ ê·¸ê±¸ë¡œ í”¼ì²˜/ë¼ë²¨ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ ë‘”ë‹¤.
    ì´ë ‡ê²Œ í•´ì•¼ ë‹¨ê¸° df í•œ ì¥ìœ¼ë¡œ ì¤‘ê¸°/ì¥ê¸° ë¼ë²¨ì„ ì¬í™œìš©í•˜ëŠ” êµ¬ë©ì´ ì•ˆ ìƒê¹€.
    """
    dfs: Dict[str, Optional[pd.DataFrame]] = {}
    feats: Dict[str, Any] = {}
    pre_lbl: Dict[str, Any] = {}

    for strat in ("ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"):
        try:
            df_s = get_kline_by_strategy(symbol, strat)
        except Exception:
            df_s = None

        if df_s is None or df_s.empty:
            dfs[strat] = None
            feats[strat] = None
            pre_lbl[strat] = None
            continue

        dfs[strat] = df_s

        # í”¼ì²˜ë„ ì „ëµë³„ dfë¡œ ê³„ì‚°
        try:
            feats[strat] = compute_features(symbol, df_s, strat)
        except Exception:
            feats[strat] = None

        # ë¼ë²¨ë„ ì „ëµë³„ dfë¡œ ì§ì ‘ ê³„ì‚° (labels.pyê°€ ì „ëµâ†’ìº”ë“¤ìˆ˜ë¡œ ì´ë¯¸ ë‚˜ëˆ ë‘ )
        try:
            pre_lbl[strat] = make_labels(df=df_s, symbol=symbol, strategy=strat, group_id=None)
        except Exception:
            pre_lbl[strat] = None

    return dfs, feats, pre_lbl


def _find_prev_model_for(symbol: str, prev_strategy: str) -> Optional[str]:
    try:
        candidates = []
        for p in glob.glob(os.path.join(MODEL_DIR, f"{symbol}_{prev_strategy}_*.ptz")):
            candidates.append((os.path.getmtime(p), p))
        if not candidates:
            for p in glob.glob(os.path.join(MODEL_DIR, symbol, prev_strategy, "*.ptz")):
                candidates.append((os.path.getmtime(p), p))
        if not candidates:
            return None
        candidates.sort(reverse=True)
        return candidates[0][1]
    except Exception:
        return None


# === [ADD] ë¼ë²¨ ìœ íš¨ì„±/ì¬ì‹œë„ ìœ í‹¸ ===
def _uniq_nonneg(labels: np.ndarray) -> int:
    try:
        v = labels[np.asarray(labels) >= 0]
        return int(np.unique(v).size) if v.size else 0
    except Exception:
        return 0


def _rebuild_labels_once(df: pd.DataFrame, symbol: str, strategy: str):
    try:
        return make_labels(df=df, symbol=symbol, strategy=strategy, group_id=None)
    except Exception:
        return None


def _rebuild_samples_with_keepset(
    fv: np.ndarray,
    labels: np.ndarray,
    window: int,
    keep_set: set[int],
    to_local: Dict[int, int],
    min_samples: int = 1,  # â† í˜•ì‹ë§Œ ë‚¨ê¸°ê³ , "í´ë˜ìŠ¤ ì»·"ì—ëŠ” ì“°ì§€ ì•ŠëŠ”ë‹¤
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ê·¸ë£¹ì— ì†í•œ í´ë˜ìŠ¤ëŠ” 'ì „ë¶€' ê·¸ëŒ€ë¡œ ì“°ëŠ” ì‹œê³„ì—´ ìƒ˜í”Œ ì¬êµ¬ì„± í•¨ìˆ˜.

    - labels: ì „ì—­ í´ë˜ìŠ¤ ì¸ë±ìŠ¤(0~ì „ì²´-1)
    - keep_set: ì´ë²ˆ ê·¸ë£¹ì— í•´ë‹¹í•˜ëŠ” ì „ì—­ í´ë˜ìŠ¤ ì§‘í•©
    - to_local: (í•„ìš”ì‹œ) ì „ì—­â†’ë¡œì»¬ ë§¤í•‘(ì§€ê¸ˆì€ local_map ìƒˆë¡œ êµ¬ì„±í•˜ë¯€ë¡œ í•„ìˆ˜ëŠ” ì•„ë‹˜)

    â— ì¤‘ìš”í•œ ì :
    - 'ìƒ˜í”Œì´ ì ë‹¤'ëŠ” ì´ìœ ë¡œ íŠ¹ì • í´ë˜ìŠ¤ë¥¼ ì˜ë¼ë‚´ì§€ ì•ŠëŠ”ë‹¤.
    - ë‹¨ì§€ ì´ë²ˆ ê·¸ë£¹ì— ì†í•˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ë§Œ ì œì™¸.
    - ë‚˜ì¤‘ì— ì „ì²´ ìƒ˜í”Œì´ 0ê°œë©´ ê·¸ë•Œë§Œ í•™ìŠµ ìŠ¤í‚µ.
    """
    # ë°©ì–´ ì½”ë“œ: ì…ë ¥ì´ ë¹„ì—ˆìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ
    if fv is None or labels is None or len(fv) == 0 or len(labels) == 0:
        feat_dim = fv.shape[1] if fv is not None and fv.ndim == 2 else 0
        return (
            np.empty((0, window, feat_dim), dtype=np.float32),
            np.empty((0,), dtype=np.int64),
        )

    n = len(fv)

    # ì´ë²ˆ ê·¸ë£¹ì— ì‚¬ìš©í•  ì „ì—­ í´ë˜ìŠ¤ ì§‘í•©
    base_keep: set[int] = set(int(c) for c in (keep_set or set()))
    if not base_keep:
        # í˜¹ì‹œë¼ë„ ë¹„ì–´ ìˆìœ¼ë©´, ê´€ì¸¡ëœ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ í›„ë³´ë¡œ ì‚¬ìš©
        base_keep = set(int(l) for l in labels.tolist() if int(l) >= 0)

    if not base_keep:
        feat_dim = fv.shape[1] if fv is not None and fv.ndim == 2 else 0
        return (
            np.empty((0, window, feat_dim), dtype=np.float32),
            np.empty((0,), dtype=np.int64),
        )

    # ì „ì—­ â†’ ë¡œì»¬ ì¸ë±ìŠ¤ ë§¤í•‘ (ì •ë ¬ëœ ìˆœì„œë¡œ 0~k-1)
    sorted_keep = sorted(base_keep)
    local_map: Dict[int, int] = {g: i for i, g in enumerate(sorted_keep)}

    X_raw: list[np.ndarray] = []
    y_out: list[int] = []

    # ìœˆë„ìš° ë‹¨ìœ„ë¡œ ìƒ˜í”Œ ìƒì„±
    for i in range(n - window):
        yi = i + window - 1
        if yi < 0 or yi >= len(labels):
            continue

        lab_g = int(labels[yi])
        if lab_g < 0:
            continue
        if lab_g not in base_keep:
            # ì´ë²ˆ ê·¸ë£¹ì— ì†í•˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ëŠ” ì œì™¸
            continue

        lab_local = local_map.get(lab_g, None)
        if lab_local is None:
            continue

        X_raw.append(fv[i : i + window])
        y_out.append(lab_local)

    if not X_raw:
        feat_dim = fv.shape[1] if fv is not None and fv.ndim == 2 else 0
        return (
            np.empty((0, window, feat_dim), dtype=np.float32),
            np.empty((0,), dtype=np.int64),
        )

    X_arr = np.asarray(X_raw, dtype=np.float32)
    y_arr = np.asarray(y_out, dtype=np.int64)

    return X_arr, y_arr


def _synthesize_minority_if_needed(
    X_raw: np.ndarray,
    y: np.ndarray,
    num_classes: int
) -> Tuple[np.ndarray, np.ndarray, bool]:
    """
    í¬ì†Œ í´ë˜ìŠ¤ë¥¼ 'ì‚­ì œ'í•˜ì§€ ì•Šê³ , ë‹¨ìˆœ ë³µì‚¬(ì˜¤ë²„ìƒ˜í”Œ)ë¡œë§Œ ìµœì†Œ ê°œìˆ˜ê¹Œì§€ ë³´ê°•í•˜ëŠ” í•¨ìˆ˜.

    - ì–´ë–¤ í´ë˜ìŠ¤ê°€ 1~ëª‡ ê°œë°–ì— ì—†ì–´ë„ ì ˆëŒ€ ë²„ë¦¬ì§€ ì•ŠëŠ”ë‹¤.
    - ê° í´ë˜ìŠ¤ ìƒ˜í”Œ ìˆ˜ê°€ MIN_CLASS_SAMPLES ë³´ë‹¤ ì‘ìœ¼ë©´, ê·¸ í´ë˜ìŠ¤ ìƒ˜í”Œì„ ë³µì‚¬í•´ì„œ ëŠ˜ë¦°ë‹¤.
    - ì „ì²´ ìƒ˜í”Œ ìˆ˜ê°€ MAX_CLASS_UPSAMPLE ë¥¼ í¬ê²Œ ë„˜ì§€ ì•Šë„ë¡ ì•ˆì „ì¥ì¹˜ë„ ë‘”ë‹¤.
    """
    if X_raw is None or y is None or len(y) == 0 or num_classes <= 0:
        return X_raw, y, False

    y = np.asarray(y, dtype=np.int64)
    X_raw = np.asarray(X_raw, dtype=np.float32)

    # ì˜ëª»ëœ ë¼ë²¨ì€ ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤
    if y.min() < 0:
        return X_raw, y, False

    # í´ë˜ìŠ¤ë³„ ê°œìˆ˜
    counts = np.bincount(y, minlength=num_classes).astype(int)
    if counts.sum() <= 0:
        return X_raw, y, False

    # ì´ë¯¸ ì¶©ë¶„íˆ ë§ì€ ë°ì´í„°ë©´ êµ³ì´ ì•ˆ ê±´ë“œë¦¼
    if counts.min() >= MIN_CLASS_SAMPLES:
        return X_raw, y, False

    rng = np.random.default_rng(int(os.getenv("GLOBAL_SEED", "20240101")))
    X_list = [X_raw]
    y_list = [y]
    changed = False

    for cls_id, cnt in enumerate(counts):
        if cnt <= 0:
            # ì‹¤ì œë¡œ í•œ ë²ˆë„ ë“±ì¥í•˜ì§€ ì•Šì€ í´ë˜ìŠ¤ëŠ” ì–´ì©” ìˆ˜ ì—†ìŒ (ë°ì´í„°ê°€ 0ì´ë‹ˆê¹Œ ë³´ê°• ë¶ˆê°€)
            continue
        if cnt >= MIN_CLASS_SAMPLES:
            continue

        # ì´ í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤
        idx = np.where(y == cls_id)[0]
        if idx.size == 0:
            continue

        # ì–¼ë§ˆë‚˜ ë” ì±„ìš¸ì§€
        need = MIN_CLASS_SAMPLES - cnt
        # ì „ì²´ ê¸¸ì´ ìƒí•œ ë³´í˜¸
        max_extra = max(0, MAX_CLASS_UPSAMPLE - len(y))
        if max_extra <= 0:
            break
        need = min(need, max_extra)
        if need <= 0:
            continue

        # ë™ì¼ ìƒ˜í”Œì„ ëœë¤ ë³µì‚¬
        dup_idx = rng.choice(idx, size=need, replace=True)
        X_extra = X_raw[dup_idx]
        y_extra = y[dup_idx]

        X_list.append(X_extra)
        y_list.append(y_extra)
        changed = True

    if not changed:
        return X_raw, y, False

    X_new = np.concatenate(X_list, axis=0)
    y_new = np.concatenate(y_list, axis=0)

    # ì„ì–´ì„œ ìˆœì„œ í¸í–¥ ì œê±°
    perm = rng.permutation(len(y_new))
    X_new = X_new[perm]
    y_new = y_new[perm]

    return X_new.astype(np.float32), y_new.astype(np.int64), True


def _ensure_val_has_two_classes(train_idx, val_idx, y, min_classes=2):
    vy = y[val_idx]
    if len(np.unique(vy)) >= min_classes:
        return train_idx, val_idx, False
    ty = y[train_idx]
    classes = np.unique(y)
    if len(classes) < min_classes:
        return train_idx, val_idx, False
    want = [c for c in classes if c not in set(vy)]
    moved = False
    for c in want[:2]:
        cand = np.where(ty == c)[0]
        if len(cand) == 0:
            continue
        take = cand[0]
        g_take = train_idx[take]
        train_idx = np.delete(train_idx, take)
        val_idx = np.append(val_idx, g_take)
        moved = True
        vy = y[val_idx]
        if len(np.unique(vy)) >= min_classes:
            break
    return train_idx, val_idx, moved




_ENFORCE_FULL_STRATEGY = False
_STRICT_HALT_ON_INCOMPLETE = False
_REQUIRE_AT_LEAST_ONE_MODEL_PER_GROUP = False
_SYMBOL_RETRY_LIMIT = int(os.getenv("SYMBOL_RETRY_LIMIT", "1"))


def make_training_summary_fields(
    rows, 
    bin_edges, 
    bin_counts, 
    class_ranges, 
    usable_samples,
    acc, 
    f1
):
    """ì›¹ UIê°€ ì½ì„ ìˆ˜ ìˆëŠ” ë§¤ìš° ì‰¬ìš´ ìš”ì•½ë³¸ë¬¸ ìƒì„±"""

    # (1) ë°ì´í„° ê°œìˆ˜
    data_summary = f"í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°: ì´ {rows}ê°œ"

    # (2) ìˆ˜ìµë¥  êµ¬ê°„ ìš”ì•½
    try:
        if isinstance(bin_edges, list) and len(bin_edges) >= 2:
            lo = float(bin_edges[0])
            hi = float(bin_edges[-1])
            dist_summary = f"ìˆ˜ìµë¥  ë¶„í¬: ìµœì†Œ {lo:.4f} ~ ìµœëŒ€ {hi:.4f}"
        else:
            dist_summary = "ìˆ˜ìµë¥  ë¶„í¬ ì •ë³´ ì—†ìŒ"
    except:
        dist_summary = "ìˆ˜ìµë¥  ë¶„í¬ ì •ë³´ ì—†ìŒ"

    # (3) í´ë˜ìŠ¤ë³„ êµ¬ê°„ ìš”ì•½
    try:
        if isinstance(class_ranges, list) and class_ranges:
            cr = "; ".join([f"{float(lo):.4f}~{float(hi):.4f}" for lo, hi in class_ranges])
            class_range_summary = f"í´ë˜ìŠ¤ë³„ ìˆ˜ìµë¥  êµ¬ê°„: {cr}"
        else:
            class_range_summary = "í´ë˜ìŠ¤ë³„ êµ¬ê°„ ì •ë³´ ì—†ìŒ"
    except:
        class_range_summary = "í´ë˜ìŠ¤ë³„ êµ¬ê°„ ì •ë³´ ì—†ìŒ"

    # (4) í´ë˜ìŠ¤ ê°œìˆ˜
    num_classes = len(class_ranges) if class_ranges else 0
    class_count_summary = f"í´ë˜ìŠ¤ ê°œìˆ˜: {num_classes}ê°œ"

    # (5) ì‹¤ì œ usable ìƒ˜í”Œ ìˆ˜
    usable_summary = f"ì‹¤ì œ í•™ìŠµì— ì‚¬ìš©í•œ ìƒ˜í”Œ: {usable_samples}ê°œ"

    # (6) ëª¨ë¸ ì„±ëŠ¥
    perf_summary = f"ì •í™•ë„ {acc:.4f}, F1 {f1:.4f}"

    return {
        "ui_data_summary": data_summary,
        "ui_dist_summary": dist_summary,
        "ui_class_range_summary": class_range_summary,
        "ui_class_count_summary": class_count_summary,
        "ui_usable_summary": usable_summary,
        "ui_performance_summary": perf_summary,
    }


def _train_full_symbol(
    symbol: str, stop_event: Optional[threading.Event] = None
) -> Tuple[bool, Dict[str, Any]]:
    """
    âœ… ìˆ˜ì • í¬ì¸íŠ¸
    - train_one_model()ì´ 3ëª¨ë¸ì„ ëª¨ë‘ ì‹œë„í•˜ë¯€ë¡œ,
      ì—¬ê¸°ì„œëŠ” "ì €ì¥ëœ ëª¨ë¸ì´ 1ê°œë¼ë„ ìˆìœ¼ë©´ ok_once=True"ë¡œ ì²˜ë¦¬(ê¸°ì¡´ê³¼ ë™ì¼)
    - detailì— ëª¨ë¸ íƒ€ì…ë„ ê°™ì´ ë‚¨ê¹€(ì–´ë–¤ ëª¨ë¸ì´ ì‹¤ì œë¡œ ì €ì¥ëëŠ”ì§€ í™•ì¸ìš©)
    """
    strategies = ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]
    detail = {}
    any_saved = False

    pre_dfs, pre_feats, pre_lbls = _build_precomputed(symbol)

    for strategy in strategies:
        if stop_event is not None and stop_event.is_set():
            return any_saved, detail
        try:
            cr = get_class_ranges(symbol=symbol, strategy=strategy)
            num_classes = len(cr) if cr else 0
            groups = get_class_groups(num_classes=max(2, num_classes))
            max_gid = len(groups) - 1
            detail[strategy] = {}

            for gid in range(max_gid + 1):
                if stop_event is not None and stop_event.is_set():
                    return any_saved, detail

                attempts = _SHORT_RETRY if strategy == "ë‹¨ê¸°" else 1
                ok_once = False
                trained_types = []

                for _ in range(attempts):
                    pf = pre_feats.get(strategy) if isinstance(pre_feats, dict) else None
                    pl = pre_lbls.get(strategy) if isinstance(pre_lbls, dict) else None
                    df_hint = pre_dfs.get(strategy) if isinstance(pre_dfs, dict) else None

                    res = train_one_model(
                        symbol,
                        strategy,
                        group_id=gid,
                        max_epochs=_epochs_for(strategy),
                        stop_event=stop_event,
                        pre_feat=pf,
                        pre_lbl=pl,
                        df_hint=df_hint,
                    )

                    if bool(res and isinstance(res, dict) and res.get("models")):
                        ok_once = True
                        any_saved = True
                        trained_types = list(res.get("trained_model_types") or [])
                        break

                detail[strategy][gid] = {"ok": ok_once, "trained_model_types": trained_types}
                time.sleep(0.01)

        except Exception as e:
            logger.log_training_result(
                symbol,
                strategy,
                model="all",
                accuracy=0.0,
                f1=0.0,
                loss=0.0,
                val_acc=0.0,
                val_f1=0.0,
                val_loss=0.0,
                engine="manual",
                window=None,
                recent_cap=None,
                rows=None,
                limit=None,
                min=None,
                augment_needed=None,
                enough_for_training=None,
                note=f"ì „ëµ ì‹¤íŒ¨: {e}",
                status="failed",
                source_exchange="BYBIT",
            )
            detail[strategy] = {-1: {"ok": False, "trained_model_types": []}}

    return any_saved, detail


def train_models(
    symbol_list, stop_event: Optional[threading.Event] = None, ignore_should: bool = False
):
    completed_symbols = []
    partial_symbols = []
    env_force = os.getenv("TRAIN_FORCE_IGNORE_SHOULD", "0") == "1"
    for symbol in symbol_list:
        if stop_event is not None and stop_event.is_set():
            break
        symbol_has_model = _has_any_model_for_symbol(symbol)
        local_ignore = ignore_should or env_force or (not symbol_has_model)
        if not local_ignore:
            if not should_train_symbol(symbol):
                _safe_print(
                    f"[ORDER] skip {symbol} (should_train_symbol=False, models_exist={symbol_has_model})"
                )
                continue
        else:
            _safe_print(f"[order-override] {symbol}: force train")

        trained_complete = False
        for _ in range(max(1, _SYMBOL_RETRY_LIMIT)):
            if stop_event is not None and stop_event.is_set():
                break
            complete, detail = _train_full_symbol(symbol, stop_event=stop_event)
            _safe_print(f"[ORDER] {symbol} â†’ complete={complete} detail={detail}")
            if complete:
                trained_complete = True
                break

        if trained_complete:
            completed_symbols.append(symbol)
            try:
                mark_symbol_trained(symbol)
            except Exception as e:
                _safe_print(f"[mark_symbol_trained err] {e}")
        else:
            partial_symbols.append(symbol)
    return completed_symbols, partial_symbols


def _scan_symbols_from_model_dir() -> List[str]:
    syms = set()
    try:
        for p in glob.glob(os.path.join(MODEL_DIR, f"*_*_*.*")):
            b = os.path.basename(p)
            m = re.match(r"^([A-Z0-9]+)_[^_]+_", b)
            if m:
                syms.add(m.group(1))
        for d in glob.glob(os.path.join(MODEL_DIR, "*")):
            if os.path.isdir(d):
                syms.add(os.path.basename(d))
    except Exception:
        pass
    return sorted(syms)


def _pick_smoke_symbol(candidates: List[str]) -> Optional[str]:
    cand = [s for s in candidates if _has_any_model_for_symbol(s)]
    if cand:
        return sorted(cand)[0]
    pool = _scan_symbols_from_model_dir()
    pool = [s for s in pool if _has_any_model_for_symbol(s)]
    return pool[0] if pool else None


def _run_smoke_predict(predict_fn, symbol: str):
    ok_any = False
    for strat in ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]:
        if _has_model_for(symbol, strat):
            try:
                predict_fn(symbol, strat, source="ê·¸ë£¹ì§í›„(ìŠ¤ëª¨í¬)", model_type=None)
                ok_any = True
            except Exception as e:
                _safe_print(f"[SMOKE fail] {symbol}-{strat}: {e}")
    return ok_any


def _safe_predict_with_timeout(
    predict_fn,
    symbol: str,
    strategy: str,
    source: str = "group_end",
    model_type: str | None = None,
    timeout: float = PREDICT_TIMEOUT_SEC,
) -> bool:
    try:
        pl_clear_stale(lock_key=(symbol, strategy))
    except Exception:
        pass
    try:
        pl_wait_free(max_wait_sec=int(max(1, timeout / 2)), lock_key=(symbol, strategy))
    except Exception:
        pass

    ok = [False]
    err = [None]

    def _run():
        try:
            predict_fn(symbol, strategy, source=source, model_type=model_type)
            ok[0] = True
        except Exception as e:
            err[0] = e

    th = threading.Thread(target=_run, daemon=True, name=f"predict-{symbol}-{strategy}")
    th.start()
    th.join(timeout=float(timeout))
    if th.is_alive():
        return False
    if err[0] is not None:
        raise err[0]
    return ok[0]


def train_symbol_group_loop(
    sleep_sec: int = 0, stop_event: Optional[threading.Event] = None
):
    env_force_ignore = os.getenv("TRAIN_FORCE_IGNORE_SHOULD", "0") == "1"
    env_reset = os.getenv("RESET_GROUP_ORDER_ON_START", "0") == "1"
    force_full_pass = env_force_ignore
    if force_full_pass or env_reset:
        try:
            reset_group_order(0)
        except Exception as e:
            _safe_print(f"[group reset skip] {e}")

    while True:
        if stop_event is not None and stop_event.is_set():
            break
        try:
            from predict import predict

            if hasattr(logger, "ensure_train_log_exists"):
                logger.ensure_train_log_exists()
            if hasattr(logger, "ensure_prediction_log_exists"):
                logger.ensure_prediction_log_exists()

            groups = [list(g) for g in SYMBOL_GROUPS]
            if not groups:
                _safe_print("[group] SYMBOL_GROUPS ë¹„ì–´ ìˆìŒ â†’ ëŒ€ê¸°")
            else:
                # âœ… í˜„ì¬ ê·¸ë£¹ index ì— ë§ì¶° í•œ ê·¸ë£¹ë§Œ ì²˜ë¦¬
                try:
                    cur_idx = get_current_group_index()
                except Exception:
                    cur_idx = 0
                try:
                    cur_idx = int(cur_idx)
                except Exception:
                    cur_idx = 0
                if cur_idx < 0 or cur_idx >= len(groups):
                    cur_idx = 0

                idx = cur_idx
                group = groups[idx]

                if stop_event is not None and stop_event.is_set():
                    break
                _safe_print(f"ğŸš€ [group] {idx+1}/{len(groups)} â†’ {group}")

                try:
                    _set_group_active(True, group_idx=idx, symbols=group)
                    _set_group_train_lock(True, group_idx=idx, symbols=group)
                except Exception as e:
                    _safe_print(f"[GROUP mark warn] {e}")

                # 1) ì´ ê·¸ë£¹ í•™ìŠµë¶€í„°
                completed_syms, partial_syms = train_models(
                    group, stop_event=stop_event, ignore_should=force_full_pass
                )
                if stop_event is not None and stop_event.is_set():
                    break

                # 2) ì˜ˆì¸¡ ê²Œì´íŠ¸ í™•ì¸
                try:
                    gate_ok = ready_for_group_predict()
                except Exception as e:
                    _safe_print(f"[PREDICT-GATE warn] {e} -> ê²Œì´íŠ¸ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•˜ê³  ì˜ˆì¸¡ ìƒëµ")
                    gate_ok = False

                if not gate_ok:
                    _safe_print(
                        f"[PREDICT-SKIP] group{idx+1}: ready_for_group_predict()==False â†’ í•™ìŠµë§Œ í•˜ê³  ì˜ˆì¸¡ì€ ì•ˆ í•¨"
                    )
                else:
                    # 3) ê²Œì´íŠ¸ê°€ Trueì¸ ê²½ìš°ì—ë§Œ ì˜ˆì¸¡ ì‹¤í–‰
                    ran_any = False
                    for symbol in group:
                        for strategy in ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]:
                            if not _has_model_for(symbol, strategy):
                                _safe_print(f"[PREDICT-SKIP] {symbol}-{strategy}: ëª¨ë¸ ì—†ìŒ")
                                continue
                            try:
                                ok = _safe_predict_with_timeout(
                                    predict_fn=predict,
                                    symbol=symbol,
                                    strategy=strategy,
                                    source="ê·¸ë£¹ì§í›„",
                                    model_type=None,
                                    timeout=PREDICT_TIMEOUT_SEC,
                                )
                                if ok:
                                    ran_any = True
                                else:
                                    _safe_print(
                                        f"[PREDICT TIMEOUT] {symbol}-{strategy} (> {PREDICT_TIMEOUT_SEC}s)"
                                    )
                            except Exception as e:
                                _safe_print(f"[PREDICT FAIL] {symbol}-{strategy}: {e}")

                    # í•˜ë‚˜ë„ ì‹¤í–‰ì´ ì•ˆ ëì„ ë•Œë§Œ ìŠ¤ëª¨í¬
                    if not ran_any:
                        cand_symbol = _pick_smoke_symbol(group)
                        if cand_symbol:
                            _safe_print(f"[SMOKE] fallback predict for {cand_symbol}")
                            try:
                                _run_smoke_predict(predict, cand_symbol)
                            except Exception as e:
                                _safe_print(f"[SMOKE fail] {e}")

                    if ran_any:
                        try:
                            mark_group_predicted()
                        except Exception as e:
                            _safe_print(f"[mark_group_predicted err] {e}")

                # 4) ê·¸ë£¹ ë‹¨ìœ„ ì •ë¦¬
                try:
                    from logger import flush_gwanwoo_summary
                    flush_gwanwoo_summary()
                except Exception:
                    pass

                try:
                    # ì˜ˆì¸¡ì„ í–ˆë“  ì•ˆ í–ˆë“  ì—¬ê¸°ì„  ê²Œì´íŠ¸ë§Œ ë‹«ì•„ì¤€ë‹¤
                    close_predict_gate(note=f"train:group{idx+1}_end")
                except Exception as e:
                    _safe_print(f"[gate close warn] {e}")

                try:
                    _set_group_active(False)
                    _set_group_train_lock(False)
                except Exception as e:
                    _safe_print(f"[GROUP clear warn] {e}")

                # ê·¸ë£¹ ì‚¬ì´ íœ´ì‹
                if sleep_sec > 0:
                    for _ in range(sleep_sec):
                        if stop_event is not None and stop_event.is_set():
                            break
                        time.sleep(1)
                    if stop_event is not None and stop_event.is_set():
                        break

            _safe_print("âœ… group pass done")
            try:
                from logger import flush_gwanwoo_summary
                flush_gwanwoo_summary()
            except Exception:
                pass
            try:
                close_predict_gate(note="train:group_pass_done")
            except Exception as e:
                _safe_print(f"[gate close warn] {e}")

            if force_full_pass and not env_force_ignore:
                force_full_pass = False
        except Exception as e:
            _safe_print(f"[group loop err] {e}\n{traceback.format_exc()}")

        _safe_print("ğŸ’“ heartbeat")
        time.sleep(max(1, int(os.getenv("TRAIN_LOOP_IDLE_SEC", "3"))))


_TRAIN_LOOP_THREAD: Optional[threading.Thread] = None
_TRAIN_LOOP_STOP: Optional[threading.Event] = None
_TRAIN_LOOP_LOCK = threading.Lock()


def start_train_loop(force_restart: bool = False, sleep_sec: int = 0):
    global _TRAIN_LOOP_THREAD, _TRAIN_LOOP_STOP
    with _TRAIN_LOOP_LOCK:
        if _TRAIN_LOOP_THREAD is not None and _TRAIN_LOOP_THREAD.is_alive():
            if not force_restart:
                _safe_print("â„¹ï¸ already running")
                return False
            stop_train_loop(timeout=30)
        _TRAIN_LOOP_STOP = threading.Event()

        def _runner():
            try:
                train_symbol_group_loop(sleep_sec=sleep_sec, stop_event=_TRAIN_LOOP_STOP)
            finally:
                _safe_print("â„¹ï¸ train loop exit")

        _TRAIN_LOOP_THREAD = threading.Thread(
            target=_runner, daemon=True
        )
        _TRAIN_LOOP_THREAD.start()
        _safe_print("âœ… train loop started")
        return True


def stop_train_loop(timeout: int | float | None = 30):
    global _TRAIN_LOOP_THREAD, _TRAIN_LOOP_STOP
    with _TRAIN_LOOP_LOCK:
        # ì´ë¯¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì¢…ë£Œ
        if _TRAIN_LOOP_THREAD is None or not _TRAIN_LOOP_THREAD.is_alive():
            _safe_print("â„¹ï¸ no loop")
            return True
        if _TRAIN_LOOP_STOP is None:
            _safe_print("âš ï¸ no stop event")
            return False

        # ë©ˆì¶”ê¸° ìš”ì²­
        _TRAIN_LOOP_STOP.set()
        _TRAIN_LOOP_THREAD.join(timeout=timeout)

        # â›”ï¸ ìˆ˜ì • í•µì‹¬: ì•„ì§ ë©ˆì¶”ì§€ ì•Šì•˜ìœ¼ë©´ ì ˆëŒ€ QWIPE í•˜ì§€ ì•ŠëŠ”ë‹¤
        if _TRAIN_LOOP_THREAD.is_alive():
            _safe_print("âš ï¸ stop timeout â€” í•™ìŠµì´ ì™„ì „íˆ ë©ˆì¶”ì§€ ì•ŠìŒ â†’ ì´ˆê¸°í™”(QWIPE) ìƒëµ")
            return False

        # ì™„ì „íˆ ë©ˆì¶˜ ê²½ìš°ì—ë§Œ ì¢…ë£Œ ì²˜ë¦¬
        _TRAIN_LOOP_THREAD = None
        _TRAIN_LOOP_STOP = None
        _safe_print("âœ… loop stopped (safe state)")
        return True


def request_stop() -> bool:
    global _TRAIN_LOOP_STOP
    with _TRAIN_LOOP_LOCK:
        if _TRAIN_LOOP_STOP is None:
            return True
        _TRAIN_LOOP_STOP.set()
        return True


def is_loop_running() -> bool:
    with _TRAIN_LOOP_LOCK:
        return bool(
            _TRAIN_LOOP_THREAD is not None and _TRAIN_LOOP_THREAD.is_alive()
        )


def train_symbol(symbol: str, strategy: str, group_id: int | None = None) -> dict:
    res = train_one_model(symbol=symbol, strategy=strategy, group_id=group_id)
    try:
        if res.get("models"):
            # í•™ìŠµ ì™„ë£Œ í‘œì‹œëŠ” ê·¸ëŒ€ë¡œ
            mark_symbol_trained(symbol)

            # âœ… ë°”ë¡œ ì˜ˆì¸¡í•˜ì§€ ë§ê³ , ê²Œì´íŠ¸ê°€ ì—´ë ¤ ìˆì„ ë•Œë§Œ ì˜ˆì¸¡
            try:
                gate_ok = ready_for_group_predict()
            except Exception:
                gate_ok = False

            if gate_ok and not (_is_group_active_file() or _is_group_lock_file()):
                try:
                    from predict import predict
                    _safe_predict_with_timeout(
                        predict_fn=predict,
                        symbol=symbol,
                        strategy=strategy,
                        source="train_symbol",
                        model_type=None,
                        timeout=PREDICT_TIMEOUT_SEC,
                    )
                except Exception:
                    # ì˜ˆì¸¡ ì‹¤íŒ¨í•´ë„ í•™ìŠµì€ ì„±ê³µì´ë¯€ë¡œ ì¡°ìš©íˆ íŒ¨ìŠ¤
                    pass
            else:
                _safe_print(
                    f"[PREDICT-SKIP] {symbol}-{strategy}: ê²Œì´íŠ¸ ë‹«í˜ì´ê±°ë‚˜ ê·¸ë£¹ í•™ìŠµ ì¤‘ì´ë¼ ì˜ˆì¸¡ ìƒëµ"
                )
    except Exception:
        pass
    return res


def train_group(group_id: int | None = None) -> dict:
    idx = get_current_group_index() if group_id is None else int(group_id)
    symbols = (
        get_current_group_symbols()
        if group_id is None
        else (SYMBOL_GROUPS[idx] if 0 <= idx < len(SYMBOL_GROUPS) else [])
    )
    out = {"group_index": idx, "symbols": symbols, "results": {}}

    try:
        _set_group_active(True)
        _set_group_train_lock(True, group_idx=idx, symbols=symbols)
    except Exception as e:
        _safe_print(f"[GROUP mark warn] {e}")

    completed, partial = train_models(
        symbols, stop_event=None, ignore_should=False
    )
    out["completed"] = completed
    out["partial"] = partial

    # ğŸ”’ ì—¬ê¸°ì„œë„ ê²Œì´íŠ¸ê°€ Trueì¼ ë•Œë§Œ ì˜ˆì¸¡
    try:
        gate_ok = ready_for_group_predict()
    except Exception:
        gate_ok = False

    if gate_ok:
        try:
            from predict import predict

            ran_any = False
            for s in symbols:
                for strat in ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]:
                    if _has_model_for(s, strat):
                        try:
                            ok = _safe_predict_with_timeout(
                                predict_fn=predict,
                                symbol=s,
                                strategy=strat,
                                source="train_group",
                                model_type=None,
                                timeout=PREDICT_TIMEOUT_SEC,
                            )
                            ran_any = ran_any or ok
                        except Exception:
                            pass
            if ran_any:
                try:
                    mark_group_predicted()
                except Exception:
                    pass
        finally:
            try:
                from logger import flush_gwanwoo_summary
                flush_gwanwoo_summary()
            except Exception:
                pass
            try:
                close_predict_gate(note=f"train_group:idx{idx}_end")
            except Exception:
                pass
    else:
        _safe_print(f"[PREDICT-SKIP] train_group idx={idx}: ê²Œì´íŠ¸ê°€ Falseë¼ ì˜ˆì¸¡ì€ ìƒëµ")

    try:
        _set_group_active(False)
        _set_group_train_lock(False)
    except Exception as e:
        _safe_print(f"[GROUP clear warn] {e}")

    return out


def train_all() -> dict:
    summary = {"groups": []}
    for gid, group in enumerate(SYMBOL_GROUPS):
        res = train_group(group_id=gid)
        summary["groups"].append(res)
    return summary


def continue_from_failure(limit: int = 50) -> dict:
    tried = []
    ok = False
    err = None
    try:
        import failure_learn as FL

        tried.append("failure_learn.run")
        ok = bool(FL.run(limit=limit))
    except Exception as e1:
        err = str(e1)
        try:
            import failure_trainer as FT

            tried.append("failure_trainer.retrain_failures")
            ok = bool(FT.retrain_failures(limit=limit))
        except Exception as e2:
            err = f"{err} | {e2}"
    return {"ok": ok, "tried": tried, "error": err}


def _apply_real_balance(X_train: np.ndarray, y_train: np.ndarray,
                        min_count: int = 12) -> Tuple[np.ndarray, np.ndarray]:
    """
    (í˜„ì¬ ë¯¸ì‚¬ìš©) YOPO í•™ìŠµ ì•ˆì •í™”ë¥¼ ìœ„í•œ 'ì§„ì§œ' í´ë˜ìŠ¤ ê· í˜• í•¨ìˆ˜ í‹€.
    - ì—¬ê¸°ì„œëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠëŠ”ë‹¤. í¬ì†Œ í´ë˜ìŠ¤ ì‚­ì œ ë°©ì§€ë¥¼ ìœ„í•´ ë¹„í™œì„± ìœ ì§€.
    """
    if len(y_train) == 0:
        return X_train, y_train
    return X_train.astype(np.float32), y_train.astype(np.int64)



if __name__ == "__main__":
    try:
        start_train_loop(force_restart=True, sleep_sec=0)
    except Exception as e:
        _safe_print(f"[MAIN] err: {e}")
