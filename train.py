import os, json, torch, torch.nn as nn, numpy as np, datetime, pytz, sys, pandas as pd
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score
from data.utils import SYMBOLS, get_kline_by_strategy, compute_features, create_dataset
from model.base_model import get_model
from model_weight_loader import get_model_weight
from feature_importance import compute_feature_importance, save_feature_importance
from wrong_data_loader import load_training_prediction_data
from failure_db import load_existing_failure_hashes
from logger import log_training_result, strategy_stats, load_failure_count
from window_optimizer import find_best_window
import hashlib
from collections import Counter
import sqlite3
from config import NUM_CLASSES
import time

training_in_progress = {"ë‹¨ê¸°": False, "ì¤‘ê¸°": False, "ì¥ê¸°": False}


DEVICE = torch.device("cpu")
MODEL_DIR = "/persistent/models"
os.makedirs(MODEL_DIR, exist_ok=True)
now_kst = lambda: datetime.datetime.now(pytz.timezone("Asia/Seoul"))
STRATEGY_WRONG_REP = {"ë‹¨ê¸°": 4, "ì¤‘ê¸°": 6, "ì¥ê¸°": 8}

def get_feature_hash_from_tensor(x, use_full=False, precision=3):
    """
    âœ… [ì„¤ëª…]
    - ë§ˆì§€ë§‰ timestep ë˜ëŠ” ì „ì²´ featureë¥¼ ë°˜ì˜¬ë¦¼ í›„ sha1 í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜
    """
    import hashlib
    if x.ndim != 2 or x.shape[0] == 0:
        return "invalid"
    try:
        flat = x.flatten() if use_full else x[-1]
        rounded = [round(float(val), precision) for val in flat]
        return hashlib.sha1(",".join(map(str, rounded)).encode()).hexdigest()
    except Exception as e:
        # âš ï¸ ë¡œê·¸ ê°„ì†Œí™”: ì˜¤ë¥˜ì‹œ ê°„ë‹¨ ì¶œë ¥
        print(f"[get_feature_hash_from_tensor ì˜¤ë¥˜] {e}")
        return "invalid"

def get_frequent_failures(min_count=5):
    """
    âœ… [ì„¤ëª…] failure_patterns.dbì—ì„œ ë™ì¼ ì‹¤íŒ¨ê°€ min_count ì´ìƒì´ë©´ ë°˜í™˜
    """
    counter = Counter()
    try:
        with sqlite3.connect("/persistent/logs/failure_patterns.db") as conn:
            rows = conn.execute("SELECT hash FROM failure_patterns").fetchall()
            for row in rows:
                counter[row[0]] += 1
    except:
        pass
    return {h for h, cnt in counter.items() if cnt >= min_count}


def save_model_metadata(symbol, strategy, model_type, acc, f1, loss, input_size=None, class_counts=None, used_feature_columns=None):
    """
    âœ… [ì„¤ëª…] ëª¨ë¸ ë©”íƒ€ì •ë³´ë¥¼ jsonìœ¼ë¡œ ì €ì¥ (used_feature_columns í¬í•¨)
    """
    meta = {
        "symbol": symbol, "strategy": strategy, "model": model_type or "unknown",
        "input_size": int(input_size) if input_size else 11,
        "accuracy": float(round(acc, 4)), "f1_score": float(round(f1, 4)),
        "loss": float(round(loss, 6)),
        "timestamp": now_kst().strftime("%Y-%m-%d %H:%M:%S")
    }
    if class_counts:
        meta["class_counts"] = {str(k): int(v) for k, v in class_counts.items()}

    # âœ… ì‚¬ìš©ëœ feature ì»¬ëŸ¼ ì €ì¥
    if used_feature_columns:
        meta["used_feature_columns"] = used_feature_columns

    path = f"{MODEL_DIR}/{symbol}_{strategy}_{model_type}.meta.json"
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)
        print(f"[ë©”íƒ€ì €ì¥] {model_type} ({symbol}-{strategy}) acc={acc:.4f}")
    except Exception as e:
        print(f"[ERROR] meta ì €ì¥ ì‹¤íŒ¨: {e}")

def train_one_model(symbol, strategy, max_epochs=20):
    import os, gc
    from focal_loss import FocalLoss
    from ssl_pretrain import masked_reconstruction
    print(f"â–¶ í•™ìŠµ ì‹œì‘: {symbol}-{strategy}")

    try:
        # âœ… SSL pretraining ì‹¤í–‰ (input_size=14 ê³ ì •)
        masked_reconstruction(symbol, strategy, input_size=14, mask_ratio=0.2, epochs=5)

        df = get_kline_by_strategy(symbol, strategy)
        if df is None or df.empty:
            print("â›” ì¤‘ë‹¨: ì‹œì„¸ ë°ì´í„° ì—†ìŒ")
            return

        df_feat = compute_features(symbol, df, strategy)
        if df_feat is None or df_feat.empty or df_feat.isnull().any().any():
            print("â›” ì¤‘ë‹¨: í”¼ì²˜ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” NaN")
            return

        window = find_best_window(symbol, strategy)
        if not isinstance(window, int) or window <= 0:
            print(f"â›” ì¤‘ë‹¨: find_best_window ì‹¤íŒ¨")
            return

        # âœ… input_size=14 ê°•ì œ ì ìš©
        X_raw, y_raw = create_dataset(df_feat.to_dict(orient="records"), window=window, strategy=strategy, input_size=14)
        if X_raw is None or y_raw is None or len(X_raw) < 5:
            print("â›” ì¤‘ë‹¨: í•™ìŠµ ë°ì´í„° ë¶€ì¡±")
            return

        print("[INFO] balance_classes(min_count=30) í˜¸ì¶œ")
        X_raw, y_raw = balance_classes(X_raw, y_raw, min_count=30)

        y_raw, X_raw = np.array(y_raw), np.array(X_raw, dtype=np.float32)
        mask = (y_raw >= 0) & (y_raw < NUM_CLASSES)
        X_raw, y_raw = X_raw[mask], y_raw[mask]
        if len(X_raw) < 5:
            print("â›” ì¤‘ë‹¨: ìœ íš¨ ìƒ˜í”Œ ë¶€ì¡±")
            return

        input_size = 14  # âœ… input_size ê³ ì •
        val_len = max(5, int(len(X_raw) * 0.2))

        # âœ… Curriculum Learning
        sorted_idx = np.argsort(y_raw)
        X_raw, y_raw = X_raw[sorted_idx], y_raw[sorted_idx]

        X_train, y_train, X_val, y_val = X_raw[:-val_len], y_raw[:-val_len], X_raw[-val_len:], y_raw[-val_len:]

        wrong_data = load_training_prediction_data(symbol, strategy, input_size, window)
        wrong_ds = TensorDataset(torch.tensor([x for x, _ in wrong_data], dtype=torch.float32),
                                 torch.tensor([y for _, y in wrong_data], dtype=torch.long)) if wrong_data else None

        from collections import Counter
        counts = Counter(y_train)
        total = sum(counts.values())
        class_weight = [total / counts.get(i, 1) for i in range(NUM_CLASSES)]
        class_weight_tensor = torch.tensor(class_weight, dtype=torch.float32).to(DEVICE)

        for model_type in ["lstm", "cnn_lstm", "transformer"]:
            model = get_model(model_type, input_size=input_size, output_size=NUM_CLASSES).to(DEVICE).train()
            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
            lossfn = nn.CrossEntropyLoss(weight=class_weight_tensor)

            train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                                     torch.tensor(y_train, dtype=torch.long))
            train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)

            # âœ… Active Sampling
            for epoch in range(max_epochs):
                indices = np.random.choice(len(X_train), int(len(X_train)*0.8), replace=False)
                sampled_X = X_train[indices]
                sampled_y = y_train[indices]

                sampled_ds = TensorDataset(torch.tensor(sampled_X, dtype=torch.float32),
                                           torch.tensor(sampled_y, dtype=torch.long))
                sampled_loader = DataLoader(sampled_ds, batch_size=32, shuffle=True, num_workers=2)

                for xb, yb in sampled_loader:
                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                    logits = model(xb)
                    loss = lossfn(logits, yb)
                    if torch.isfinite(loss):
                        optimizer.zero_grad(); loss.backward(); optimizer.step()

            # âœ… ì‹¤íŒ¨ ì§‘ì¤‘ í•™ìŠµ
            if wrong_ds:
                wrong_loader = DataLoader(wrong_ds, batch_size=16, shuffle=True, num_workers=2)
                for _ in range(3):
                    for xb, yb in wrong_loader:
                        xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                        logits = model(xb)
                        loss = lossfn(logits, yb)
                        if torch.isfinite(loss):
                            optimizer.zero_grad(); loss.backward(); optimizer.step()
                del wrong_loader, wrong_ds
                torch.cuda.empty_cache()
                gc.collect()

            # âœ… ê²€ì¦
            model.eval()
            with torch.no_grad():
                xb = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)
                yb = torch.tensor(y_val, dtype=torch.long).to(DEVICE)
                logits = model(xb)
                preds = torch.argmax(logits, dim=1).cpu().numpy()
                acc = accuracy_score(y_val, preds)
                f1 = f1_score(y_val, preds, average="macro")
                val_loss = lossfn(logits, yb).item()
                print(f"[ê²€ì¦] {model_type} acc={acc:.4f}, f1={f1:.4f}")

            save_model_metadata(
                symbol, strategy, model_type, acc, f1, val_loss,
                input_size=input_size,
                class_counts=Counter(y_train),
                used_feature_columns=list(df_feat.drop(columns=["timestamp"]).columns)
            )

            log_training_result(symbol, strategy, model_type, acc, f1, val_loss)
            torch.save(model.state_dict(), f"{MODEL_DIR}/{symbol}_{strategy}_{model_type}.pt")

            try:
                imps = compute_feature_importance(model, xb, yb, list(df_feat.drop(columns=["timestamp"]).columns))
                save_feature_importance(imps, symbol, strategy, model_type)
            except:
                pass

            del model, xb, yb, logits
            torch.cuda.empty_cache()
            gc.collect()

    except Exception as e:
        print(f"[ERROR] {symbol}-{strategy}: {e}")
        log_training_result(symbol, strategy, f"ì‹¤íŒ¨({str(e)})", 0.0, 0.0, 0.0)

def balance_classes(X, y, min_count=20):
    import numpy as np
    from collections import Counter
    from imblearn.over_sampling import SMOTE
    from logger import log_prediction  # âœ… logger import ì¶”ê°€

    if X is None or y is None or len(X) == 0 or len(y) == 0:
        print("[âŒ balance_classes ì‹¤íŒ¨] X ë˜ëŠ” y ë¹„ì–´ìˆìŒ")
        return X, y

    y = y.astype(np.int64)
    mask = (y != -1) & np.isfinite(y)
    X, y = X[mask], y[mask]

    if len(y) == 0:
        raise Exception("[âŒ balance_classes ì‹¤íŒ¨] ë¼ë²¨ ì œê±° í›„ ìƒ˜í”Œ ì—†ìŒ")

    class_counts = Counter(y)
    print(f"[ğŸ”¢ ê¸°ì¡´ í´ë˜ìŠ¤ ë¶„í¬] {dict(class_counts)}")

    nsamples, nx, ny = X.shape
    X_balanced, y_balanced = list(X), list(y)

    max_count = max(class_counts.values()) if class_counts else min_count
    target_count = max(min_count, int(max_count * 0.8))

    for cls in range(21):  # NUM_CLASSES = 21
        indices = [i for i, label in enumerate(y) if label == cls]
        count = len(indices)
        needed = max(0, target_count - count)

        if needed > 0:
            if count >= 2:
                try:
                    X_cls = X[indices].reshape((count, nx * ny))
                    k_neighbors = min(count - 1, 5)
                    smote = SMOTE(random_state=42, sampling_strategy={cls: count + needed}, k_neighbors=k_neighbors)
                    X_res, y_res = smote.fit_resample(X_cls, np.array([cls]*count))
                    X_new = X_res[count:].reshape((-1, nx, ny))
                    if len(X_new) > needed:
                        X_new = X_new[:needed]
                    X_balanced.extend(X_new)
                    y_balanced.extend([cls]*len(X_new))
                    print(f"[âœ… SMOTE ì„±ê³µ] í´ë˜ìŠ¤ {cls} â†’ {len(X_new)}ê°œ ì¶”ê°€")

                    log_prediction(
                        symbol="augmentation", strategy="augmentation",
                        direction=f"SMOTE-{cls}", entry_price=0, target_price=0,
                        model="augmentation", success=True,
                        reason=f"SMOTE {cls} {len(X_new)}ê°œ ì¶”ê°€",
                        rate=0.0, timestamp=None, return_value=0.0,
                        volatility=False, source="augmentation",
                        predicted_class=cls, label=cls, augmentation="smote"
                    )

                except Exception as e:
                    print(f"[âš ï¸ SMOTE ì‹¤íŒ¨] í´ë˜ìŠ¤ {cls} â†’ fallback: {e}")
                    reps = np.random.choice(indices, needed, replace=True)
                    noisy_samples = X[reps] + np.random.normal(0, 0.05, X[reps].shape).astype(np.float32)
                    
                    # âœ… ì¶”ê°€: Noise + Mixup + Time Masking
                    mixup_samples = noisy_samples.copy()
                    for i in range(len(mixup_samples)):
                        j = np.random.randint(len(X))
                        lam = np.random.beta(0.2, 0.2)
                        mixup_samples[i] = lam * mixup_samples[i] + (1 - lam) * X[j]

                        # Time Masking
                        t = np.random.randint(0, nx)
                        mixup_samples[i][t] = 0.0

                    X_balanced.extend(mixup_samples)
                    y_balanced.extend([cls]*needed)
                    print(f"[ë³µì œ+Noise+Mixup+Masking] í´ë˜ìŠ¤ {cls} â†’ {needed}ê°œ ì¶”ê°€")
            elif count == 1:
                reps = np.repeat(indices[0], needed)
                noisy_samples = X[reps] + np.random.normal(0, 0.05, X[reps].shape).astype(np.float32)
                X_balanced.extend(noisy_samples)
                y_balanced.extend([cls]*needed)
                print(f"[ë³µì œ+Noise] í´ë˜ìŠ¤ {cls} â†’ {needed}ê°œ ì¶”ê°€ (1ê°œ ë³µì œ)")
            else:
                print(f"[ìŠ¤í‚µ] í´ë˜ìŠ¤ {cls} â†’ ìƒ˜í”Œ ì—†ìŒ, noise sample ìƒì„± ìƒëµ")

    combined = list(zip(X_balanced, y_balanced))
    np.random.shuffle(combined)
    X_shuffled, y_shuffled = zip(*combined)

    final_counts = Counter(y_shuffled)
    print(f"[ğŸ“Š ìµœì¢… í´ë˜ìŠ¤ ë¶„í¬] {dict(final_counts)}")
    print(f"[âœ… balance_classes ì™„ë£Œ] ìµœì¢… ìƒ˜í”Œìˆ˜: {len(y_shuffled)}")

    return np.array(X_shuffled), np.array(y_shuffled, dtype=np.int64)

def train_all_models():
    """
    âœ… [ì„¤ëª…] SYMBOLS ì „ì²´ì— ëŒ€í•´ ë‹¨ê¸°, ì¤‘ê¸°, ì¥ê¸° í•™ìŠµ ìˆ˜í–‰
    """
    global training_in_progress
    from telegram_bot import send_message
    strategies = ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]

    for strategy in strategies:
        if training_in_progress.get(strategy, False):
            print(f"âš ï¸ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: {strategy}")
            continue

        print(f"ğŸš€ {strategy} í•™ìŠµ ì‹œì‘")
        training_in_progress[strategy] = True

        try:
            for symbol in SYMBOLS:
                try:
                    train_one_model(symbol, strategy)
                except Exception as e:
                    print(f"[ì˜¤ë¥˜] {symbol}-{strategy} í•™ìŠµ ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"[ì¹˜ëª… ì˜¤ë¥˜] {strategy} í•™ìŠµ ì¤‘ë‹¨: {e}")
        finally:
            training_in_progress[strategy] = False
            print(f"âœ… {strategy} í•™ìŠµ ì™„ë£Œ")

        time.sleep(5)

    send_message("âœ… ì „ì²´ í•™ìŠµ ì™„ë£Œ. ì˜ˆì¸¡ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")


def train_models(symbol_list):
    """
    âœ… [ì„¤ëª…] íŠ¹ì • symbol_listì— ëŒ€í•´ ë‹¨ê¸°, ì¤‘ê¸°, ì¥ê¸° í•™ìŠµ ìˆ˜í–‰
    - meta ë³´ì • í›„ ì˜ˆì¸¡ê¹Œì§€ ìë™ ì‹¤í–‰
    """
    global training_in_progress
    from telegram_bot import send_message
    from predict_test import main as run_prediction
    import maintenance_fix_meta

    strategies = ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]

    for strategy in strategies:
        if training_in_progress.get(strategy, False):
            print(f"âš ï¸ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: {strategy}")
            continue

        print(f"ğŸš€ {strategy} í•™ìŠµ ì‹œì‘")
        training_in_progress[strategy] = True

        try:
            for symbol in symbol_list:
                try:
                    train_one_model(symbol, strategy)
                except Exception as e:
                    print(f"[ì˜¤ë¥˜] {symbol}-{strategy} í•™ìŠµ ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"[ì¹˜ëª… ì˜¤ë¥˜] {strategy} ì „ì²´ í•™ìŠµ ì¤‘ë‹¨: {e}")
        finally:
            training_in_progress[strategy] = False
            print(f"âœ… {strategy} í•™ìŠµ ì™„ë£Œ")

        time.sleep(5)

        try:
            maintenance_fix_meta.fix_all_meta_json()
            print(f"âœ… meta ë³´ì • ì™„ë£Œ: {strategy}")
        except Exception as e:
            print(f"[âš ï¸ meta ë³´ì • ì‹¤íŒ¨] {e}")

        try:
            run_prediction(strategy, symbols=symbol_list)
            print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {strategy}")
        except Exception as e:
            print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {strategy} â†’ {e}")

    send_message("âœ… í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ")

def train_model_loop(strategy):
    """
    âœ… [ì„¤ëª…] íŠ¹ì • strategy í•™ìŠµì„ ë¬´í•œ ë£¨í”„ë¡œ ì‹¤í–‰
    - training_in_progress ìƒíƒœ ê´€ë¦¬ í¬í•¨
    """
    global training_in_progress
    if training_in_progress.get(strategy, False):
        print(f"âš ï¸ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: {strategy}")
        return

    training_in_progress[strategy] = True
    print(f"ğŸš€ {strategy} ë¬´í•œ í•™ìŠµ ë£¨í”„ ì‹œì‘")

    try:
        for symbol in SYMBOLS:
            try:
                train_one_model(symbol, strategy)
            except Exception as e:
                print(f"[ì˜¤ë¥˜] {symbol}-{strategy} í•™ìŠµ ì‹¤íŒ¨: {e}")
    finally:
        training_in_progress[strategy] = False
        print(f"âœ… {strategy} ë£¨í”„ ì¢…ë£Œ")


def train_symbol_group_loop(delay_minutes=5):
    """
    âœ… [ì„¤ëª…] SYMBOL_GROUPS ë‹¨ìœ„ë¡œ ì „ì²´ ê·¸ë£¹ í•™ìŠµ ë£¨í”„ ì‹¤í–‰
    - ê° ê·¸ë£¹ í•™ìŠµ ì „ cache clear
    - ê° ê·¸ë£¹ í•™ìŠµ í›„ meta ë³´ì •, ì˜ˆì¸¡ ì‹¤í–‰ í¬í•¨
    """
    import time
    import maintenance_fix_meta
    from data.utils import SYMBOL_GROUPS, _kline_cache, _feature_cache

    group_count = len(SYMBOL_GROUPS)
    print(f"ğŸš€ ì „ì²´ {group_count}ê°œ ê·¸ë£¹ í•™ìŠµ ë£¨í”„ ì‹œì‘")

    while True:
        for idx, group in enumerate(SYMBOL_GROUPS):
            print(f"\nğŸš€ [ê·¸ë£¹ {idx}] í•™ìŠµ ì‹œì‘ â†’ {group}")

            # âœ… ìºì‹œ clear ì¶”ê°€
            _kline_cache.clear()
            _feature_cache.clear()
            print("[âœ… cache cleared] _kline_cache, _feature_cache")

            try:
                train_models(group)

                maintenance_fix_meta.fix_all_meta_json()
                print(f"âœ… meta ë³´ì • ì™„ë£Œ: ê·¸ë£¹ {idx}")

                # âœ… ì˜ˆì¸¡ ì‹¤í–‰
                for symbol in group:
                    for strategy in ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]:
                        try:
                            from recommend import main
                            main(symbol=symbol, strategy=strategy, force=True)
                            print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {symbol}-{strategy}")
                        except Exception as e:
                            print(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {symbol}-{strategy} â†’ {e}")

                print(f"ğŸ•’ ê·¸ë£¹ {idx} ì™„ë£Œ â†’ {delay_minutes}ë¶„ ëŒ€ê¸°")
                time.sleep(delay_minutes * 60)

            except Exception as e:
                print(f"âŒ ê·¸ë£¹ {idx} ë£¨í”„ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
def pretrain_ssl_features(symbol, strategy, pretrain_epochs=5):
    """
    âœ… [ì„¤ëª…] Self-Supervised Learning pretraining
    - feature reconstruction ê¸°ë°˜ ì‚¬ì „í•™ìŠµ
    """
    from model.base_model import get_model

    print(f"â–¶ SSL Pretraining ì‹œì‘: {symbol}-{strategy}")

    df = get_kline_by_strategy(symbol, strategy)
    if df is None or df.empty:
        print("â›” ì¤‘ë‹¨: ì‹œì„¸ ë°ì´í„° ì—†ìŒ")
        return

    df_feat = compute_features(symbol, df, strategy)
    if df_feat is None or df_feat.empty or df_feat.isnull().any().any():
        print("â›” ì¤‘ë‹¨: í”¼ì²˜ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” NaN")
        return

    features_only = df_feat.drop(columns=["timestamp"], errors="ignore")
    feat_scaled = MinMaxScaler().fit_transform(features_only)
    X = np.expand_dims(feat_scaled, axis=1)  # (samples, 1, features)

    input_size = X.shape[2]
    model = get_model("autoencoder", input_size=input_size, output_size=input_size).to(DEVICE).train()

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    lossfn = nn.MSELoss()

    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(X, dtype=torch.float32))
    loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)

    for epoch in range(pretrain_epochs):
        total_loss = 0.0
        for xb, yb in loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            out = model(xb)
            loss = lossfn(out, yb)
            optimizer.zero_grad(); loss.backward(); optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(loader)
        print(f"[SSL Pretrain {epoch+1}/{pretrain_epochs}] loss={avg_loss:.6f}")

    torch.save(model.state_dict(), f"{MODEL_DIR}/{symbol}_{strategy}_ssl_pretrain.pt")
    print(f"âœ… SSL Pretraining ì™„ë£Œ: {symbol}-{strategy}")
