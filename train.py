import os, json, torch, torch.nn as nn, numpy as np, datetime, pytz, sys, pandas as pd
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score
from data.utils import SYMBOLS, get_kline_by_strategy, compute_features, create_dataset
from model.base_model import get_model
from model_weight_loader import get_model_weight
from feature_importance import compute_feature_importance, save_feature_importance
from wrong_data_loader import load_training_prediction_data
from failure_db import load_existing_failure_hashes
from logger import log_training_result, strategy_stats, load_failure_count
from window_optimizer import find_best_window
import hashlib
from collections import Counter
import sqlite3
from config import NUM_CLASSES


DEVICE = torch.device("cpu")
MODEL_DIR = "/persistent/models"
os.makedirs(MODEL_DIR, exist_ok=True)
now_kst = lambda: datetime.datetime.now(pytz.timezone("Asia/Seoul"))
STRATEGY_WRONG_REP = {"ë‹¨ê¸°": 4, "ì¤‘ê¸°": 6, "ì¥ê¸°": 8}

def get_feature_hash_from_tensor(x):
    if x.ndim != 2 or x.shape[0] == 0:
        return "invalid"
    last = x[-1].tolist()
    rounded = [round(float(val), 2) for val in last]
    return hashlib.sha1(",".join(map(str, rounded)).encode()).hexdigest()

def get_frequent_failures(min_count=5):
    counter = Counter()
    try:
        with sqlite3.connect("/persistent/logs/failure_patterns.db") as conn:
            rows = conn.execute("SELECT hash FROM failure_patterns").fetchall()
            for row in rows:
                counter[row[0]] += 1
    except:
        return set()
    return {h for h, cnt in counter.items() if cnt >= min_count}

def save_model_metadata(symbol, strategy, model_type, acc, f1, loss):
    meta = {
        "symbol": symbol,
        "strategy": strategy,
        "model": model_type,
        "accuracy": float(round(acc, 4)),
        "f1_score": float(round(f1, 4)),
        "loss": float(round(loss, 6)),
        "timestamp": now_kst().strftime("%Y-%m-%d %H:%M:%S")
    }
    path = f"{MODEL_DIR}/{symbol}_{strategy}_{model_type}.meta.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2, ensure_ascii=False)
    print(f"ğŸ—˜ ì €ì¥ë¨: {path}"); sys.stdout.flush()

def train_one_model(symbol, strategy, max_epochs=20):
    import os, gc
    import numpy as np
    import pandas as pd
    import torch
    from collections import Counter
    from logger import (
        get_fine_tune_targets, get_recent_predicted_classes, log_prediction,
        get_feature_hash_from_tensor, log_training_result
    )
    from model.base_model import get_model
    from feature_importance import compute_feature_importance, save_feature_importance
    from failure_db import load_existing_failure_hashes
    from focal_loss import FocalLoss
    from sklearn.metrics import accuracy_score, f1_score
    from torch.utils.data import TensorDataset, DataLoader
    from config import NUM_CLASSES

    print(f"â–¶ í•™ìŠµ ì‹œì‘: {symbol}-{strategy}")

    try:
        df = get_kline_by_strategy(symbol, strategy)
        if df is None or df.empty:
            print("â›” ì¤‘ë‹¨: get_kline_by_strategy() â†’ ë°ì´í„° ì—†ìŒ")
            return

        df_feat = compute_features(symbol, df, strategy)
        if df_feat is None or len(df_feat) < 30:
            print(f"â›” ì¤‘ë‹¨: compute_features ê²°ê³¼ ë¶€ì¡± â†’ {len(df_feat)}ê°œ")
            return

        if "timestamp" not in df_feat.columns:
            print("âš ï¸ timestamp ì—†ìŒ â†’ datetimeìœ¼ë¡œ ëŒ€ì²´")
            df_feat["timestamp"] = df_feat.get("datetime", pd.Timestamp.now())
        df_feat = df_feat.dropna()
        features = df_feat.to_dict(orient="records")

        window = find_best_window(symbol, strategy)
        if not isinstance(window, int) or window <= 0:
            print(f"â›” ì¤‘ë‹¨: find_best_window ì‹¤íŒ¨ â†’ {window}")
            return

        result = create_dataset(features, window=window, strategy=strategy)
        if not result or not isinstance(result, (list, tuple)) or len(result) != 2:
            print("â›” ì¤‘ë‹¨: create_dataset() ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜")
            return

        X_raw, y_raw = result
        if X_raw is None or y_raw is None:
            print("â›” ì¤‘ë‹¨: create_dataset() â†’ None ë°˜í™˜")
            return

        y_raw = np.array(y_raw)
        X_raw = np.array(X_raw)
        mask = (y_raw >= 0) & (y_raw < NUM_CLASSES)
        y_raw = y_raw[mask]
        X_raw = X_raw[mask]

        if len(X_raw) < 5:
            print(f"â›” ì¤‘ë‹¨: í•™ìŠµ ìƒ˜í”Œ ë¶€ì¡± â†’ X_raw ê°œìˆ˜: {len(X_raw)}")
            return

        observed = Counter(int(c) for c in y_raw if c >= 0)
        if len(observed) < 2:
            print(f"â›” ì¤‘ë‹¨: í´ë˜ìŠ¤ ë¶€ì¡± â†’ {len(observed)}ê°œ")
            return

        print(f"[ì§„ë‹¨] í”¼ì²˜ ìˆ˜: {len(df_feat)}")
        print(f"[ì§„ë‹¨] í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(X_raw)}")
        print(f"[ì§„ë‹¨] í´ë˜ìŠ¤ ìˆ˜: {len(set(y_raw))}")
        print(f"[ì§„ë‹¨] ì‹œí€€ìŠ¤ í¬ê¸°: {X_raw.shape}")
        print(f"[ì§„ë‹¨] ì…ë ¥ í”¼ì²˜ ìˆ˜: {X_raw.shape[2]} / í´ë˜ìŠ¤ ìˆ˜: {NUM_CLASSES}")

        input_size = X_raw.shape[2]
        val_len = max(5, int(len(X_raw) * 0.2))

        X_bal, y_bal = balance_classes(X_raw[:-val_len], y_raw[:-val_len], min_samples=20, target_classes=range(NUM_CLASSES))
        X_train, y_train = X_bal, y_bal
        X_val, y_val = X_raw[-val_len:], y_raw[-val_len:]

        failure_hashes = load_existing_failure_hashes()
        frequent_failures = get_frequent_failures(min_count=5)
        wrong_data = load_training_prediction_data(symbol, strategy, input_size, window, source_type="wrong")

        wrong_filtered, used_hashes = [], set()
        for s in wrong_data:
            if isinstance(s, (list, tuple)) and len(s) >= 2:
                xb, yb = s[:2]
                if not isinstance(xb, np.ndarray) or xb.shape != (window, input_size):
                    continue
                if not isinstance(yb, (int, np.integer)) or not (0 <= yb < NUM_CLASSES):
                    continue
                feature_hash = get_feature_hash_from_tensor(torch.tensor(xb))
                if feature_hash in used_hashes or feature_hash in failure_hashes or feature_hash in frequent_failures:
                    continue
                used_hashes.add(feature_hash)
                wrong_filtered.append((xb, yb))

        for model_type in ["lstm", "cnn_lstm", "transformer"]:
            model = get_model(model_type, input_size=input_size, output_size=NUM_CLASSES).train()
            model_path = f"/persistent/models/{symbol}_{strategy}_{model_type}.pt"
            if os.path.exists(model_path):
                try:
                    model.load_state_dict(torch.load(model_path, map_location=torch.device("cpu")))
                    print(f"ğŸ” ì´ì–´ í•™ìŠµ: {model_path}")
                except:
                    print(f"[ë¡œë“œ ì‹¤íŒ¨] {model_path} â†’ ìƒˆë¡œ í•™ìŠµ")

            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
            lossfn = FocalLoss(gamma=2)

            def train_failures(batch_data, repeat=6):
                ds = TensorDataset(torch.tensor([x for x, _ in batch_data], dtype=torch.float32),
                                   torch.tensor([y for _, y in batch_data], dtype=torch.long))
                loader = DataLoader(ds, batch_size=16, shuffle=True)
                for _ in range(repeat):
                    for xb, yb in loader:
                        model.train()
                        logits = model(xb)
                        loss = lossfn(logits, yb)
                        if not torch.isfinite(loss): continue
                        optimizer.zero_grad(); loss.backward(); optimizer.step()

            train_failures([(x, y) for x, y in wrong_filtered if y >= 8], repeat=6)
            train_failures([(x, y) for x, y in wrong_filtered if y < 8], repeat=2)

            try:
                target_class_set = set()
                recent_pred_classes = get_recent_predicted_classes(strategy, recent_days=3)
                fine_tune_targets = get_fine_tune_targets()
                if recent_pred_classes:
                    target_class_set.update([(strategy, c) for c in recent_pred_classes])
                for _, row in fine_tune_targets.iterrows():
                    target_class_set.add((row["strategy"], row["class"]))

                train_failures([(x, y) for x, y in wrong_filtered if (strategy, y) in target_class_set], repeat=6)
            except:
                print("âš ï¸ fine-tune ëŒ€ìƒ ë¶„ì„ ì‹¤íŒ¨ â†’ ì „ì²´ í•™ìŠµ ìœ ì§€")

            train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                                     torch.tensor(y_train, dtype=torch.long))
            train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)

            for _ in range(max_epochs):
                model.train()
                for xb, yb in train_loader:
                    logits = model(xb)
                    loss = lossfn(logits, yb)
                    if not torch.isfinite(loss): break
                    optimizer.zero_grad(); loss.backward(); optimizer.step()

            model.eval()
            with torch.no_grad():
                xb = torch.tensor(X_val, dtype=torch.float32)
                yb = torch.tensor(y_val, dtype=torch.long)
                logits = model(xb)
                preds = torch.argmax(logits, dim=1).numpy()
                acc = accuracy_score(y_val, preds)
                f1 = f1_score(y_val, preds, average="macro")
                val_loss = lossfn(logits, yb).item()

            if acc >= 1.0 and len(set(y_val)) <= 2:
                print(f"âš ï¸ ì˜¤ë²„í• ê°ì§€ â†’ ì €ì¥ ì¤‘ë‹¨")
                log_training_result(symbol, strategy, f"ì˜¤ë²„í•({model_type})", acc, f1, val_loss)
                continue
            if f1 > 1.0 or val_loss > 1.5 or acc < 0.3:
                print(f"âš ï¸ ë¹„ì •ìƒ ê²°ê³¼ ê°ì§€ â†’ ì €ì¥ ì¤‘ë‹¨ (acc={acc:.2f}, f1={f1:.2f}, loss={val_loss:.2f})")
                log_training_result(symbol, strategy, f"ë¹„ì •ìƒ({model_type})", acc, f1, val_loss)
                continue

            # âœ… ì˜ˆì¸¡ í´ë˜ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬ í¬í•¨í•œ ì•ˆì „ ì²˜ë¦¬
            try:
                predicted_class = int(preds[0]) if len(preds) > 0 else -1
            except:
                predicted_class = -1

            if predicted_class == -1 or not isinstance(predicted_class, int):
                print(f"âš ï¸ ì˜ˆì¸¡ í´ë˜ìŠ¤ ì—†ìŒ ë˜ëŠ” ë¬´íš¨ â†’ log_prediction ìƒëµ")
            else:
                log_prediction(
                    symbol=symbol,
                    strategy=strategy,
                    model=model_type,
                    predicted_class=predicted_class,
                    success=True,
                    rate=0.0,
                    source="í›ˆë ¨"
                )

            torch.save(model.state_dict(), model_path)
            save_model_metadata(symbol, strategy, model_type, acc, f1, val_loss)
            log_training_result(symbol, strategy, model_type, acc, f1, val_loss)

            try:
                imps = compute_feature_importance(model, xb, yb, list(df_feat.drop(columns=["timestamp"]).columns))
                save_feature_importance(imps, symbol, strategy, model_type)
            except:
                print("âš ï¸ ì¤‘ìš”ë„ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œë¨)")

            del model, xb, yb, logits
            torch.cuda.empty_cache()
            gc.collect()

    except Exception as e:
        print(f"[ì˜¤ë¥˜] {symbol}-{strategy} â†’ {e}")
        try:
            log_training_result(symbol, strategy, f"ì‹¤íŒ¨({str(e)})", 0.0, 0.0, 0.0)
        except:
            print("âš ï¸ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨")

def train_all_models():
    for strat in ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]:
        for sym in SYMBOLS:
            try:
                train_one_model(sym, strat)
            except Exception as e:
                print(f"[ì „ì²´ í•™ìŠµ ì˜¤ë¥˜] {sym}-{strat} â†’ {e}")
training_in_progress = {}

def train_model_loop(strategy):
    global training_in_progress
    if training_in_progress.get(strategy, False):
        print(f"âš ï¸ ì´ë¯¸ ì‹¤í–‰ ì¤‘: {strategy} í•™ìŠµ ì¤‘ë³µ ë°©ì§€")
        return
    training_in_progress[strategy] = True

    try:
        for symbol in SYMBOLS:
            try:
                train_one_model(symbol, strategy)
            except Exception as e:
                print(f"[í•™ìŠµ ì‹¤íŒ¨] {symbol}-{strategy} â†’ {e}")
    finally:
        training_in_progress[strategy] = False
        
def balance_classes(X, y, min_samples=20, target_classes=None):
    from collections import Counter
    import random
    import numpy as np

    if target_classes is None:
        target_classes = range(NUM_CLASSES)  # âœ… NUM_CLASSES = 21 ê¸°ì¤€ìœ¼ë¡œ ì ìš©

    class_counts = Counter(y)
    X_balanced, y_balanced = list(X), list(y)

    for cls in target_classes:
        count = class_counts.get(cls, 0)
        if count == 0:
            continue  # ì•„ì˜ˆ ì—†ëŠ” í´ë˜ìŠ¤ëŠ” ê±´ë„ˆëœ€
        if count >= min_samples:
            continue  # ì¶©ë¶„íˆ ë§ìœ¼ë©´ ê±´ë„ˆëœ€

        existing = [(x, y_val) for x, y_val in zip(X, y) if y_val == cls]
        while class_counts[cls] < min_samples and existing:
            x_dup, y_dup = random.choice(existing)
            X_balanced.append(x_dup)
            y_balanced.append(y_dup)
            class_counts[cls] += 1

    return np.array(X_balanced), np.array(y_balanced)
