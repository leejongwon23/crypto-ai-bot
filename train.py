import os, json, torch, torch.nn as nn, numpy as np, datetime, pytz, sys, pandas as pd
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score
from data.utils import SYMBOLS, get_kline_by_strategy, compute_features, create_dataset
from model.base_model import get_model
from model_weight_loader import get_model_weight
from feature_importance import compute_feature_importance, save_feature_importance
from wrong_data_loader import load_training_prediction_data
from failure_db import load_existing_failure_hashes
from logger import log_training_result, strategy_stats, load_failure_count
from window_optimizer import find_best_window
import hashlib
from collections import Counter
import sqlite3
import time
from data_augmentation import balance_classes
from config import get_NUM_CLASSES, get_FEATURE_INPUT_SIZE
NUM_CLASSES = get_NUM_CLASSES()
FEATURE_INPUT_SIZE = get_FEATURE_INPUT_SIZE()
from config import get_class_groups


training_in_progress = {"ë‹¨ê¸°": False, "ì¤‘ê¸°": False, "ì¥ê¸°": False}


DEVICE = torch.device("cpu")
MODEL_DIR = "/persistent/models"
os.makedirs(MODEL_DIR, exist_ok=True)
now_kst = lambda: datetime.datetime.now(pytz.timezone("Asia/Seoul"))
STRATEGY_WRONG_REP = {"ë‹¨ê¸°": 4, "ì¤‘ê¸°": 6, "ì¥ê¸°": 8}

def get_feature_hash_from_tensor(x, use_full=False, precision=3):
    """
    âœ… [ì„¤ëª…]
    - ë§ˆì§€ë§‰ timestep ë˜ëŠ” ì „ì²´ featureë¥¼ ë°˜ì˜¬ë¦¼ í›„ sha1 í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜
    """
    import hashlib
    if x.ndim != 2 or x.shape[0] == 0:
        return "invalid"
    try:
        flat = x.flatten() if use_full else x[-1]
        rounded = [round(float(val), precision) for val in flat]
        return hashlib.sha1(",".join(map(str, rounded)).encode()).hexdigest()
    except Exception as e:
        # âš ï¸ ë¡œê·¸ ê°„ì†Œí™”: ì˜¤ë¥˜ì‹œ ê°„ë‹¨ ì¶œë ¥
        print(f"[get_feature_hash_from_tensor ì˜¤ë¥˜] {e}")
        return "invalid"

def get_frequent_failures(min_count=5):
    """
    âœ… [ì„¤ëª…] failure_patterns.dbì—ì„œ ë™ì¼ ì‹¤íŒ¨ê°€ min_count ì´ìƒì´ë©´ ë°˜í™˜
    """
    counter = Counter()
    try:
        with sqlite3.connect("/persistent/logs/failure_patterns.db") as conn:
            rows = conn.execute("SELECT hash FROM failure_patterns").fetchall()
            for row in rows:
                counter[row[0]] += 1
    except:
        pass
    return {h for h, cnt in counter.items() if cnt >= min_count}


def save_model_metadata(symbol, strategy, model_type, acc, f1, loss, input_size=None, class_counts=None, used_feature_columns=None):
    """
    âœ… [ì„¤ëª…] ëª¨ë¸ ë©”íƒ€ì •ë³´ë¥¼ jsonìœ¼ë¡œ ì €ì¥ (used_feature_columns í¬í•¨)
    """
    meta = {
        "symbol": symbol, "strategy": strategy, "model": model_type or "unknown",
        "input_size": int(input_size) if input_size else 11,
        "accuracy": float(round(acc, 4)), "f1_score": float(round(f1, 4)),
        "loss": float(round(loss, 6)),
        "timestamp": now_kst().strftime("%Y-%m-%d %H:%M:%S")
    }

    if class_counts:
        meta["class_counts"] = {str(k): int(v) for k, v in class_counts.items()}

    if used_feature_columns:
        meta["used_feature_columns"] = used_feature_columns

    path = f"{MODEL_DIR}/{symbol}_{strategy}_{model_type}.meta.json"
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)  # âœ… ê²½ë¡œ ì—†ì„ ì‹œ ìë™ìƒì„±
        with open(path, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)
        print(f"[ë©”íƒ€ì €ì¥] {model_type} ({symbol}-{strategy}) acc={acc:.4f}")
    except Exception as e:
        print(f"[ERROR] meta ì €ì¥ ì‹¤íŒ¨: {e}")

def get_class_groups(num_classes=21, group_size=7):
    """
    âœ… í´ë˜ìŠ¤ ê·¸ë£¹í™” í•¨ìˆ˜ (YOPO v4.1)
    - num_classesë¥¼ group_size í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ê·¸ë£¹í™”
    - num_classes â‰¤ group_size ì‹œ ë‹¨ì¼ ê·¸ë£¹ ë°˜í™˜
    - ex) num_classes=21, group_size=7 â†’ [[0-6], [7-13], [14-20]]
    """
    if num_classes <= group_size:
        return [list(range(num_classes))]
    return [list(range(i, min(i+group_size, num_classes))) for i in range(0, num_classes, group_size)]

def train_one_model(symbol, strategy, group_id=None, max_epochs=20):
    import os, gc, traceback, torch, json, numpy as np, pandas as pd
    from datetime import datetime; from collections import Counter
    from ssl_pretrain import masked_reconstruction
    from config import get_FEATURE_INPUT_SIZE, get_class_groups
    from torch.utils.data import TensorDataset, DataLoader
    from model.base_model import get_model
    from logger import log_training_result
    from data_augmentation import balance_classes
    from wrong_data_loader import load_training_prediction_data
    from feature_importance import compute_feature_importance, drop_low_importance_features
    from ranger_adabelief import RangerAdaBelief as Ranger
    from window_optimizer import find_best_windows
    from data.utils import get_kline_by_strategy, compute_features, create_dataset
    from focal_loss import FocalLoss
    from meta_learning import maml_train_entry
    import pytz

    now_kst = lambda: datetime.now(pytz.timezone("Asia/Seoul"))
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    input_size = get_FEATURE_INPUT_SIZE()
    class_groups_list = get_class_groups()

    print(f"âœ… [train_one_model í˜¸ì¶œë¨] â–¶ [í•™ìŠµì‹œì‘] {symbol}-{strategy}-group{group_id}")
    trained_any = False

    try:
        masked_reconstruction(symbol, strategy, input_size=input_size, mask_ratio=0.2, epochs=5)

        df = get_kline_by_strategy(symbol, strategy)
        if df is None or df.empty:
            print("[âš ï¸ get_kline_by_strategy ê²°ê³¼ ì—†ìŒ â†’ dummyë¡œ ëŒ€ì²´]")
            df = pd.DataFrame([{"timestamp": i, "close": 100 + i} for i in range(100)])

        df_feat = compute_features(symbol, df, strategy)
        if df_feat is None or df_feat.empty or df_feat.isnull().values.any():
            print("[âš ï¸ compute_features ê²°ê³¼ ì´ìƒ â†’ dummyë¡œ ëŒ€ì²´]")
            df_feat = pd.DataFrame(np.random.normal(0, 1, size=(100, input_size)), columns=[f"f{i}" for i in range(input_size)])

        try:
            dummy_X = torch.tensor(np.random.rand(10, 20, input_size), dtype=torch.float32).to(DEVICE)
            dummy_y = torch.randint(0, 2, (10,), dtype=torch.long).to(DEVICE)
            importances = compute_feature_importance(get_model("lstm", input_size=input_size, output_size=2).to(DEVICE), dummy_X, dummy_y, [c for c in df_feat.columns if c not in ["timestamp", "strategy"]], method="baseline")
            df_feat = drop_low_importance_features(df_feat, importances, threshold=0.01, min_features=5)
        except: pass

        for window in find_best_windows(symbol, strategy) or [20]:
            print(f"â–¶ï¸ window={window} â†’ dataset ìƒì„± ì‹œì‘")
            X_y = create_dataset(df_feat.to_dict(orient="records"), window=window, strategy=strategy, input_size=input_size)
            if not X_y or not isinstance(X_y, tuple) or len(X_y) != 2:
                print(f"[âŒ create_dataset unpack ì‹¤íŒ¨] â†’ window={window}")
                log_training_result(symbol, strategy, f"í•™ìŠµì‹¤íŒ¨:dataset_unpackì‹¤íŒ¨_window{window}", 0.0, 0.0, 0.0)
                continue

            X_raw, y_raw = X_y
            if X_raw is None or y_raw is None or len(X_raw) == 0:
                print(f"[âŒ dataset ìƒì„± ì‹¤íŒ¨] â†’ window={window}")
                log_training_result(symbol, strategy, f"í•™ìŠµì‹¤íŒ¨:datasetì—†ìŒ_window{window}", 0.0, 0.0, 0.0)
                continue

            fail_X, fail_y = load_training_prediction_data(symbol, strategy, input_size=input_size, window=window)
            if fail_X is not None and len(fail_X) > 0:
                X_raw = np.concatenate([X_raw, fail_X], axis=0)
                y_raw = np.concatenate([y_raw, fail_y], axis=0)

            val_len = max(5, int(len(X_raw) * 0.2))
            if len(X_raw) <= val_len:
                val_indices = np.random.choice(len(X_raw), val_len, replace=True)
                X_val, y_val, X_train, y_train = X_raw[val_indices], y_raw[val_indices], X_raw, y_raw
            else:
                X_train, y_train, X_val, y_val = X_raw[:-val_len], y_raw[:-val_len], X_raw[-val_len:], y_raw[-val_len:]

            for gid in [group_id] if group_id is not None else range(len(class_groups_list)):
                group_classes = class_groups_list[gid]
                print(f"â–¶ï¸ í•™ìŠµ group{gid}: í´ë˜ìŠ¤ ìˆ˜ {len(group_classes)}")

                if not group_classes:
                    log_training_result(symbol, strategy, f"í•™ìŠµì‹¤íŒ¨:ë¹ˆê·¸ë£¹_group{gid}_window{window}", 0.0, 0.0, 0.0)
                    continue

                tm = np.isin(y_train, group_classes); vm = np.isin(y_val, group_classes)
                X_train_group, y_train_group = X_train[tm], y_train[tm]
                X_val_group, y_val_group = X_val[vm], y_val[vm]

                print(f"â–¶ï¸ group{gid} â†’ train:{len(y_train_group)}, val:{len(y_val_group)}")

                if len(y_train_group) < 2:
                    print(f"[âŒ train ë¶€ì¡±] group{gid}")
                    log_training_result(symbol, strategy, f"í•™ìŠµì‹¤íŒ¨:ë°ì´í„°ë¶€ì¡±_group{gid}_window{window}", 0.0, 0.0, 0.0)
                    continue
                if len(y_val_group) == 0:
                    print(f"[âŒ val ì—†ìŒ] group{gid}")
                    log_training_result(symbol, strategy, f"í•™ìŠµì‹¤íŒ¨:valì—†ìŒ_group{gid}_window{window}", 0.0, 0.0, 0.0)
                    continue

                try:
                    y_train_group = np.array([group_classes.index(y) for y in y_train_group])
                    y_val_group = np.array([group_classes.index(y) for y in y_val_group])
                except Exception as e:
                    log_training_result(symbol, strategy, f"í•™ìŠµì‹¤íŒ¨:labelë¶ˆì¼ì¹˜_group{gid}_window{window}", 0.0, 0.0, 0.0)
                    continue

                X_train_group, y_train_group = balance_classes(X_train_group, y_train_group, min_count=20, num_classes=len(group_classes))

                for model_type in ["lstm", "cnn_lstm", "transformer"]:
                    try:
                        model = get_model(model_type, input_size=input_size, output_size=len(group_classes)).to(DEVICE).train()
                        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
                        lossfn = FocalLoss()
                        to_tensor = lambda x: torch.tensor(x[:, -1, :], dtype=torch.float32)
                        Xtt, ytt = to_tensor(X_train_group), torch.tensor(y_train_group, dtype=torch.long)
                        Xvt, yvt = to_tensor(X_val_group), torch.tensor(y_val_group, dtype=torch.long)
                        train_loader = DataLoader(TensorDataset(Xtt, ytt), batch_size=32, shuffle=True)
                        val_loader = DataLoader(TensorDataset(Xvt, yvt), batch_size=32)

                        for _ in range(max_epochs):
                            for xb, yb in train_loader:
                                xb, yb = xb.to(DEVICE), yb.to(DEVICE)
                                loss = lossfn(model(xb), yb)
                                if torch.isfinite(loss): optimizer.zero_grad(); loss.backward(); optimizer.step()

                        maml_train_entry(model, train_loader, val_loader, inner_lr=0.01, outer_lr=0.001, inner_steps=1)
                        model.eval()
                        with torch.no_grad():
                            val_preds = torch.argmax(model(Xvt.to(DEVICE)), dim=1)
                            val_acc = (val_preds == yvt.to(DEVICE)).float().mean().item()

                        model_name = f"{model_type}_AdamW_FocalLoss_lr1e-4_bs=32_hs=64_dr=0.3_group{gid}_window{window}"
                        model_path = f"/persistent/models/{symbol}_{strategy}_{model_name}.pt"
                        torch.save(model.state_dict(), model_path)
                        print(f"[âœ… ì €ì¥ë¨] {model_path}")

                        meta = {
                            "symbol": symbol,
                            "strategy": strategy,
                            "model": model_type,
                            "group_id": gid,
                            "window": window,
                            "input_size": input_size,
                            "output_size": len(group_classes),
                            "model_name": model_name,
                            "timestamp": now_kst().isoformat()
                        }

                        with open(model_path.replace(".pt", ".meta.json"), "w", encoding="utf-8") as f:
                            json.dump(meta, f, ensure_ascii=False, indent=2)

                        log_training_result(symbol, strategy, model_name, acc=val_acc, f1=0.0, loss=loss.item())
                        trained_any = True

                        del model, optimizer, lossfn, train_loader, val_loader
                        torch.cuda.empty_cache(); gc.collect()
                    except Exception as inner_e:
                        reason = f"{type(inner_e).__name__}: {inner_e}"
                        log_training_result(symbol, strategy, f"í•™ìŠµì‹¤íŒ¨:{reason}", 0.0, 0.0, 0.0)
                        print(f"[âŒ ë‚´ë¶€ ì˜ˆì™¸] group{gid} window{window} â†’ {reason}")

        if not trained_any:
            print(f"[âŒ ëª¨ë¸ ì „ë¶€ ì‹¤íŒ¨] ì €ì¥ëœ ëª¨ë¸ ì—†ìŒ â†’ {symbol}-{strategy}")
            log_training_result(symbol, strategy, f"í•™ìŠµì‹¤íŒ¨:ì „ê·¸ë£¹í•™ìŠµë¶ˆê°€", 0.0, 0.0, 0.0)

    except Exception as e:
        reason = f"{type(e).__name__}: {e}"
        log_training_result(symbol, strategy, f"í•™ìŠµì‹¤íŒ¨:ì „ì²´ì˜ˆì™¸:{reason}", 0.0, 0.0, 0.0)
        print(f"[âŒ ì „ì²´ ì˜ˆì™¸] {symbol}-{strategy}: {reason}")


# âœ… augmentation í•¨ìˆ˜ ì¶”ê°€
def augment_and_expand(X_train_group, y_train_group, repeat_factor, group_classes, target_count):
    import numpy as np
    import random
    from data_augmentation import add_gaussian_noise, apply_scaling, apply_shift, apply_dropout_mask

    X_aug, y_aug = [], []

    # âœ… í´ë˜ìŠ¤ë³„ ê°œìˆ˜ ê³„ì‚°
    class_counts = {cls: np.sum(y_train_group == cls) for cls in group_classes}
    max_count = max(class_counts.values()) if class_counts else 1
    per_class_target = int(max_count * 0.8)  # ğŸ”¥ ìµœëŒ€ í´ë˜ìŠ¤ì˜ 80%ë¡œ í†µì¼

    for cls in group_classes:
        cls_indices = np.where(y_train_group == cls)[0]

        if len(cls_indices) == 0:
            # âœ… í•´ë‹¹ í´ë˜ìŠ¤ ìƒ˜í”Œ ì—†ìœ¼ë©´ random noise + ì•ˆì „ ë¼ë²¨ ë¶€ì—¬
            dummy = np.random.normal(0, 1, (per_class_target, X_train_group.shape[1], X_train_group.shape[2])).astype(np.float32)
            X_cls_aug = dummy
            y_cls_aug = np.array([cls] * per_class_target, dtype=np.int64)
        else:
            X_cls = X_train_group[cls_indices]
            y_cls = y_train_group[cls_indices]

            # ğŸ” ë¶€ì¡±ë¶„ ë³µì œ + augmentation (noise + scaling + shift + dropout)
            n_repeat = int(np.ceil(per_class_target / len(cls_indices)))
            X_cls_oversampled = np.tile(X_cls, (n_repeat, 1, 1))[:per_class_target]
            y_cls_oversampled = np.tile(y_cls, n_repeat)[:per_class_target]

            X_cls_aug = []
            for x in X_cls_oversampled:
                x1 = add_gaussian_noise(x)
                x2 = apply_scaling(x1)
                x3 = apply_shift(x2)
                x4 = apply_dropout_mask(x3)
                # âœ… mixup ì¶”ê°€ (ê°„ë‹¨ ë¯¹ìŠ¤ì—… â€“ ìê¸° ìì‹  + noise)
                mixup_factor = np.random.uniform(0.7, 1.0)
                x4 = x4 * mixup_factor + np.random.normal(0, 0.05, x4.shape).astype(np.float32) * (1 - mixup_factor)
                X_cls_aug.append(x4)

            X_cls_aug = np.array(X_cls_aug, dtype=np.float32)
            y_cls_aug = y_cls_oversampled

        X_aug.append(X_cls_aug)
        y_aug.append(y_cls_aug)

    X_aug = np.concatenate(X_aug, axis=0)
    y_aug = np.concatenate(y_aug, axis=0)

    # âœ… ìµœì¢… target_count ë„ë‹¬ ë³´ì¥
    if len(X_aug) < target_count:
        idx = np.random.choice(len(X_aug), target_count - len(X_aug))
        X_aug = np.concatenate([X_aug, X_aug[idx]], axis=0)
        y_aug = np.concatenate([y_aug, y_aug[idx]], axis=0)
    else:
        X_aug = X_aug[:target_count]
        y_aug = y_aug[:target_count]

    # âœ… ë¼ë²¨ ì¬ì¸ì½”ë”©
    y_encoded = []
    X_encoded = []
    for i, y in enumerate(y_aug):
        try:
            encoded = group_classes.index(y)
            y_encoded.append(encoded)
            X_encoded.append(X_aug[i])
        except ValueError:
            print(f"[âŒ ë¼ë²¨ ì¬ì¸ì½”ë”© ì˜¤ë¥˜] {y} not in group_classes â†’ ì œê±°")
            continue

    X_encoded = np.array(X_encoded, dtype=np.float32)
    y_encoded = np.array(y_encoded, dtype=np.int64)

    # âœ… ë””ë²„ê·¸ ì¶œë ¥
    from collections import Counter
    print(f"[âœ… augment_and_expand] ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(y_encoded)}, ë¼ë²¨ ë¶„í¬: {Counter(y_encoded)}")

    return X_encoded, y_encoded


def train_all_models():
    """
    âœ… [ì„¤ëª…] SYMBOLS ì „ì²´ì— ëŒ€í•´ ë‹¨ê¸°, ì¤‘ê¸°, ì¥ê¸° í•™ìŠµ ìˆ˜í–‰
    """
    global training_in_progress
    from telegram_bot import send_message
    strategies = ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]

    for strategy in strategies:
        if training_in_progress.get(strategy, False):
            print(f"âš ï¸ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: {strategy}")
            continue

        print(f"ğŸš€ {strategy} í•™ìŠµ ì‹œì‘")
        training_in_progress[strategy] = True

        try:
            for symbol in SYMBOLS:
                try:
                    train_one_model(symbol, strategy)
                except Exception as e:
                    print(f"[ì˜¤ë¥˜] {symbol}-{strategy} í•™ìŠµ ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"[ì¹˜ëª… ì˜¤ë¥˜] {strategy} í•™ìŠµ ì¤‘ë‹¨: {e}")
        finally:
            training_in_progress[strategy] = False
            print(f"âœ… {strategy} í•™ìŠµ ì™„ë£Œ")

        time.sleep(5)

    send_message("âœ… ì „ì²´ í•™ìŠµ ì™„ë£Œ. ì˜ˆì¸¡ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

def train_models(symbol_list):
    """
    âœ… [YOPO êµ¬ì¡° ë°˜ì˜]
    - ì‹¬ë³¼ í•˜ë‚˜ë‹¹: í´ë˜ìŠ¤ê·¸ë£¹ ì „ì²´ â†’ ì „ëµ ìˆœì„œë¡œ í•™ìŠµ
    - ëª¨ë“  ê·¸ë£¹/ì „ëµ í•™ìŠµ ì™„ë£Œ í›„ ë‹¤ìŒ ì‹¬ë³¼ë¡œ ë„˜ì–´ê°
    - ì˜ˆì¸¡ ì‹¤í–‰ì€ í•˜ì§€ ì•ŠìŒ (ì™¸ë¶€ recommendì—ì„œ í˜¸ì¶œ)
    - meta.json ì¼ê´„ ë³´ì • ìˆ˜í–‰
    """
    global training_in_progress
    from telegram_bot import send_message
    import maintenance_fix_meta
    from config import get_class_groups
    import time

    strategies = ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]
    class_groups = get_class_groups()
    group_ids = list(range(len(class_groups)))

    print(f"ğŸš€ [train_models] ì‹¬ë³¼ í•™ìŠµ ì‹œì‘: {symbol_list}")

    for symbol in symbol_list:
        print(f"\nğŸ” [ì‹¬ë³¼ ì‹œì‘] {symbol}")

        for group_id in group_ids:
            print(f"â–¶ ê·¸ë£¹ {group_id} í•™ìŠµ ì‹œì‘")

            for strategy in strategies:
                if training_in_progress.get(strategy, False):
                    print(f"âš ï¸ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: {strategy}")
                    continue

                training_in_progress[strategy] = True
                try:
                    train_one_model(symbol, strategy, group_id=group_id)
                except Exception as e:
                    print(f"[âŒ í•™ìŠµ ì‹¤íŒ¨] {symbol}-{strategy}-group{group_id} â†’ {e}")
                finally:
                    training_in_progress[strategy] = False
                    print(f"âœ… {symbol}-{strategy}-group{group_id} í•™ìŠµ ì™„ë£Œ")
                    time.sleep(2)

    # âœ… ëª¨ë“  í•™ìŠµ í›„ ë©”íƒ€ ë³´ì •
    try:
        maintenance_fix_meta.fix_all_meta_json()
        print(f"âœ… meta ë³´ì • ì™„ë£Œ: {symbol_list}")
    except Exception as e:
        print(f"[âš ï¸ meta ë³´ì • ì‹¤íŒ¨] {e}")

    # âœ… ì‹¤íŒ¨í•™ìŠµ ìë™ ì‹¤í–‰ ì¶”ê°€
    try:
        import failure_trainer
        failure_trainer.run_failure_training()
    except Exception as e:
        print(f"[âŒ ì‹¤íŒ¨í•™ìŠµ ë£¨í”„ ì˜ˆì™¸] {e}")

    send_message(f"âœ… ì „ì²´ ì‹¬ë³¼ í•™ìŠµ ì™„ë£Œ: {symbol_list}")


def train_model_loop(strategy):
    """
    âœ… [ì„¤ëª…] íŠ¹ì • strategy í•™ìŠµì„ ë¬´í•œ ë£¨í”„ë¡œ ì‹¤í–‰
    - training_in_progress ìƒíƒœ ê´€ë¦¬ í¬í•¨
    """
    global training_in_progress
    if training_in_progress.get(strategy, False):
        print(f"âš ï¸ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: {strategy}")
        return

    training_in_progress[strategy] = True
    print(f"ğŸš€ {strategy} ë¬´í•œ í•™ìŠµ ë£¨í”„ ì‹œì‘")

    try:
        for symbol in SYMBOLS:
            try:
                train_one_model(symbol, strategy)
            except Exception as e:
                print(f"[ì˜¤ë¥˜] {symbol}-{strategy} í•™ìŠµ ì‹¤íŒ¨: {e}")
    finally:
        training_in_progress[strategy] = False
        print(f"âœ… {strategy} ë£¨í”„ ì¢…ë£Œ")

def train_symbol_group_loop(delay_minutes=5):
    """
    âœ… ì‹¬ë³¼ â†’ ì „ëµ ìˆœì„œë¡œ ìˆœì°¨ í•™ìŠµë˜ë„ë¡ ê°œì„ 
    âœ… ì‹¬ë³¼ë³„ ì „ëµë³„ í´ë˜ìŠ¤ ì „ì²´ ê·¸ë£¹ í•™ìŠµ ì™„ë£Œ í›„ ë‹¤ìŒ ì‹¬ë³¼ë¡œ ì´ë™
    """
    import time
    import maintenance_fix_meta
    from data.utils import SYMBOL_GROUPS, _kline_cache, _feature_cache
    from train import train_one_model

    group_count = len(SYMBOL_GROUPS)
    print(f"ğŸš€ ì „ì²´ {group_count}ê°œ ê·¸ë£¹ í•™ìŠµ ë£¨í”„ ì‹œì‘")

    loop_count = 0
    while True:
        loop_count += 1
        print(f"\nğŸ”„ ê·¸ë£¹ í•™ìŠµ ë£¨í”„ #{loop_count} ì‹œì‘")

        for idx, group in enumerate(SYMBOL_GROUPS):
            print(f"\nğŸš€ [ê·¸ë£¹ {idx}/{group_count}] í•™ìŠµ ì‹œì‘ | ì‹¬ë³¼: {group}")

            _kline_cache.clear()
            _feature_cache.clear()
            print("[âœ… cache cleared] _kline_cache, _feature_cache")

            try:
                # âœ… ê° ì‹¬ë³¼ì— ëŒ€í•´ ì „ëµ ìˆœì°¨ í•™ìŠµ (â†’ í´ë˜ìŠ¤ ê·¸ë£¹ ì „ì²´ í•™ìŠµ)
                for symbol in group:
                    for strategy in ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]:
                        try:
                            train_one_model(symbol, strategy, group_id=None)  # âœ… í•µì‹¬ ìˆ˜ì •
                            print(f"[âœ… í•™ìŠµ ì™„ë£Œ] {symbol}-{strategy}")
                        except Exception as e:
                            print(f"[âŒ í•™ìŠµ ì‹¤íŒ¨] {symbol}-{strategy} â†’ {e}")

                # âœ… ë©”íƒ€ ì •ë³´ ë³´ì •
                maintenance_fix_meta.fix_all_meta_json()
                print(f"[âœ… meta ë³´ì • ì™„ë£Œ] ê·¸ë£¹ {idx}")

                # âœ… í•™ìŠµ í›„ ì˜ˆì¸¡ê¹Œì§€ ìë™ ìˆ˜í–‰
                from recommend import main
                for symbol in group:
                    for strategy in ["ë‹¨ê¸°", "ì¤‘ê¸°", "ì¥ê¸°"]:
                        try:
                            main(symbol=symbol, strategy=strategy, force=True, allow_prediction=True)
                            print(f"[âœ… ì˜ˆì¸¡ ì™„ë£Œ] {symbol}-{strategy}")
                        except Exception as e:
                            print(f"[âŒ ì˜ˆì¸¡ ì‹¤íŒ¨] {symbol}-{strategy} â†’ {e}")

                print(f"ğŸ•’ ê·¸ë£¹ {idx} ë£¨í”„ ì™„ë£Œ â†’ {delay_minutes}ë¶„ ëŒ€ê¸°")
                time.sleep(delay_minutes * 60)

            except Exception as e:
                print(f"[âŒ ê·¸ë£¹ {idx} ë£¨í”„ ì¤‘ ì˜¤ë¥˜] {e}")
                continue


def pretrain_ssl_features(symbol, strategy, pretrain_epochs=5):
    """
    âœ… [ì„¤ëª…] Self-Supervised Learning pretraining
    - feature reconstruction ê¸°ë°˜ ì‚¬ì „í•™ìŠµ
    """
    from model.base_model import get_model

    print(f"â–¶ SSL Pretraining ì‹œì‘: {symbol}-{strategy}")

    df = get_kline_by_strategy(symbol, strategy)
    if df is None or df.empty:
        print("â›” ì¤‘ë‹¨: ì‹œì„¸ ë°ì´í„° ì—†ìŒ")
        return

    df_feat = compute_features(symbol, df, strategy)
    if df_feat is None or df_feat.empty or df_feat.isnull().any().any():
        print("â›” ì¤‘ë‹¨: í”¼ì²˜ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” NaN")
        return

    features_only = df_feat.drop(columns=["timestamp"], errors="ignore")
    feat_scaled = MinMaxScaler().fit_transform(features_only)
    X = np.expand_dims(feat_scaled, axis=1)  # (samples, 1, features)

    input_size = X.shape[2]
    model = get_model("autoencoder", input_size=input_size, output_size=input_size).to(DEVICE).train()

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    lossfn = nn.MSELoss()

    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(X, dtype=torch.float32))
    loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)

    for epoch in range(pretrain_epochs):
        total_loss = 0.0
        for xb, yb in loader:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            out = model(xb)
            loss = lossfn(out, yb)
            optimizer.zero_grad(); loss.backward(); optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(loader)
        print(f"[SSL Pretrain {epoch+1}/{pretrain_epochs}] loss={avg_loss:.6f}")

    torch.save(model.state_dict(), f"{MODEL_DIR}/{symbol}_{strategy}_ssl_pretrain.pt")
    print(f"âœ… SSL Pretraining ì™„ë£Œ: {symbol}-{strategy}")
