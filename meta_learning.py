# meta_learning.py

import torch
import torch.nn as nn
import torch.optim as optim

class MAML:
    def __init__(self, model, inner_lr=0.01, outer_lr=0.001, inner_steps=1):
        self.model = model
        self.inner_lr = inner_lr
        self.outer_lr = outer_lr
        self.inner_steps = inner_steps
        self.optimizer = optim.Adam(self.model.parameters(), lr=outer_lr)
        self.loss_fn = nn.CrossEntropyLoss()

    def adapt(self, X, y):
        """
        âœ… [ìˆ˜ì •]
        - deepcopy ì œê±° â†’ ë©”ëª¨ë¦¬ ìµœì í™”
        - functional API ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¡œ ë³€ê²½
        """
        adapted_params = {name: param for name, param in self.model.named_parameters()}
        for _ in range(self.inner_steps):
            logits = self.model.forward(X, params=adapted_params) if hasattr(self.model, 'forward') else self.model(X)
            loss = self.loss_fn(logits, y)
            grads = torch.autograd.grad(loss, adapted_params.values(), create_graph=True)

            # âœ… inplace gradient step
            adapted_params = {
                name: param - self.inner_lr * grad
                for (name, param), grad in zip(adapted_params.items(), grads)
            }

        # âœ… adapted_params ë°˜í™˜ â†’ ì™¸ë¶€ì—ì„œ functional forward í˜¸ì¶œ ê°€ëŠ¥
        return adapted_params

    def meta_update(self, tasks):
        meta_loss = 0.0
        for X_train, y_train, X_val, y_val in tasks:
            adapted_params = self.adapt(X_train, y_train)
            # functional forward with adapted_params
            logits = self.model.forward(X_val, params=adapted_params) if hasattr(self.model, 'forward') else self.model(X_val)
            loss = self.loss_fn(logits, y_val)
            meta_loss += loss

        meta_loss /= len(tasks)

        self.optimizer.zero_grad()
        meta_loss.backward()
        self.optimizer.step()

        return meta_loss.item()

# âœ… [ì¶”ê°€] train.pyì—ì„œ í˜¸ì¶œ ê°€ëŠ¥ êµ¬ì¡° ì˜ˆì‹œ
def maml_train_entry(model, train_loader, val_loader, inner_lr=0.01, outer_lr=0.001, inner_steps=1):
    from meta_learning import MAML  # ë‚´ë¶€ ì •ì˜ëœ í´ë˜ìŠ¤ë¼ë©´ ì´ ì„í¬íŠ¸ í•„ìš”

    try:
        maml = MAML(model, inner_lr=inner_lr, outer_lr=outer_lr, inner_steps=inner_steps)
        tasks = []

        for (X_train, y_train), (X_val, y_val) in zip(train_loader, val_loader):
            tasks.append((X_train, y_train, X_val, y_val))

        if not tasks:
            print("[MAML skip] ìœ íš¨í•œ meta task ì—†ìŒ â†’ meta update ìƒëµ")
            return None

        loss = maml.meta_update(tasks)
        print(f"[âœ… MAML meta-update ì™„ë£Œ] task={len(tasks)}, loss={loss:.4f}")
        return loss

    except Exception as e:
        print(f"[âŒ MAML ì˜ˆì™¸ ë°œìƒ] {e}")
        return None



import os, pickle
import numpy as np
from sklearn.linear_model import LogisticRegression

META_MODEL_PATH = "/persistent/models/meta_learner.pkl"

def train_meta_learner(model_outputs_list, true_labels):
    X = [np.array(mo).flatten() for mo in model_outputs_list]
    y = np.array(true_labels)

    clf = LogisticRegression(max_iter=500)
    clf.fit(X, y)

    with open(META_MODEL_PATH, "wb") as f:
        pickle.dump(clf, f)
    print(f"[âœ… meta learner í•™ìŠµ ì™„ë£Œ ë° ì €ì¥] {META_MODEL_PATH}")

    return clf

def load_meta_learner():
    if os.path.exists(META_MODEL_PATH):
        with open(META_MODEL_PATH, "rb") as f:
            clf = pickle.load(f)
        print("[âœ… meta learner ë¡œë“œ ì™„ë£Œ]")
        return clf
    else:
        print("[âš ï¸ meta learner íŒŒì¼ ì—†ìŒ â†’ None ë°˜í™˜]")
        return None

import numpy as np

def get_meta_prediction(model_outputs_list, feature_tensor=None, meta_info=None):
    """
    ë©”íƒ€ ëŸ¬ë„ˆ: ì—¬ëŸ¬ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼(softmax í™•ë¥  ë²¡í„°)ì™€ ê³¼ê±° ì„±ëŠ¥Â·ì˜ˆìƒ ìˆ˜ìµë¥  ì •ë³´ë¥¼ ì´ìš©í•´
    ìµœì¢… ì˜ˆì¸¡ í´ë˜ìŠ¤(final_pred_class)ë¥¼ ê²°ì •.
    - ì„±ê³µë¥  ë°ì´í„°ê°€ ì—†ìœ¼ë©´: softmax í™•ë¥  + íŒ¨í„´ ì í•©ë„ ê¸°ë°˜
    - ì„±ê³µë¥  ë°ì´í„°ê°€ ìˆìœ¼ë©´: ì„±ê³µë¥  + ì‹¤íŒ¨ìœ¨ + softmax + ìˆ˜ìµë¥  ì¢…í•© í‰ê°€
    """

    import numpy as np

    # âœ… 1. ìœ íš¨ì„± ê²€ì‚¬
    if not model_outputs_list or len(model_outputs_list) == 0:
        raise ValueError("âŒ get_meta_prediction: ëª¨ë¸ ì¶œë ¥ ì—†ìŒ")

    # âœ… 2. softmax í™•ë¥  ë²¡í„° ì¶”ì¶œ
    softmax_list = []
    for m in model_outputs_list:
        if "probs" not in m:
            raise KeyError(f"âŒ get_meta_prediction: 'probs' í‚¤ ëˆ„ë½ â†’ {m}")
        softmax_list.append(np.array(m["probs"], dtype=np.float32))

    num_classes = len(softmax_list[0])
    avg_softmax = np.mean(softmax_list, axis=0)

    # âœ… 3. meta_info
    success_rate_dict = meta_info.get("success_rate", {}) if meta_info else {}
    expected_return_dict = meta_info.get("expected_return", {}) if meta_info else {}
    failure_rate_dict = {
        cls: (1.0 - success_rate_dict.get(cls, 0.5))
        for cls in range(num_classes)
    }

    # âœ… 4. ì ìˆ˜ ê³„ì‚°
    scores = np.zeros(num_classes, dtype=np.float32)

    if not success_rate_dict:  
        # ğŸ“Œ ì„±ê³µë¥  ë°ì´í„° ì—†ì„ ë•Œ â†’ softmax + ê¸°ë³¸ íŒ¨í„´ ì í•©ë„ ê¸°ë°˜
        # (íŒ¨í„´ ì í•©ë„: softmax ì•ˆì •ì„±, í‘œì¤€í¸ì°¨ ë‚®ì„ìˆ˜ë¡ ì•ˆì •)
        stability_weight = 1.0 - np.std(softmax_list, axis=0)
        for cls in range(num_classes):
            scores[cls] = avg_softmax[cls] * stability_weight[cls]

        mode = "ê¸°ë³¸ ë©”íƒ€ (ì„±ê³µë¥  ç„¡)"
    else:
        # ğŸ“Œ ì„±ê³µë¥  ë°ì´í„° ìˆì„ ë•Œ â†’ ì„±ê³µë¥  + ì‹¤íŒ¨ìœ¨ + softmax + ì˜ˆìƒìˆ˜ìµë¥  ì¢…í•©
        for cls in range(num_classes):
            sr = success_rate_dict.get(cls, 0.5)
            fr = failure_rate_dict.get(cls, 0.5)
            er = expected_return_dict.get(cls, 1.0)
            # ì„±ê³µë¥  ë†’ì„ìˆ˜ë¡, ì‹¤íŒ¨ìœ¨ ë‚®ì„ìˆ˜ë¡, softmax ë†’ì„ìˆ˜ë¡, ìˆ˜ìµë¥  ë†’ì„ìˆ˜ë¡ ê°€ì 
            scores[cls] = avg_softmax[cls] * (sr - fr) * abs(er)

        mode = "ì„±ê³µë¥  ê¸°ë°˜ ë©”íƒ€"

    # âœ… 5. ìµœì¢… í´ë˜ìŠ¤ ì„ íƒ
    final_pred_class = int(np.argmax(scores))

    # âœ… 6. ìƒì„¸ ë¡œê·¸
    print(f"[META] {mode} â†’ í´ë˜ìŠ¤ë³„ ì ìˆ˜ ê³„ì‚°:")
    for cls in range(num_classes):
        sr = success_rate_dict.get(cls, 0.0) if success_rate_dict else None
        er = expected_return_dict.get(cls, None)
        print(f"  cls {cls}: softmax={avg_softmax[cls]:.4f}, "
              f"{'ì„±ê³µë¥ ='+str(round(sr,2)) if sr is not None else ''} "
              f"{'ì˜ˆìƒìˆ˜ìµë¥ ='+str(round(er,2)) if er is not None else ''} "
              f"ì ìˆ˜={scores[cls]:.4f}")

    print(f"[META] ìµœì¢… í´ë˜ìŠ¤ ì„ íƒ â†’ {final_pred_class} "
          f"(ì ìˆ˜: {scores[final_pred_class]:.4f})")

    return final_pred_class
