# === feature_importance.py (SAFE: no_grad, ì‹œê°„ì˜ˆì‚°/ì¡°ê¸°ì¤‘ë‹¨, í¼ë®¤í…Œì´ì…˜ í†µì¼) ===
import os
import json
import time
import torch
import numpy as np
import pandas as pd

PERSIST_DIR = "/persistent"
IMPORTANCE_DIR = os.path.join(PERSIST_DIR, "importances")
os.makedirs(IMPORTANCE_DIR, exist_ok=True)

def _ensure_feature_names(feature_names, n_features):
    if isinstance(feature_names, (list, tuple)) and len(feature_names) == n_features:
        return list(feature_names)
    # ë¶€ì¡±/ì—†ìŒ â†’ ê¸°ë³¸ ì´ë¦„ ë¶€ì—¬
    return [f"f_{i}" for i in range(n_features)]

@torch.no_grad()
def compute_feature_importance(
    model,
    X_val: torch.Tensor,
    y_val: torch.Tensor,
    feature_names,
    method: str = "baseline",
    *,
    max_seconds: float = 30.0,    # â±ï¸ ì „ì²´ ê³„ì‚° ì‹œê°„ì˜ˆì‚°(ì´ˆ)
    print_every: int = 20         # ì§„í–‰ ë¡œê·¸ ê°„ê²©(íŠ¹ì„± ê°œìˆ˜ ê¸°ì¤€)
):
    """
    âœ… feature importance ê³„ì‚° (permutation ë°©ì‹, ë©”ëª¨ë¦¬/ì‹œê°„ ê°€ë“œ)
    - ì…ë ¥ X_val shape: (N, T, F)
    - ì˜ˆì‚° ì´ˆê³¼ ì‹œ ë‚¨ì€ í”¼ì²˜ëŠ” 0.0ìœ¼ë¡œ ì±„ìš°ê³  ì¡°ê¸° ì¢…ë£Œ
    """
    model.eval()
    start = time.time()

    try:
        logits = model(X_val)
        y_val = y_val.view(-1).long()
        baseline_loss = torch.nn.CrossEntropyLoss()(logits, y_val).item()
    except Exception as e:
        print(f"[ERROR] ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨ ({method}): {e}")
        n_feat = X_val.shape[2]
        names = _ensure_feature_names(feature_names, n_feat)
        return dict(zip(names, [0.0] * n_feat))

    n_feat = X_val.shape[2]
    names = _ensure_feature_names(feature_names, n_feat)
    importances = np.zeros(n_feat, dtype=float)

    for i in range(n_feat):
        # â±ï¸ ì‹œê°„ ì˜ˆì‚° ì²´í¬
        if (time.time() - start) > float(max_seconds):
            print(f"[WARN] importance ì‹œê°„ì˜ˆì‚° ì´ˆê³¼ â†’ i={i}/{n_feat}ì—ì„œ ì¤‘ë‹¨(ì”ì—¬=0.0)")
            break
        try:
            X_perm = X_val.clone()
            # âœ… permutation ì¼ê´€í™”: ë°°ì¹˜ ì¶•ë§Œ ì…”í”Œ
            perm_idx = torch.randperm(X_val.shape[0], device=X_val.device)
            X_perm[:, :, i] = X_perm[perm_idx, :, i]
            logits_perm = model(X_perm)
            loss = torch.nn.CrossEntropyLoss()(logits_perm, y_val).item()
            importances[i] = float(loss - baseline_loss)
            if print_every and (i % max(1, int(print_every)) == 0):
                print(f"[imp] {i+1}/{n_feat} lossÎ”={importances[i]:.6f}")
        except Exception as e:
            print(f"[ERROR] ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨ ({method} feature {i}): {e}")
            importances[i] = 0.0

    return dict(zip(names, importances.tolist()))

def compute_permutation_importance(model, X_val, y_val, feature_names, **kwargs):
    """
    âœ… ê¸°ì¡´ permutation importance í•¨ìˆ˜ â†’ compute_feature_importanceì™€ ë™ì¼ ë°©ì‹ìœ¼ë¡œ í†µì¼
    """
    return compute_feature_importance(model, X_val, y_val, feature_names, method="permutation", **kwargs)

def save_feature_importance(importances, symbol, strategy, model_type, method="baseline"):
    suffix = f"_importance_{method}"
    fname_json = f"{symbol}_{strategy}_{model_type}{suffix}.json"
    fname_csv = f"{symbol}_{strategy}_{model_type}{suffix}.csv"
    path_json = os.path.join(IMPORTANCE_DIR, fname_json)
    path_csv = os.path.join(IMPORTANCE_DIR, fname_csv)

    importances = {str(k): float(v) for k, v in (importances or {}).items()}

    with open(path_json, "w", encoding="utf-8") as f:
        json.dump(importances, f, ensure_ascii=False, indent=2)

    df = pd.DataFrame(importances.items(), columns=["feature", "importance"]).sort_values(by="importance", ascending=False)
    df.to_csv(path_csv, index=False, encoding="utf-8-sig")

    print(f"âœ… ì¤‘ìš”ë„ ì €ì¥ ì™„ë£Œ: {path_json}, {path_csv}")

def drop_low_importance_features(
    df: pd.DataFrame,
    importances: dict,
    threshold: float = 0.05,
    input_size: int = None,
    min_features: int = 5
) -> pd.DataFrame:
    """
    âœ… feature importance ê¸°ë°˜ low-importance feature drop í•¨ìˆ˜
    - threshold ì´í•˜ feature ì œê±°
    - ìµœì†Œ min_features ê°œìˆ˜ ìœ ì§€ (ë¶€ì¡± ì‹œ pad ì»¬ëŸ¼ ì¶”ê°€)
    """
    importances = importances or {}
    drop_cols = [col for col, imp in importances.items() if (imp is not None and float(imp) < threshold)]
    remaining_cols = [c for c in df.columns if c not in drop_cols and c not in ["timestamp", "strategy"]]

    # âœ… ìµœì†Œ ê°œìˆ˜ ìœ ì§€
    if len(remaining_cols) < min_features:
        for i in range(len(remaining_cols), min_features):
            pad_col = f"pad_{i}"
            if pad_col not in df.columns:
                df[pad_col] = 0.0
            remaining_cols.append(pad_col)

    # âœ… ëª¨ë“  ì»¬ëŸ¼ ì œê±° ë°©ì§€
    if not remaining_cols:
        print("[ê²½ê³ ] ëª¨ë“  featureê°€ ì œê±°ë˜ì—ˆìŒ. pad_0 ì»¬ëŸ¼ ì¶”ê°€")
        df["pad_0"] = 0.0
        remaining_cols = ["pad_0"]

    print(f"ğŸ§¹ ì œê±°ëœ feature ìˆ˜: {len(drop_cols)} â†’ {drop_cols}")
    cols_out = remaining_cols + [c for c in ["timestamp", "strategy"] if c in df.columns]
    return df[cols_out]

def get_top_features(importances: dict, top_n: int = 10) -> pd.DataFrame:
    if not importances:
        return pd.DataFrame(columns=["feature", "importance"])
    df = pd.DataFrame(importances.items(), columns=["feature", "importance"])
    return df.sort_values(by="importance", ascending=False).head(top_n)
