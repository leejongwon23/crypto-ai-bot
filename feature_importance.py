import torch
import numpy as np
import os
import json
import pandas as pd

PERSIST_DIR = "/persistent"
IMPORTANCE_DIR = os.path.join(PERSIST_DIR, "importances")
os.makedirs(IMPORTANCE_DIR, exist_ok=True)

def compute_feature_importance(model, X_val, y_val, feature_names, method="baseline"):
    """
    âœ… feature importance ê³„ì‚° (baseline permutation ë°©ì‹)
    """
    model.eval()

    try:
        logits = model(X_val)
        y_val = y_val.view(-1).long()
        baseline_loss = torch.nn.CrossEntropyLoss()(logits, y_val).item()
    except Exception as e:
        print(f"[ERROR] ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨ ({method}): {e}")
        return dict(zip(feature_names, [0.0] * len(feature_names)))

    importances = []
    for i in range(X_val.shape[2]):
        try:
            X_permuted = X_val.clone()
            # âœ… permutation ë°©ì‹ ì¼ê´€í™”
            perm_idx = torch.randperm(X_val.shape[0])
            X_permuted[:, :, i] = X_permuted[perm_idx, :, i]
            logits_perm = model(X_permuted)
            loss = torch.nn.CrossEntropyLoss()(logits_perm, y_val).item()
            importances.append(loss - baseline_loss)
        except Exception as e:
            print(f"[ERROR] ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨ ({method} feature {i}): {e}")
            importances.append(0.0)

    return dict(zip(feature_names, importances))


def compute_feature_importance(model, X_val, y_val, feature_names, method="baseline"):
    """
    âœ… feature importance ê³„ì‚° (baseline permutation ë°©ì‹)
    """
    model.eval()

    try:
        logits = model(X_val)
        y_val = y_val.view(-1).long()
        baseline_loss = torch.nn.CrossEntropyLoss()(logits, y_val).item()
    except Exception as e:
        print(f"[ERROR] ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨ ({method}): {e}")
        return dict(zip(feature_names, [0.0] * len(feature_names)))

    importances = []
    for i in range(X_val.shape[2]):
        try:
            X_permuted = X_val.clone()
            # âœ… permutation ë°©ì‹ ì¼ê´€í™”
            perm_idx = torch.randperm(X_val.shape[0])
            X_permuted[:, :, i] = X_permuted[perm_idx, :, i]
            logits_perm = model(X_permuted)
            loss = torch.nn.CrossEntropyLoss()(logits_perm, y_val).item()
            importances.append(loss - baseline_loss)
        except Exception as e:
            print(f"[ERROR] ì¤‘ìš”ë„ ê³„ì‚° ì‹¤íŒ¨ ({method} feature {i}): {e}")
            importances.append(0.0)

    return dict(zip(feature_names, importances))


def compute_permutation_importance(model, X_val, y_val, feature_names):
    """
    âœ… ê¸°ì¡´ permutation importance í•¨ìˆ˜ â†’ compute_feature_importanceì™€ ë™ì¼ ë°©ì‹ìœ¼ë¡œ í†µì¼
    """
    return compute_feature_importance(model, X_val, y_val, feature_names, method="permutation")

def save_feature_importance(importances, symbol, strategy, model_type, method="baseline"):
    suffix = f"_importance_{method}"
    fname_json = f"{symbol}_{strategy}_{model_type}{suffix}.json"
    fname_csv = f"{symbol}_{strategy}_{model_type}{suffix}.csv"
    path_json = os.path.join(IMPORTANCE_DIR, fname_json)
    path_csv = os.path.join(IMPORTANCE_DIR, fname_csv)

    importances = {k: float(v) for k, v in importances.items()}

    with open(path_json, "w") as f:
        json.dump(importances, f, indent=2)

    df = pd.DataFrame(importances.items(), columns=["feature", "importance"]).sort_values(by="importance", ascending=False)
    df.to_csv(path_csv, index=False, encoding="utf-8-sig")

    print(f"âœ… ì¤‘ìš”ë„ ì €ìž¥ ì™„ë£Œ: {path_json}, {path_csv}")



def drop_low_importance_features(df: pd.DataFrame, importances: dict, threshold: float = 0.05) -> pd.DataFrame:
    drop_cols = [col for col, imp in importances.items() if imp < threshold]
    remaining_cols = [col for col in df.columns if col not in drop_cols]
    if not remaining_cols:
        print("[ê²½ê³ ] ëª¨ë“  featureê°€ ì œê±°ë˜ì—ˆìŒ. ìµœì†Œ 1ê°œ ì´ìƒ ìœ ì§€ í•„ìš”.")
        return df
    print(f"ðŸ§¹ ì œê±°ëœ feature ìˆ˜: {len(drop_cols)} â†’ {drop_cols}")
    return df[remaining_cols]


def get_top_features(importances: dict, top_n: int = 10) -> pd.DataFrame:
    if not importances:
        return pd.DataFrame(columns=["feature", "importance"])
    df = pd.DataFrame(importances.items(), columns=["feature", "importance"])
    df_sorted = df.sort_values(by="importance", ascending=False).head(top_n)
    return df_sorted
