# predict.py (FIXED â€” flexible meta matching + robust model discovery + ensured logging)
# âœ… ë³€ê²½ ìš”ì•½:
# - ëª¨ë¸ íƒìƒ‰ì´ .pt / .ptz / .safetensors ëª¨ë‘ ì¸ì‹
# - ë””ë ‰í„°ë¦¬ ë³„ì¹­(SYMBOL/STRATEGY/{model}.{ext})ë„ ë™ì¼í•˜ê²Œ íƒìƒ‰
# - ë¡œë”© ê²½ë¡œëŠ” model_io.load_model(...) ì‚¬ìš©(ë¬´ì†ì‹¤ ì••ì¶•/ì•ˆì „ ì €ì¥ í¬ë§· ì§€ì›)
# - ğŸ”§ evaluate_predictions: entry_price<=0 ë˜ëŠ” label==-1 â†’ prediction_logì— ì¶”ê°€ ë¡œê·¸ ë‚¨ê¸°ì§€ ì•ŠìŒ
#   (status="invalid"ìœ¼ë¡œ ê³ ì •, failure_dbì—ë§Œ ì •ê·œí™” ì‚¬ìœ ë¡œ 1íšŒ ê¸°ë¡)

import os, sys, json, datetime, pytz, random, time, tempfile, shutil, csv, glob
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from sklearn.preprocessing import MinMaxScaler

from data.utils import get_kline_by_strategy, compute_features

# --- window_optimizer í˜¸í™˜ ì„í¬íŠ¸ ---
try:
    from window_optimizer import find_best_windows  # ì„ í˜¸
except Exception:
    try:
        from window_optimizer import find_best_window
    except Exception:
        find_best_window = None
    def find_best_windows(symbol, strategy):
        try:
            if callable(find_best_window):
                best = int(find_best_window(symbol, strategy, window_list=[10, 20, 30, 40, 60], group_id=None))
            else:
                best = 60
        except Exception:
            best = 60
        return [best, best, best]

# --- (ì˜µì…˜) ë ˆì§/ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“ˆ: ì—†ìœ¼ë©´ ì•ˆì „ íŒ¨ìŠ¤ ---
try:
    from regime_detector import detect_regime
except Exception:
    def detect_regime(symbol, strategy, now=None):
        return "unknown"

try:
    from calibration import apply_calibration, get_calibration_version
except Exception:
    def apply_calibration(probs, *, symbol=None, strategy=None, regime=None, model_meta=None):
        return probs  # no-op
    def get_calibration_version():
        return "none"

# âœ… ëª¨ë¸ ì…ì¶œë ¥ í†µí•©(.pt/.ptz/.safetensors ì§€ì›)
try:
    from model_io import load_model as load_model_any  # í‘œì¤€ ê²½ë¡œ
except Exception:
    # í´ë°±(êµ¬ë²„ì „): ê¸°ì¡´ ìºì‹œ ë¡œë”ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ìµœí›„ì—” state_dict ë¡œë”©
    try:
        from model_weight_loader import load_model_cached as load_model_any  # íƒ€ì… í˜¸í™˜
    except Exception:
        def load_model_any(path, model, ttl_sec=600):
            try:
                sd = torch.load(path, map_location="cpu")
                if isinstance(sd, dict):
                    model.load_state_dict(sd)
                else:
                    model = sd
                return model
            except Exception:
                return None

# logger
from logger import log_prediction, update_model_success, PREDICTION_HEADERS, ensure_prediction_log_exists
from failure_db import insert_failure_record, load_existing_failure_hashes, ensure_failure_db
from predict_trigger import get_recent_class_frequencies, adjust_probs_with_diversity
from model.base_model import get_model
from config import (
    get_NUM_CLASSES, get_FEATURE_INPUT_SIZE, get_class_groups,
    get_class_return_range, class_to_expected_return
)

DEVICE = torch.device("cpu")
MODEL_DIR = "/persistent/models"
PREDICTION_LOG_PATH = "/persistent/prediction_log.csv"

NUM_CLASSES = get_NUM_CLASSES()
FEATURE_INPUT_SIZE = get_FEATURE_INPUT_SIZE()
now_kst = lambda: datetime.datetime.now(pytz.timezone("Asia/Seoul"))

# âœ… ìµœì†Œ ì˜ˆì¸¡ ê¸°ëŒ€ìˆ˜ìµë¥  ì„ê³„ì¹˜(ê¸°ë³¸ 1%) â€” ì´ë³´ë‹¤ ì‘ì€ í´ë˜ìŠ¤ëŠ” ì„ íƒ/ê¸°ë¡í•˜ì§€ ì•ŠìŒ
MIN_RET_THRESHOLD = float(os.getenv("PREDICT_MIN_RETURN", "0.01"))

# =======================
# (NEW) íƒí—˜(Explore) ì„¤ì •
# =======================
EXPLORE_STATE_PATH = "/persistent/logs/meta_explore_state.json"
EXPLORE_EPS_BASE   = float(os.getenv("EXPLORE_EPS_BASE", "0.15"))
EXPLORE_DECAY_MIN  = float(os.getenv("EXPLORE_DECAY_MIN", "120"))
EXPLORE_NEAR_GAP   = float(os.getenv("EXPLORE_NEAR_GAP", "0.07"))
EXPLORE_GAMMA      = float(os.getenv("EXPLORE_GAMMA", "0.05"))

def _load_explore_state():
    try:
        if os.path.exists(EXPLORE_STATE_PATH):
            with open(EXPLORE_STATE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {}

def _save_explore_state(state):
    try:
        os.makedirs(os.path.dirname(EXPLORE_STATE_PATH), exist_ok=True)
        with open(EXPLORE_STATE_PATH, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def _bump_model_usage(symbol, strategy, model_path, explored=False):
    key = f"{symbol}|{strategy}"
    st = _load_explore_state()
    st.setdefault(key, {})
    rec = st[key].setdefault(model_path, {"n": 0, "n_explore": 0, "last_explore_ts": 0.0})
    rec["n"] = int(rec.get("n", 0)) + 1
    if explored:
        rec["n_explore"] = int(rec.get("n_explore", 0)) + 1
        rec["last_explore_ts"] = float(time.time())
    st[key][model_path] = rec
    _save_explore_state(st)

def _get_model_usage(symbol, strategy, model_path):
    key = f"{symbol}|{strategy}"
    st = _load_explore_state()
    rec = ((st.get(key) or {}).get(model_path)) or {"n": 0, "n_explore": 0, "last_explore_ts": 0.0}
    return int(rec.get("n", 0)), int(rec.get("n_explore", 0)), float(rec.get("last_explore_ts", 0.0))

# -----------------------------
# ë¡œì»¬ í—¬í¼: feature hash
# -----------------------------
def _get_feature_hash(feature_row) -> str:
    try:
        import hashlib
        if feature_row is None:
            return "none"
        if isinstance(feature_row, torch.Tensor):
            arr = feature_row.detach().cpu().flatten().numpy().astype(float)
        elif isinstance(feature_row, np.ndarray):
            arr = feature_row.flatten().astype(float)
        elif isinstance(feature_row, (list, tuple)):
            arr = np.array(feature_row, dtype=float).flatten()
        else:
            arr = np.array([float(feature_row)], dtype=float)
        rounded = [round(float(x), 2) for x in arr]
        joined = ",".join(map(str, rounded))
        return hashlib.sha1(joined.encode()).hexdigest()
    except Exception:
        return "hash_error"

# -----------------------------
# ìœ ì—°í•œ ëª¨ë¸/ë©”íƒ€ íƒìƒ‰ (í•µì‹¬ FIX)
# -----------------------------
_KNOWN_EXTS = (".pt", ".ptz", ".safetensors")

def _stem_without_ext(filename: str) -> str:
    for ext in _KNOWN_EXTS:
        if filename.endswith(ext):
            return filename[: -len(ext)]
    # fallback
    return os.path.splitext(filename)[0]

def _resolve_meta_for_weight(weight_basename: str) -> str | None:
    """
    weight ì´ë¦„(ì˜ˆ: BTCUSDT_ë‹¨ê¸°_lstm.pt/ptz/safetensors)ì— ëŒ€í•´ ë‹¤ìŒ ìš°ì„ ìˆœìœ„ë¡œ metaë¥¼ ì°¾ëŠ”ë‹¤.
    1) ë™ì¼ stem: BTCUSDT_ë‹¨ê¸°_lstm.meta.json
    2) ê·¸ë£¹/í´ë˜ìŠ¤ ë²„ì „: BTCUSDT_ë‹¨ê¸°_lstm_*.meta.json
    3) ë””ë ‰í„°ë¦¬ ë³„ì¹­: models/SYMBOL/STRATEGY/{model}.meta.json
    """
    base_no_ext = _stem_without_ext(weight_basename)
    # 1) ë™ì¼ ë² ì´ìŠ¤
    cand = os.path.join(MODEL_DIR, f"{base_no_ext}.meta.json")
    if os.path.exists(cand):
        return cand
    # 2) group/clsê°€ ë¶™ì€ ì›ë³¸ ë©”íƒ€
    pattern = os.path.join(MODEL_DIR, f"{base_no_ext}_*.meta.json")
    matches = sorted(glob.glob(pattern))
    if matches:
        return matches[0]
    # 3) ë””ë ‰í„°ë¦¬ êµ¬ì¡° ë³„ì¹­ (SYMBOL/STRATEGY/{model}.meta.json)
    try:
        parts = base_no_ext.split("_")
        if len(parts) >= 3:
            sym, strat, mtype = parts[0], parts[1], parts[2]
            cand2 = os.path.join(MODEL_DIR, sym, strat, f"{mtype}.meta.json")
            if os.path.exists(cand2):
                return cand2
    except Exception:
        pass
    return None

def _glob_many(pattern_stem: str, exts=_KNOWN_EXTS):
    """pattern_stemì— ëŒ€í•´ ì—¬ëŸ¬ í™•ì¥ì ì¡°í•©ìœ¼ë¡œ glob."""
    out = []
    for ext in exts:
        out.extend(glob.glob(f"{pattern_stem}{ext}"))
    return out

def get_available_models(symbol: str, strategy: str):
    """
    PT/PTZ/SAFETENSORSì™€ META íŒŒì¼ëª…ì´ ì„œë¡œ ë‹¬ë¼ë„(í‰íƒ„/ê·¸ë£¹/ë””ë ‰í„°ë¦¬ í˜¼ì¬) ì•ˆì „í•˜ê²Œ ëª¨ë¸ì„ ìˆ˜ì§‘.
    """
    try:
        if not os.path.isdir(MODEL_DIR):
            return []
        items = []
        prefix = f"{symbol}_"
        needle = f"_{strategy}_"
        # 1) í‰íƒ„ íŒŒì¼ ì—´ê±°(.pt/.ptz/.safetensors)
        for fn in os.listdir(MODEL_DIR):
            if not any(fn.endswith(ext) for ext in _KNOWN_EXTS):
                continue
            if not fn.startswith(prefix):
                continue
            if needle not in fn:
                continue
            weight_path = os.path.join(MODEL_DIR, fn)
            meta_path = _resolve_meta_for_weight(fn)
            if not meta_path or not os.path.exists(meta_path):
                fallback = os.path.join(MODEL_DIR, f"{_stem_without_ext(fn)}.meta.json")
                if not os.path.exists(fallback):
                    print(f"[ë©”íƒ€ ë¯¸ë°œê²¬] weight={fn} â†’ meta ì°¾ê¸° ì‹¤íŒ¨")
                    continue
                meta_path = fallback
            items.append({"pt_file": fn, "meta_path": meta_path})

        # 2) ê·¸ë£¹í˜• ì›ë³¸(.pt|.ptz|.safetensors)
        group_stem = os.path.join(MODEL_DIR, f"{symbol}_{strategy}_*group*cls*")
        for gpath in _glob_many(group_stem):
            gfn = os.path.basename(gpath)
            meta_path = _resolve_meta_for_weight(gfn)
            if meta_path and {"pt_file": gfn, "meta_path": meta_path} not in items:
                items.append({"pt_file": gfn, "meta_path": meta_path})

        # ì •ë ¬
        items.sort(key=lambda x: x["pt_file"])
        return items
    except Exception as e:
        print(f"[get_available_models ì˜¤ë¥˜] {e}")
        return []

# -----------------------------
# ì‹¤íŒ¨ ê²°ê³¼ ë¹ ë¥¸ ê¸°ë¡
# -----------------------------
def failed_result(symbol, strategy, model_type="unknown", reason="", source="ì¼ë°˜", X_input=None):
    from datetime import datetime as _dt
    t = _dt.now(pytz.timezone("Asia/Seoul")).strftime("%Y-%m-%d %H:%M:%S")
    result = {
        "symbol": symbol, "strategy": strategy, "success": False, "reason": reason,
        "model": str(model_type or "unknown"), "rate": 0.0, "class": -1,
        "timestamp": t, "source": source, "predicted_class": -1, "label": -1
    }
    try:
        ensure_prediction_log_exists()
        log_prediction(
            symbol=symbol, strategy=strategy, direction="ì˜ˆì¸¡ì‹¤íŒ¨",
            entry_price=0, target_price=0, model=str(model_type or "unknown"),
            success=False, reason=reason, rate=0.0, timestamp=t,
            return_value=0.0, volatility=True, source=source,
            predicted_class=-1, label=-1
        )
    except Exception as e:
        print(f"[failed_result log_prediction ì˜¤ë¥˜] {e}")
    try:
        if X_input is not None:
            feature_hash = _get_feature_hash(X_input)
            insert_failure_record(result, feature_hash, feature_vector=np.array(X_input).flatten().tolist(), label=-1)
    except Exception as e:
        print(f"[failed_result insert_failure_record ì˜¤ë¥˜] {e}")
    return result

# -----------------------------
# ë©”ì¸ ì˜ˆì¸¡
# -----------------------------
def predict(symbol, strategy, source="ì¼ë°˜", model_type=None):
    """
    - ì €ì¥ëœ ëª¨ë¸ ì¶œë ¥ ì·¨í•©(íŒŒì¼ëª… ë¶ˆì¼ì¹˜ë„ ìœ ì—° ë§¤ì¹­)
    - ë©”íƒ€ëŸ¬ë„ˆ/ë‹¨ì¼ìµœê³ í™•ë¥ (+íƒí—˜) ì„ íƒ
    - âœ… MIN_RET_THRESHOLD ë¯¸ë§Œ í´ë˜ìŠ¤ëŠ” ì œì™¸
    """
    # ğŸ“Œ ë¡œê·¸ í—¤ë” ë³´ì¥(ë¹„ì–´ìˆì—ˆë‹¤ê³  í–ˆìœ¼ë‹ˆ ì‹œì‘ë§ˆë‹¤ ë³´ê°•)
    try:
        ensure_prediction_log_exists()
    except Exception as _e:
        print(f"[í—¤ë”ë³´ì¥ ì‹¤íŒ¨] {_e}")

    try:
        from evo_meta_learner import predict_evo_meta
    except Exception:
        predict_evo_meta = None
    try:
        from meta_learning import get_meta_prediction
    except Exception:
        def get_meta_prediction(probs_list, feature_tensor, meta_info=None):
            avg = np.mean(np.array(probs_list), axis=0)
            return int(np.argmax(avg))

    ensure_failure_db()
    os.makedirs("/persistent/logs", exist_ok=True)

    if not symbol or not strategy:
        return failed_result(symbol or "None", strategy or "None", reason="invalid_symbol_strategy", X_input=None)

    regime = detect_regime(symbol, strategy, now=now_kst())
    calib_ver = get_calibration_version()

    # 1) ì¤€ë¹„
    window_list = find_best_windows(symbol, strategy)
    if not window_list:
        return failed_result(symbol, strategy, reason="window_list_none", X_input=None)

    df = get_kline_by_strategy(symbol, strategy)
    if df is None or len(df) < max(window_list) + 1:
        return failed_result(symbol, strategy, reason="df_short", X_input=None)

    feat = compute_features(symbol, df, strategy)
    if feat is None or feat.dropna().shape[0] < max(window_list) + 1:
        return failed_result(symbol, strategy, reason="feature_short", X_input=None)

    features_only = feat.drop(columns=["timestamp", "strategy"], errors="ignore")
    feat_scaled = MinMaxScaler().fit_transform(features_only)
    if feat_scaled.shape[1] < FEATURE_INPUT_SIZE:
        feat_scaled = np.pad(feat_scaled, ((0, 0), (0, FEATURE_INPUT_SIZE - feat_scaled.shape[1])), mode="constant")
    else:
        feat_scaled = feat_scaled[:, :FEATURE_INPUT_SIZE]

    models = get_available_models(symbol, strategy)
    if not models:
        return failed_result(symbol, strategy, reason="no_models", X_input=feat_scaled[-1])

    recent_freq = get_recent_class_frequencies(strategy)
    feature_tensor = torch.tensor(feat_scaled[-1], dtype=torch.float32)

    # 2) ëª¨ë¸ë³„ í™•ë¥  ê³„ì‚°
    model_outputs_list, all_model_predictions = get_model_predictions(
        symbol, strategy, models, df, feat_scaled, window_list, recent_freq, regime=regime
    )
    if not model_outputs_list:
        return failed_result(symbol, strategy, reason="no_valid_model", X_input=feat_scaled[-1])

    # 3) (ì˜µì…˜) ì§„í™”í˜• ë©”íƒ€ ì‚¬ìš© â€” ì„ê³„ ë¯¸ë§Œ í´ë˜ìŠ¤ë©´ ë¬´ì‹œ
    final_pred_class = None
    meta_choice = "best_single"
    chosen_info = None
    used_minret_filter = False
    use_evo = False

    # evo ë©”íƒ€ íŒŒì¼ì€ ì–´ë–¤ í™•ì¥ìë“  í—ˆìš©
    evo_candidates = _glob_many(os.path.join(MODEL_DIR, "evo_meta_learner"))
    if evo_candidates:
        try:
            from evo_meta_learner import predict_evo_meta  # ì¬í™•ì¸
            if callable(predict_evo_meta):
                evo_pred = predict_evo_meta(feature_tensor.unsqueeze(0), input_size=FEATURE_INPUT_SIZE)
                if evo_pred is not None:
                    evo_pred = int(evo_pred)
                    cls_min_evo, _ = get_class_return_range(evo_pred, symbol, strategy)
                    if cls_min_evo >= MIN_RET_THRESHOLD:
                        final_pred_class = evo_pred
                        use_evo = True
                    else:
                        print(f"[META] ì§„í™”í˜• ì˜ˆì¸¡ {evo_pred} ìµœì†Œìˆ˜ìµ {cls_min_evo:.4f} < ì„ê³„ {MIN_RET_THRESHOLD:.4f} â†’ ë¬´ì‹œ")
        except Exception as e:
            print(f"[âš ï¸ ì§„í™”í˜• ë©”íƒ€ëŸ¬ë„ˆ ì˜ˆì™¸] {e}")

    # 4) 'ìµœê³  ì„±ê³µí™•ë¥  ë‹¨ì¼ ëª¨ë¸' + (NEW) íƒí—˜
    if final_pred_class is None:
        # ê° ëª¨ë¸ì˜ ì ìˆ˜ ê³„ì‚°
        best_idx, best_score, best_pred = -1, -1.0, None
        scores = []  # [(idx, score, candidate_pred)]
        for i, m in enumerate(model_outputs_list):
            calib_probs = m["calib_probs"]
            adj = adjust_probs_with_diversity(calib_probs, recent_freq, class_counts=None, alpha=0.10, beta=0.10)
            val_f1 = float(m.get("val_f1", 0.6))

            # ì„ê³„ì¹˜ í•„í„° ë§ˆìŠ¤í¬
            valid_mask = np.zeros_like(adj, dtype=float)
            for ci in range(len(adj)):
                try:
                    cls_min, _ = get_class_return_range(ci, symbol, strategy)
                    if float(cls_min) >= MIN_RET_THRESHOLD:
                        valid_mask[ci] = 1.0
                except Exception:
                    pass
            adj_filtered = adj * valid_mask
            if adj_filtered.sum() > 0:
                adj_filtered = adj_filtered / adj_filtered.sum()
                pred = int(np.argmax(adj_filtered))
                prob_for_score = float(adj_filtered[pred])
                used_filter_here = True
            else:
                pred = int(np.argmax(adj))
                prob_for_score = float(adj[pred])
                used_filter_here = False

            score = prob_for_score * (0.5 + 0.5 * max(0.0, min(1.0, val_f1)))
            m["adjusted_probs"] = adj
            m["success_score"] = score
            m["filtered_used"] = used_filter_here
            m["filtered_probs"] = adj_filtered if used_filter_here else None
            m["candidate_pred"] = pred
            scores.append((i, score, pred))

            if score > best_score:
                best_score, best_idx, best_pred = score, i, pred
                used_minret_filter = used_filter_here

        # ----- (NEW) íƒí—˜ ë¡œì§ -----
        explore_used = False
        explore_alt_idx = None
        if len(scores) >= 2:
            scores_sorted = sorted(scores, key=lambda x: x[1], reverse=True)
            top1_i, top1_score, _ = scores_sorted[0]
            top2_i, top2_score, _ = scores_sorted[1]
            gap = float(top1_score - top2_score)

            # ìµœê·¼ íƒí—˜ ê°ì‡„ ë°˜ì˜(ì‹¬ë³¼/ì „ëµ ë‹¨ìœ„)
            key = f"{symbol}|{strategy}"
            st = _load_explore_state()
            last_explore = 0.0
            if key in st:
                last_explore = max((rec.get("last_explore_ts", 0.0) or 0.0) for rec in st[key].values()) if st[key] else 0.0
            minutes_since = (time.time() - last_explore) / 60.0 if last_explore > 0 else 1e9
            eps = EXPLORE_EPS_BASE * (0.5 if minutes_since < EXPLORE_DECAY_MIN else 1.0)

            if gap <= EXPLORE_NEAR_GAP and random.random() < eps:
                # í›„ë³´ë“¤ì—ê²Œ 'ëœ ì„ íƒëœ ëª¨ë¸' ë³´ë„ˆìŠ¤ ë¶€ì—¬
                cand_scores = []
                for i, base_score, _pred in scores_sorted[:min(3, len(scores_sorted))]:
                    mp = model_outputs_list[i].get("model_path", "")
                    n_chosen, _n_exp, _ts = _get_model_usage(symbol, strategy, mp)
                    bonus = EXPLORE_GAMMA / np.sqrt(1.0 + float(n_chosen))
                    cand_scores.append((i, base_score + bonus, base_score, bonus))

                cand_scores.sort(key=lambda x: x[1], reverse=True)
                if cand_scores:
                    explore_alt_idx = cand_scores[0][0]
                    if explore_alt_idx != top1_i:
                        best_idx = explore_alt_idx
                        best_pred = model_outputs_list[best_idx]["candidate_pred"]
                        best_score = model_outputs_list[best_idx]["success_score"]
                        explore_used = True
                        meta_choice = "best_single_explore"

        final_pred_class = int(best_pred)
        chosen_info = model_outputs_list[best_idx]
        if meta_choice != "best_single_explore":
            meta_choice = os.path.basename(chosen_info["model_path"])
    else:
        meta_choice = "evo_meta_learner"
        chosen_info = max(model_outputs_list, key=lambda m: m.get("success_score", 0.0)) if model_outputs_list else None
        explore_used = False
        explore_alt_idx = None

    # ìµœì¢… ê°€ë“œ: ì„ê³„ ë¯¸ë§Œì´ë©´ ì „ ëª¨ë¸ì„ ê°€ë¡œì§ˆëŸ¬ ëŒ€ì²´ í›„ë³´ íƒìƒ‰
    try:
        cls_min_sel, _ = get_class_return_range(final_pred_class, symbol, strategy)
        if float(cls_min_sel) < MIN_RET_THRESHOLD:
            print(f"[GUARD] ì„ íƒ í´ë˜ìŠ¤ {final_pred_class} ìµœì†Œìˆ˜ìµ {cls_min_sel:.4f} < ì„ê³„ {MIN_RET_THRESHOLD:.4f} â†’ ëŒ€ì²´ íƒìƒ‰")
            best_global_idx, best_global_score, best_global_class = None, -1.0, None
            for m in model_outputs_list:
                adj = m.get("adjusted_probs", m["calib_probs"])
                val_f1 = float(m.get("val_f1", 0.6))
                for ci in range(len(adj)):
                    try:
                        cmin, _ = get_class_return_range(ci, symbol, strategy)
                        if float(cmin) < MIN_RET_THRESHOLD:
                            continue
                        score = float(adj[ci]) * (0.5 + 0.5 * max(0.0, min(1.0, val_f1)))
                        if score > best_global_score:
                            best_global_score, best_global_idx, best_global_class = score, m, int(ci)
                    except Exception:
                        continue
            if best_global_class is not None:
                final_pred_class, chosen_info, used_minret_filter = best_global_class, best_global_idx, True
                explore_used = False  # ì„ê³„ì¹˜ ê°€ë“œê°€ ìš°ì„ 
            else:
                return failed_result(symbol, strategy, reason="no_class_ge_min_return", X_input=feat_scaled[-1])
    except Exception as e:
        print(f"[ì„ê³„ì¹˜ ìµœì¢… ê°€ë“œ ì˜ˆì™¸] {e}")

    print(f"[META] {'ì§„í™”í˜•' if meta_choice=='evo_meta_learner' else 'ìµœê³ í™•ë¥ ëª¨ë¸'} ì„ íƒ: í´ë˜ìŠ¤ {final_pred_class}")

    # 5) ë¡œê¹… ë° ì„±ê³µíŒì •(ë©”íƒ€ ìµœì¢…)
    cls_min, _ = get_class_return_range(final_pred_class, symbol, strategy)
    current_price = float(df.iloc[-1]["close"])
    expected_ret = class_to_expected_return(final_pred_class, symbol, strategy)
    entry_price = float(current_price)  # í˜„ì¬ê°€ë¥¼ ì§„ì…ê°€ë¡œ ì¼ì¹˜
    actual_return_meta = 0.0  # ì§„ì… ì‹œì  ì¦‰ì‹œ ìˆ˜ìµë¥ ì€ 0ìœ¼ë¡œ ê¸°ë¡(í‰ê°€ëŠ” ì´í›„)

    def _topk(probs, k=3):
        idx = np.argsort(probs)[::-1][:k]
        return [int(i) for i in idx]

    calib_topk = _topk((chosen_info or model_outputs_list[0])["calib_probs"]) if (chosen_info or model_outputs_list) else []

    # íƒí—˜ ì´ë ¥ ì—…ë°ì´íŠ¸
    try:
        if meta_choice in ["best_single_explore"] or (isinstance(chosen_info, dict) and chosen_info.get("model_path")):
            _bump_model_usage(symbol, strategy, chosen_info.get("model_path", ""), explored=(meta_choice=="best_single_explore"))
    except Exception as _e:
        print(f"[íƒí—˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨] {_e}")

    note_payload = {
        "regime": regime,
        "meta_choice": meta_choice,
        "raw_prob_pred": float((chosen_info or model_outputs_list[0])["raw_probs"][final_pred_class]) if (chosen_info or model_outputs_list) else None,
        "calib_prob_pred": float((chosen_info or model_outputs_list[0])["calib_probs"][final_pred_class]) if (chosen_info or model_outputs_list) else None,
        "calib_ver": get_calibration_version(),
        "min_return_threshold": float(MIN_RET_THRESHOLD),
        "used_minret_filter": bool(used_minret_filter),
        "explore_used": bool('best_single_explore' in str(meta_choice)),
    }

    ensure_prediction_log_exists()
    log_prediction(
        symbol=symbol,
        strategy=strategy,
        direction="ì˜ˆì¸¡",
        entry_price=entry_price,
        target_price=entry_price * (1 + expected_ret),
        model="meta",
        model_name="evo_meta_learner" if meta_choice=="evo_meta_learner" else str(meta_choice),
        predicted_class=final_pred_class,
        label=final_pred_class,
        note=json.dumps(note_payload, ensure_ascii=False),
        top_k=calib_topk,
        success=False,                 # ì¦‰ì‹œ ì„±ê³µíŒì • X â†’ í‰ê°€ì—ì„œ ê²°ì •
        reason="predicted",
        rate=expected_ret,
        return_value=actual_return_meta,
        source="ì§„í™”í˜•" if meta_choice=="evo_meta_learner" else "ê¸°ë³¸",
        group_id=(chosen_info.get("group_id") if chosen_info else None) if isinstance(chosen_info, dict) else None,
        feature_vector=feature_tensor.numpy()
    )

    # ğŸ”¥ ë©”íƒ€ì— ì„ íƒë˜ì§€ ì•Šì€ "ëª¨ë“  ëª¨ë¸"ë„ ì„€ë„ìš° ì˜ˆì¸¡ìœ¼ë¡œ ê¸°ë¡
    try:
        for m in model_outputs_list:
            if chosen_info and m.get("model_path") == chosen_info.get("model_path"):
                continue

            adj = m.get("adjusted_probs", m["calib_probs"])
            filt = m.get("filtered_probs", None)
            # ì„ê³„ì¹˜ ë§Œì¡±í•˜ëŠ” í´ë˜ìŠ¤ ìš°ì„ 
            if filt is not None and np.sum(filt) > 0:
                pred_i = int(np.argmax(filt))
                topk_src = filt
            else:
                mask = np.zeros_like(adj, dtype=float)
                for ci in range(len(adj)):
                    try:
                        cmin, _ = get_class_return_range(ci, symbol, strategy)
                        if float(cmin) >= MIN_RET_THRESHOLD:
                            mask[ci] = 1.0
                    except Exception:
                        pass
                adj2 = adj * mask
                if np.sum(adj2) == 0:
                    continue
                adj2 = adj2 / np.sum(adj2)
                pred_i = int(np.argmax(adj2))
                topk_src = adj2

            exp_ret_i = class_to_expected_return(pred_i, symbol, strategy)
            top_k_i = [int(i) for i in np.argsort(topk_src)[::-1][:3]]
            note_shadow = {
                "regime": regime,
                "shadow": True,
                "model_path": os.path.basename(m.get("model_path","")),
                "model_type": m.get("model_type",""),
                "val_f1": float(m.get("val_f1",0.0)),
                "calib_ver": get_calibration_version(),
                "min_return_threshold": float(MIN_RET_THRESHOLD)
            }
            log_prediction(
                symbol=symbol,
                strategy=strategy,
                direction="ì˜ˆì¸¡(ì„€ë„ìš°)",
                entry_price=entry_price,
                target_price=entry_price * (1 + exp_ret_i),
                model=m.get("model_type","model"),
                model_name=os.path.basename(m.get("model_path","")),
                predicted_class=pred_i,
                label=pred_i,
                note=json.dumps(note_shadow, ensure_ascii=False),
                top_k=top_k_i,
                success=False,
                reason="shadow",
                rate=exp_ret_i,
                return_value=0.0,
                source="ì„€ë„ìš°",
                group_id=m.get("group_id", 0),
                feature_vector=feature_tensor.numpy()
            )
    except Exception as e:
        print(f"[ì„€ë„ìš° ë¡œê¹… ì˜ˆì™¸] {e}")

    return {
        "symbol": symbol,
        "strategy": strategy,
        "model": "meta",
        "class": final_pred_class,
        "expected_return": expected_ret,
        "timestamp": now_kst().isoformat(),
        "reason": "ì§„í™”í˜• ë©”íƒ€ ìµœì¢… ì„ íƒ" if meta_choice=="evo_meta_learner" else f"ìµœê³  í™•ë¥  ë‹¨ì¼ ëª¨ë¸: {meta_choice}",
        "source": source,
        "regime": regime
    }

# -----------------------------
# ë°°ì¹˜ í‰ê°€ (ë©”ëª¨ë¦¬ ì•ˆì „ ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
# -----------------------------
def evaluate_predictions(get_price_fn):
    import pandas as pd
    from failure_db import check_failure_exists

    ensure_failure_db()
    ensure_prediction_log_exists()

    PREDICTION_LOG = PREDICTION_LOG_PATH
    now_local = lambda: datetime.datetime.now(pytz.timezone("Asia/Seoul"))
    date_str = now_local().strftime("%Y-%m-%d")
    LOG_DIR = "/persistent/logs"
    os.makedirs(LOG_DIR, exist_ok=True)
    EVAL_RESULT = os.path.join(LOG_DIR, f"evaluation_{date_str}.csv")
    WRONG = os.path.join(LOG_DIR, f"wrong_{date_str}.csv")

    eval_horizon_map = {"ë‹¨ê¸°": 4, "ì¤‘ê¸°": 24, "ì¥ê¸°": 168}

    try:
        with open(PREDICTION_LOG, "r", encoding="utf-8-sig", newline="") as f_in:
            reader = csv.DictReader(f_in)
            if reader.fieldnames is None:
                print("[ì˜¤ë¥˜] prediction_log.csv í—¤ë” ì—†ìŒ")
                return
            base = list(PREDICTION_HEADERS)
            extras = ["status", "return"]
            fieldnames = base + [c for c in extras if c not in base]

            dir_name = os.path.dirname(PREDICTION_LOG) or "."
            fd_tmp, tmp_path = tempfile.mkstemp(prefix="predlog_", suffix=".csv", dir=dir_name, text=True)
            os.close(fd_tmp)
            with open(tmp_path, "w", encoding="utf-8-sig", newline="") as f_tmp, \
                 open(EVAL_RESULT, "w", encoding="utf-8-sig", newline="") as f_eval, \
                 open(WRONG, "w", encoding="utf-8-sig", newline="") as f_wrong:

                w_all = csv.DictWriter(f_tmp, fieldnames=fieldnames)
                w_all.writeheader()

                eval_fields_written = False
                wrong_fields_written = False

                for r in reader:
                    try:
                        if r.get("status") not in [None, "", "pending", "v_pending"]:
                            out = {k: r.get(k, "") for k in fieldnames}
                            w_all.writerow(out)
                            continue

                        symbol = r.get("symbol", "UNKNOWN")
                        strategy = r.get("strategy", "ì•Œìˆ˜ì—†ìŒ")
                        model = r.get("model", "unknown")
                        try:
                            group_id = int(float(r.get("group_id", 0)))
                        except Exception:
                            group_id = 0

                        def to_int(x, default):
                            try:
                                if x in [None, ""]:
                                    return default
                                return int(float(x))
                            except Exception:
                                return default
                        pred_class = to_int(r.get("predicted_class", -1), -1)
                        label = to_int(r.get("label", -1), -1)
                        r["label"] = label

                        try:
                            entry_price = float(r.get("entry_price", 0) or 0)
                        except Exception:
                            entry_price = 0.0

                        # ğŸ”§ ì¤‘ìš”: invalid(ë¼ë²¨/ì—”íŠ¸ë¦¬ ì˜¤ë¥˜) í–‰ì€ prediction_logì— ìƒˆ í–‰ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                        if entry_price <= 0 or label == -1:
                            reason = "invalid_entry_or_label"
                            r.update({"status": "invalid", "reason": reason, "return": 0.0, "return_value": 0.0})
                            # ì‹¤íŒ¨ DB(ì¤‘ë³µ ë°©ì§€)
                            if not check_failure_exists(r):
                                insert_failure_record(r, f"{symbol}-{strategy}-{now_local().isoformat()}",
                                                     feature_vector=None, label=label)
                            # í˜„ì¬ í–‰ë§Œ ê°±ì‹ í•´ì„œ ì¬ì‘ì„±
                            w_all.writerow({k: r.get(k, "") for k in fieldnames})
                            # wrong.csvì—ë„ ê¸°ë¡
                            if not wrong_fields_written:
                                wrong_writer = csv.DictWriter(f_wrong, fieldnames=sorted(r.keys()))
                                wrong_writer.writeheader()
                                wrong_fields_written = True
                            wrong_writer.writerow({k: r.get(k, "") for k in r.keys()})
                            continue

                        ts = pd.to_datetime(r.get("timestamp"), errors="coerce")
                        if ts is None or pd.isna(ts):
                            r.update({"status": "invalid", "reason": "timestamp_parse_error", "return": 0.0, "return_value": 0.0})
                            w_all.writerow({k: r.get(k, "") for k in fieldnames})
                            if not wrong_fields_written:
                                wrong_writer = csv.DictWriter(f_wrong, fieldnames=sorted(r.keys()))
                                wrong_writer.writeheader()
                                wrong_fields_written = True
                            wrong_writer.writerow({k: r.get(k, "") for k in r.keys()})
                            continue
                        if ts.tzinfo is None:
                            ts = ts.tz_localize("Asia/Seoul")
                        else:
                            ts = ts.tz_convert("Asia/Seoul")

                        eval_hours = eval_horizon_map.get(strategy, 6)
                        deadline = ts + pd.Timedelta(hours=eval_hours)

                        df_price = get_price_fn(symbol, strategy)
                        if df_price is None or "timestamp" not in df_price.columns:
                            r.update({"status": "invalid", "reason": "no_price_data", "return": 0.0, "return_value": 0.0})
                            w_all.writerow({k: r.get(k, "") for k in fieldnames})
                            if not wrong_fields_written:
                                wrong_writer = csv.DictWriter(f_wrong, fieldnames=sorted(r.keys()))
                                wrong_writer.writeheader()
                                wrong_fields_written = True
                            wrong_writer.writerow({k: r.get(k, "") for k in r.keys()})
                            continue

                        dfp = df_price.copy()
                        dfp["timestamp"] = pd.to_datetime(dfp["timestamp"], errors="coerce")
                        dfp["timestamp"] = dfp["timestamp"].dt.tz_localize("UTC").dt.tz_convert("Asia/Seoul")
                        mask_window = (dfp["timestamp"] >= ts) & (dfp["timestamp"] <= deadline)
                        future_df = dfp.loc[mask_window]

                        if future_df.empty:
                            if now_local() < deadline:
                                r.update({"status": "pending", "reason": "â³ í‰ê°€ ëŒ€ê¸° ì¤‘(ë§ˆê° ì „ ë°ì´í„° ì—†ìŒ)", "return": 0.0, "return_value": 0.0})
                                w_all.writerow({k: r.get(k, "") for k in fieldnames})
                                continue
                            else:
                                r.update({"status": "invalid", "reason": "no_data_until_deadline", "return": 0.0, "return_value": 0.0})
                                w_all.writerow({k: r.get(k, "") for k in fieldnames})
                                if not wrong_fields_written:
                                    wrong_writer = csv.DictWriter(f_wrong, fieldnames=sorted(r.keys()))
                                    wrong_writer.writeheader()
                                    wrong_fields_written = True
                                wrong_writer.writerow({k: r.get(k, "") for k in r.keys()})
                                continue

                        actual_max = float(future_df["high"].max())
                        gain = (actual_max - entry_price) / (entry_price + 1e-12)

                        if pred_class >= 0:
                            cls_min, cls_max = get_class_return_range(pred_class, symbol, strategy)
                        else:
                            cls_min, cls_max = (0.0, 0.0)

                        reached_target = gain >= cls_min

                        # ğŸ”§ ì¡°ê¸° ì„±ê³µ ì²˜ë¦¬: ë§ˆê° ì „ ëª©í‘œ ë„ë‹¬ ì‹œ ì¦‰ì‹œ success/v_success ê¸°ë¡ ë° ë¡œê·¸ ì¶”ê°€
                        if now_local() < deadline and reached_target:
                            status = "success"
                            vol = str(r.get("volatility", "")).strip().lower() in ["1", "true"]
                            if vol:
                                status = "v_success"
                            r.update({
                                "status": status,
                                "reason": f"[ì¡°ê¸°ì„±ê³µ pred_class={pred_class}] gain={gain:.3f} (cls_min={cls_min}, cls_max={cls_max})",
                                "return": round(gain, 5),
                                "return_value": round(gain, 5),
                                "group_id": group_id
                            })
                            log_prediction(
                                symbol=symbol, strategy=strategy, direction=f"í‰ê°€:{status}",
                                entry_price=entry_price, target_price=entry_price * (1 + gain),
                                timestamp=now_local().isoformat(), model=model, predicted_class=pred_class,
                                success=True, reason=r["reason"], rate=gain, return_value=gain,
                                volatility=vol, source="í‰ê°€", label=label, group_id=group_id
                            )
                            if model == "meta":
                                update_model_success(symbol, strategy, model, True)
                            w_all.writerow({k: r.get(k, "") for k in fieldnames})
                            if not eval_fields_written:
                                eval_writer = csv.DictWriter(f_eval, fieldnames=sorted(r.keys()))
                                eval_writer.writeheader()
                                eval_fields_written = True
                            eval_writer.writerow({k: r.get(k, "") for k in r.keys()})
                            continue  # â† ì¡°ê¸° ì„±ê³µì´ë©´ ì—¬ê¸°ì„œ ë

                        if now_local() < deadline and not reached_target:
                            r.update({"status": "pending", "reason": "â³ í‰ê°€ ëŒ€ê¸° ì¤‘", "return": round(gain, 5), "return_value": round(gain, 5)})
                            w_all.writerow({k: r.get(k, "") for k in fieldnames})
                            continue

                        status = "success" if reached_target else "fail"

                        vol = str(r.get("volatility", "")).strip().lower() in ["1", "true"]
                        if vol:
                            status = "v_success" if status == "success" else "v_fail"

                        r.update({
                            "status": status,
                            "reason": f"[pred_class={pred_class}] gain={gain:.3f} (cls_min={cls_min}, cls_max={cls_max})",
                            "return": round(gain, 5),
                            "return_value": round(gain, 5),
                            "group_id": group_id
                        })

                        # ì •ìƒ í‰ê°€ ê²°ê³¼ë§Œ prediction_logì— ì´ë²¤íŠ¸ ì¶”ê°€
                        log_prediction(
                            symbol=symbol, strategy=strategy, direction=f"í‰ê°€:{status}",
                            entry_price=entry_price, target_price=entry_price * (1 + gain),
                            timestamp=now_local().isoformat(), model=model, predicted_class=pred_class,
                            success=(status in ["success", "v_success"]), reason=r["reason"],
                            rate=gain, return_value=gain, volatility=vol, source="í‰ê°€",
                            label=label, group_id=group_id
                        )

                        if status in ["fail", "v_fail"] and not check_failure_exists(r):
                            insert_failure_record(r, f"{symbol}-{strategy}-{now_local().isoformat()}",
                                                  feature_vector=None, label=label)

                        if model == "meta":
                            update_model_success(symbol, strategy, model, status in ["success", "v_success"])

                        w_all.writerow({k: r.get(k, "") for k in fieldnames})

                        if not eval_fields_written:
                            eval_writer = csv.DictWriter(f_eval, fieldnames=sorted(r.keys()))
                            eval_writer.writeheader()
                            eval_fields_written = True
                        eval_writer.writerow({k: r.get(k, "") for k in r.keys()})

                        if status in ["fail", "v_fail"]:
                            if not wrong_fields_written:
                                wrong_writer = csv.DictWriter(f_wrong, fieldnames=sorted(r.keys()))
                                wrong_writer.writeheader()
                                wrong_fields_written = True
                            wrong_writer.writerow({k: r.get(k, "") for k in r.keys()})

                    except Exception as e:
                        r.update({"status": "invalid", "reason": f"exception:{e}", "return": 0.0, "return_value": 0.0})
                        w_all.writerow({k: r.get(k, "") for k in fieldnames})
                        if not wrong_fields_written:
                            wrong_writer = csv.DictWriter(f_wrong, fieldnames=sorted(r.keys()))
                            wrong_writer.writeheader()
                            wrong_fields_written = True
                        wrong_writer.writerow({k: r.get(k, "") for k in r.keys()})

            shutil.move(tmp_path, PREDICTION_LOG)
            print("[âœ… í‰ê°€ ì™„ë£Œ] ìŠ¤íŠ¸ë¦¬ë° ì¬ì‘ì„± ì„±ê³µ")
    except FileNotFoundError:
        print(f"[ì •ë³´] {PREDICTION_LOG} ì—†ìŒ â†’ í‰ê°€ ìŠ¤í‚µ")
    except Exception as e:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        print(f"[ì˜¤ë¥˜] evaluate_predictions ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ â†’ {e}")

# -----------------------------
# ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ì·¨í•© (+ìº˜ë¦¬ë¸Œë ˆì´ì…˜)
# -----------------------------
def get_model_predictions(symbol, strategy, models, df, feat_scaled, window_list, recent_freq, regime="unknown"):
    model_outputs_list, all_model_predictions = [], []

    for model_info in models:
        try:
            pt_file = model_info.get("pt_file")
            meta_path = model_info.get("meta_path")
            if not pt_file:
                continue
            model_path = os.path.join(MODEL_DIR, pt_file)
            if not os.path.exists(model_path):
                # í˜¹ì‹œ ë””ë ‰í„°ë¦¬ ë³„ì¹­ í˜•íƒœë©´ í™•ì¥ì ì „ì²´ ê²€ì‚¬
                try:
                    parts = _stem_without_ext(pt_file).split("_")
                    if len(parts) >= 3:
                        sym, strat, mtype = parts[0], parts[1], parts[2]
                        for ext in _KNOWN_EXTS:
                            alt = os.path.join(MODEL_DIR, sym, strat, f"{mtype}{ext}")
                            if os.path.exists(alt):
                                model_path = alt
                                break
                except Exception:
                    pass

            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)

            model_type = meta.get("model", "lstm")
            group_id = meta.get("group_id", 0)
            input_size = meta.get("input_size", FEATURE_INPUT_SIZE)
            num_classes = meta.get("num_classes", NUM_CLASSES)
            val_f1 = float(meta.get("metrics", {}).get("val_f1", 0.6))

            idx = min(int(group_id), max(0, len(window_list) - 1))
            window = window_list[idx]
            input_seq = feat_scaled[-window:]
            if input_seq.shape[0] < window:
                print(f"[âš ï¸ ë°ì´í„° ë¶€ì¡±] {symbol}-{strategy}-group{group_id}")
                continue

            input_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0)

            model = get_model(model_type, input_size=input_size, output_size=num_classes)
            # âœ… í†µí•© ë¡œë” ì‚¬ìš©(.pt/.ptz/.safetensors ëª¨ë‘ ì§€ì›)
            model = load_model_any(model_path, model)
            if model is None:
                print(f"[âš ï¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨] {model_path}")
                continue
            model.eval()

            with torch.no_grad():
                out = model(input_tensor.to(DEVICE))
                softmax_probs = F.softmax(out, dim=1)
                raw_probs = softmax_probs.squeeze().cpu().numpy()

            calib_probs = apply_calibration(
                raw_probs,
                symbol=symbol, strategy=strategy, regime=regime, model_meta=meta
            ).astype(float)

            model_outputs_list.append({
                "raw_probs": raw_probs,
                "calib_probs": calib_probs,
                "predicted_class": int(np.argmax(calib_probs)),
                "group_id": group_id,
                "model_type": model_type,
                "model_path": model_path,
                "val_f1": val_f1,
                "symbol": symbol, "strategy": strategy
            })

            entry_price = df["close"].iloc[-1]
            all_model_predictions.append({
                "class": int(np.argmax(calib_probs)),
                "probs": calib_probs, "entry_price": float(entry_price),
                "num_classes": num_classes, "group_id": group_id,
                "model_name": model_type, "model_symbol": symbol,
                "symbol": symbol, "strategy": strategy
            })

        except Exception as e:
            print(f"[âŒ ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨] {model_info} â†’ {e}")
            continue

    return model_outputs_list, all_model_predictions


if __name__ == "__main__":
    res = predict("BTCUSDT", "ë‹¨ê¸°")
    print(res)
    try:
        df = pd.read_csv(PREDICTION_LOG_PATH, encoding="utf-8-sig")
        print("[âœ… prediction_log.csv ìƒìœ„ 20ì¤„ ì¶œë ¥]")
        print(df.head(20))
    except Exception as e:
        print(f"[ì˜¤ë¥˜] prediction_log.csv ë¡œë“œ ì‹¤íŒ¨ â†’ {e}")
